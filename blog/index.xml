<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>statsforecast</title>
<link>https://Nixtla.github.io/statsforecast/blog/index.html</link>
<atom:link href="https://Nixtla.github.io/statsforecast/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Time series forecasting suite using statistical models</description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Wed, 05 Oct 2022 00:00:00 GMT</lastBuildDate>
<item>
  <title>Scalable Time Series Modeling with open-source projects</title>
  <dc:creator>Fugue</dc:creator>
  <dc:creator>Nixtla</dc:creator>
  <link>https://Nixtla.github.io/statsforecast/blog/posts/2022-10-05-distributed-fugue/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="https://github.com/nixtla/statsforecast"><img src="https://Nixtla.github.io/statsforecast/blog/posts/2022-10-05-distributed-fugue/https:/img.shields.io/github/stars/nixtla/statsforecast?label=GitHub%20Stars&amp;style=social.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>By <a href="https://github.com/fugue-project/">Fugue</a> and <a href="https://github.com/nixtla/">Nixtla</a>. Originally posted on <a href="https://medium.com/towards-data-science/distributed-forecast-of-1m-time-series-in-under-15-minutes-with-spark-nixtla-and-fugue-e9892da6fd5c">TDS</a>.</p>
<blockquote class="blockquote">
<p><strong>TL:DR We will show how you can leverage the distributed power of Spark and the highly efficient code from StatsForecast to fit millions of models in a couple of minutes.</strong></p>
</blockquote>
<p>Time-series modeling, analysis, and prediction of trends and seasonalities for data collected over time is a rapidly growing category of software applications.</p>
<p>Businesses, from electricity and economics to healthcare analytics, collect time-series data daily to predict patterns and build better data-driven product experiences. For example, temperature and humidity prediction is used in manufacturing to prevent defects, streaming metrics predictions help identify music’s popular artists, and sales forecasting for thousands of SKUs across different locations in the supply chain is used to optimize inventory costs. As data generation increases, the forecasting necessities have evolved from modeling a few time series to predicting millions.</p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p><a href="https://github.com/nixtla">Nixtla</a> is an open-source project focused on state-of-the-art time series forecasting. They have a couple of libraries such as <a href="https://github.com/nixtla/statsforecast">StatsForecast</a> for statistical models, <a href="https://github.com/nixtla/neuralforecast">NeuralForecast</a> for deep learning, and <a href="https://github.com/nixtla/hierarchicalforecast">HierarchicalForecast</a> for forecast aggregations across different levels of hierarchies. These are production-ready time series libraries focused on different modeling techniques.</p>
<p>This article looks at <a href="https://github.com/nixtla/statsforecast">StatsForecast</a>, a lightning-fast forecasting library with statistical and econometrics models. The AutoARIMA model of Nixtla is 20x faster than <a href="http://alkaline-ml.com/pmdarima/">pmdarima</a>, and the ETS (error, trend, seasonal) models performed 4x faster than <a href="https://github.com/statsmodels/statsmodels">statsmodels</a> and are more robust. The benchmarks and code to reproduce can be found <a href="https://github.com/Nixtla/statsforecast#-accuracy---speed">here</a>. A huge part of the performance increase is due to using a JIT compiler called <a href="https://numba.pydata.org/">numba</a> to achieve high speeds.</p>
<p>The faster iteration time means that data scientists can run more experiments and converge to more accurate models faster. It also means that running benchmarks at scale becomes easier.</p>
<p>In this article, we are interested in the scalability of the StatsForecast library in fitting models over <a href="https://spark.apache.org/docs/latest/api/python/index.html">Spark</a> or <a href="https://github.com/dask/dask">Dask</a> using the <a href="https://github.com/fugue-project/fugue/">Fugue</a> library. This combination will allow us to train a huge number of models distributedly over a temporary cluster quickly.</p>
</section>
<section id="experiment-setup" class="level2">
<h2 class="anchored" data-anchor-id="experiment-setup">Experiment Setup</h2>
<p>When dealing with large time series data, users normally have to deal with thousands of logically independent time series (think of telemetry of different users or different product sales). In this case, we can train one big model over all of the series, or we can create one model for each series. Both are valid approaches since the bigger model will pick up trends across the population, while training thousands of models may fit individual series data better.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note: to pick up both the micro and macro trends of the time series population in one model, check the Nixtla <a href="https://github.com/Nixtla/hierarchicalforecast">HierarchicalForecast</a> library, but this is also more computationally expensive and trickier to scale.</p>
</div>
</div>
<p>This article will deal with the scenario where we train a couple of models (AutoARIMA or ETS) per univariate time series. For this setup, we group the full data by time series, and then train each model for each group. The image below illustrates this. The distributed DataFrame can either be a Spark or Dask DataFrame.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://Nixtla.github.io/statsforecast/blog/posts/2022-10-05-distributed-fugue/https:/miro.medium.com/max/1400/0*HbHd-D8XmtN5F2bI.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">AutoARIMA per partition</figcaption><p></p>
</figure>
</div>
<p>Nixtla previously released benchmarks with <a href="https://www.anyscale.com/">Anyscale</a> on distributing this model training on Ray. The setup and results can be found in this <a href="https://www.anyscale.com/blog/how-nixtla-uses-ray-to-accurately-predict-more-than-a-million-time-series">blog</a>. The results are also shown below. It took 2000 cpus to run one million AutoARIMA models in 35 minutes. We’ll compare this against running on Spark.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://Nixtla.github.io/statsforecast/blog/posts/2022-10-05-distributed-fugue/https:/miro.medium.com/max/1400/0*bnlD5NAslUxfTniv.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">StatsForecast on Ray results</figcaption><p></p>
</figure>
</div>
</section>
<section id="statsforecast-code" class="level2">
<h2 class="anchored" data-anchor-id="statsforecast-code">StatsForecast code</h2>
<p>First, we’ll look at the StatsForecast code used to run the AutoARIMA distributedly on <a href="https://docs.ray.io/en/latest/index.html">Ray</a>. This is a simplified version to run the scenario with a one million time series. It is also updated for the recent StatsForecast v1.0.0 release, so it may look a bit different from the code in the previous benchmarks.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> time <span class="im" style="color: #00769E;">import</span> time</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> statsforecast.utils <span class="im" style="color: #00769E;">import</span> generate_series</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> statsforecast.models <span class="im" style="color: #00769E;">import</span> AutoARIMA</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> statsforecast.core <span class="im" style="color: #00769E;">import</span> StatsForecast</span>
<span id="cb1-7"></span>
<span id="cb1-8">series <span class="op" style="color: #5E5E5E;">=</span> generate_series(n_series<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000000</span>, seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10">model <span class="op" style="color: #5E5E5E;">=</span> StatsForecast(df<span class="op" style="color: #5E5E5E;">=</span>series,</span>
<span id="cb1-11">                      models<span class="op" style="color: #5E5E5E;">=</span>[AutoARIMA()], </span>
<span id="cb1-12">                      freq<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'D'</span>, </span>
<span id="cb1-13">                      n_jobs<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb1-14">              ray_address<span class="op" style="color: #5E5E5E;">=</span>ray_address)</span>
<span id="cb1-15"></span>
<span id="cb1-16">init <span class="op" style="color: #5E5E5E;">=</span> time()</span>
<span id="cb1-17">forecasts <span class="op" style="color: #5E5E5E;">=</span> model.forecast(<span class="dv" style="color: #AD0000;">7</span>)</span>
<span id="cb1-18"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'n_series: 1000000 total time: </span><span class="sc" style="color: #5E5E5E;">{</span>(time() <span class="op" style="color: #5E5E5E;">-</span> init) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">60</span><span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
<p>The interface of StatsForecast is very minimal. It is already designed to perform the AutoARIMA on each group of data. Just supplying the ray_address will make this code snippet run distributedly. Without it, n_jobswill indicate the number of parallel processes for forecasting. model.forecast() will do the fit and predict in one step, and the input to this method in the time horizon to forecast.</p>
</section>
<section id="using-fugue-to-run-on-spark-and-dask" class="level2">
<h2 class="anchored" data-anchor-id="using-fugue-to-run-on-spark-and-dask">Using Fugue to run on Spark and Dask</h2>
<p><a href="https://github.com/fugue-project/fugue">Fugue</a> is an abstraction layer that ports Python, Pandas, and SQL code to Spark and Dask. The most minimal interface is the <code>transform()</code> function. This function takes in a function and DataFrame, and brings it to Spark or Dask. We can use the <code>transform()</code> function to bring StatsForecast execution to Spark.</p>
<p>There are two parts to the code below. First, we have the forecast logic defined in the <code>forecast_series</code> function. Some parameters are hardcoded for simplicity. The most important one is that <code>n_jobs=1</code>. This is because Spark or Dask will already serve as the parallelization layer, and having two stages of parallelism can cause resource deadlocks.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> fugue <span class="im" style="color: #00769E;">import</span> transform</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="kw" style="color: #003B4F;">def</span> forecast_series(df: pd.DataFrame, models) <span class="op" style="color: #5E5E5E;">-&gt;</span> pd.DataFrame:</span>
<span id="cb2-4">    tdf <span class="op" style="color: #5E5E5E;">=</span> df.set_index(<span class="st" style="color: #20794D;">"unique_id"</span>)</span>
<span id="cb2-5">    model <span class="op" style="color: #5E5E5E;">=</span> StatsForecast(df<span class="op" style="color: #5E5E5E;">=</span>tdf, models<span class="op" style="color: #5E5E5E;">=</span>models, freq<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'D'</span>, n_jobs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-6">    <span class="cf" style="color: #003B4F;">return</span> model.forecast(<span class="dv" style="color: #AD0000;">7</span>).reset_index()</span>
<span id="cb2-7"></span>
<span id="cb2-8">transform(series.reset_index(),</span>
<span id="cb2-9">          forecast_series,</span>
<span id="cb2-10">          params<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">dict</span>(models<span class="op" style="color: #5E5E5E;">=</span>[AutoARIMA()]),</span>
<span id="cb2-11">          schema<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"unique_id:int, ds:date, AutoARIMA:float"</span>,</span>
<span id="cb2-12">          partition<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"by"</span>: <span class="st" style="color: #20794D;">"unique_id"</span>},</span>
<span id="cb2-13">          engine<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"spark"</span></span>
<span id="cb2-14">          ).show()</span></code></pre></div>
<p>Second, the <code>transform()</code> function is used to apply the <code>forecast_series()</code> function on Spark. The first two arguments are the DataFrame and function to be applied. Output schema is a requirement for Spark, so we need to pass it in, and the partition argument will take care of splitting the time series modelling by <code>unique_id</code>.</p>
<p>This code already works and returns a Spark DataFrame output.</p>
</section>
<section id="nixtlas-fuguebackend" class="level2">
<h2 class="anchored" data-anchor-id="nixtlas-fuguebackend">Nixtla’s FugueBackend</h2>
<p>The <code>transform()</code> above is a general look at what Fugue can do. In practice, the Fugue and Nixtla teams collaborated to add a more native <code>FugueBackend</code> to the StatsForecast library. Along with it is a utility <code>forecast()</code> function to simplify the forecasting interface. Below is an end-to-end example of running StatsForecast on one million time series.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> statsforecast.distributed.utils <span class="im" style="color: #00769E;">import</span> forecast</span>
<span id="cb3-2"><span class="im" style="color: #00769E;">from</span> statsforecast.distributed.fugue <span class="im" style="color: #00769E;">import</span> FugueBackend</span>
<span id="cb3-3"><span class="im" style="color: #00769E;">from</span> statsforecast.models <span class="im" style="color: #00769E;">import</span> AutoARIMA</span>
<span id="cb3-4"><span class="im" style="color: #00769E;">from</span> statsforecast.core <span class="im" style="color: #00769E;">import</span> StatsForecast</span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="im" style="color: #00769E;">from</span> pyspark.sql <span class="im" style="color: #00769E;">import</span> SparkSession</span>
<span id="cb3-7"></span>
<span id="cb3-8">spark <span class="op" style="color: #5E5E5E;">=</span> SparkSession.builder.getOrCreate()</span>
<span id="cb3-9">backend <span class="op" style="color: #5E5E5E;">=</span> FugueBackend(spark, {<span class="st" style="color: #20794D;">"fugue.spark.use_pandas_udf"</span>:<span class="va" style="color: #111111;">True</span>})</span>
<span id="cb3-10"></span>
<span id="cb3-11">forecast(spark.read.parquet(<span class="st" style="color: #20794D;">"/tmp/1m.parquet"</span>), </span>
<span id="cb3-12">         [AutoARIMA()], </span>
<span id="cb3-13">         freq<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"D"</span>, </span>
<span id="cb3-14">         h<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">7</span>, </span>
<span id="cb3-15">         parallel<span class="op" style="color: #5E5E5E;">=</span>backend).toPandas()</span></code></pre></div>
<p>We just need to create the FugueBackend, which takes in a SparkSession and passes it to <code>forecast()</code>. This function can take either a DataFrame or file path to the data. If a file path is provided, it will be loaded with the parallel backend. In this example above, we replaced the file each time we ran the experiment to generate benchmarks.</p>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>It’s also important to note that we can test locally before running the <code>forecast()</code> on full data. All we have to do is not supply anything for the parallel argument; everything will run on Pandas sequentially.</p>
</div>
</div>
</section>
<section id="benchmark-results" class="level2">
<h2 class="anchored" data-anchor-id="benchmark-results">Benchmark Results</h2>
<p>The benchmark results can be seen below. As of the time of this writing, Dask and Ray made recent releases, so only the Spark metrics are up to date. We will make a follow-up article after running these experiments with the updates.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://Nixtla.github.io/statsforecast/blog/posts/2022-10-05-distributed-fugue/https:/miro.medium.com/max/1400/0*2ovS-D5XHQcVQobK.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Spark and Dask benchmarks for StatsForecast at scale</figcaption><p></p>
</figure>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note: The attempt was to use 2000 cpus but we were limited by available compute instances on AWS.</p>
</div>
</div>
<p>The important part here is that AutoARIMA trained one million time series models in less than 15 minutes. The cluster configuration is attached in the appendix. With very few lines of code, we were able to orchestrate the training of these time series models distributedly.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Training thousands of time series models distributedly normally takes a lot of coding with Spark and Dask, but we were able to run these experiments with very few lines of code. Nixtla’s StatsForecast offers the ability to quickly utilize all of the compute resources available to find the best model for each time series. All users need to do is supply a relevant parallel backend (Ray or Fugue) to run on a cluster.</p>
<p>On the scale of one million timeseries, our total training time took 12 minutes for AutoARIMA. This is the equivalent of close to 400 cpu-hours that we ran immediately, allowing data scientists to quickly iterate at scale without having to write the explicit code for parallelization. Because we used an ephemeral cluster, the cost is effectively the same as running this sequentially on an EC2 instance (parallelized over all cores).</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ol type="1">
<li><a href="https://github.com/Nixtla/statsforecast">Nixtla StatsForecast repo</a></li>
<li><a href="https://nixtla.github.io/statsforecast/">StatsForecast docs</a></li>
<li><a href="https://github.com/fugue-project/fugue/">Fugue repo</a></li>
<li><a href="https://fugue-tutorials.readthedocs.io/">Fugue tutorials</a></li>
</ol>
<p>To chat with us:</p>
<ol type="1">
<li><a href="http://slack.fugue.ai/">Fugue Slack</a></li>
<li><a href="https://join.slack.com/t/nixtlaworkspace/shared_invite/zt-135dssye9-fWTzMpv2WBthq8NK0Yvu6A">Nixtla Slack</a></li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<p>For anyone. interested in the cluster configuration, it can be seen below. This will spin up a Databricks cluster. The important thing is the node_type_id that has the machines used.</p>
<pre><code>{
    "num_workers": 20,
    "cluster_name": "fugue-nixtla-2",
    "spark_version": "10.4.x-scala2.12",
    "spark_conf": {
        "spark.speculation": "true",
        "spark.sql.shuffle.partitions": "8000",
        "spark.sql.adaptive.enabled": "false",
        "spark.task.cpus": "1"
    },
    "aws_attributes": {
        "first_on_demand": 1,
        "availability": "SPOT_WITH_FALLBACK",
        "zone_id": "us-west-2c",
        "spot_bid_price_percent": 100,
        "ebs_volume_type": "GENERAL_PURPOSE_SSD",
        "ebs_volume_count": 1,
        "ebs_volume_size": 32
    },
    "node_type_id": "m5.24xlarge",
    "driver_node_type_id": "m5.2xlarge",
    "ssh_public_keys": [],
    "custom_tags": {},
    "spark_env_vars": {
        "MKL_NUM_THREADS": "1",
        "OPENBLAS_NUM_THREADS": "1",
        "VECLIB_MAXIMUM_THREADS": "1",
        "OMP_NUM_THREADS": "1",
        "NUMEXPR_NUM_THREADS": "1"
    },
    "autotermination_minutes": 20,
    "enable_elastic_disk": false,
    "cluster_source": "UI",
    "init_scripts": [],
    "runtime_engine": "STANDARD",
    "cluster_id": "0728-004950-oefym0ss"
}</code></pre>


</section>

<p>Give us a ⭐&nbsp;on <a href="https://github.com/nixtla/statsforecast">Github</a></p> ]]></description>
  <guid>https://Nixtla.github.io/statsforecast/blog/posts/2022-10-05-distributed-fugue/index.html</guid>
  <pubDate>Wed, 05 Oct 2022 00:00:00 GMT</pubDate>
  <media:content url="https://img.shields.io/github/stars/nixtla/statsforecast?label=GitHub%20Stars&amp;style=social" medium="image"/>
</item>
</channel>
</rss>
