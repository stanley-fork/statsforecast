[
  {
    "objectID": "distributed.utils.html",
    "href": "distributed.utils.html",
    "title": "Distributed utils",
    "section": "",
    "text": "source\n\nforecast\n\n forecast (df, models, freq, h, fallback_model=None, X_df=None,\n           level=None,\n           parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\nsource\n\n\ncross_validation\n\n cross_validation (df, models, freq, h, n_windows=1, step_size=1,\n                   test_size=None, input_size=None,\n                   parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "",
    "text": "You can install StatsForecast with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\nVist our Installation Guide for further instructions.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Quick Start",
    "text": "Quick Start\nMinimal Example\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nsf = StatsForecast(\n    models = [AutoARIMA(season_length = 12)],\n    freq = 'M'\n)\n\nsf.fit(df)\nsf.predict(h=12, level=[95])\nGet Started with this quick guide.\nFollow this end-to-end walkthrough for best practices."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Why?",
    "text": "Why?\nCurrent Python alternatives for statistical models are slow, inaccurate and don‚Äôt scale well. So we created a library that can be used to forecast in production environments or as benchmarks. StatsForecast includes an extensive battery of models that can efficiently fit millions of time series."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Features",
    "text": "Features\n\nFastest and most accurate implementations of AutoARIMA, AutoETS, AutoCES, MSTL and Theta in Python.\nOut-of-the-box compatibility with Spark, Dask, and Ray.\nProbabilistic Forecasting and Confidence Intervals.\nSupport for exogenous Variables and static covariates.\nAnomaly Detection.\nFamiliar sklearn syntax: .fit and .predict."
  },
  {
    "objectID": "index.html#highlights",
    "href": "index.html#highlights",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Highlights",
    "text": "Highlights\n\nInclusion of exogenous variables and prediction intervals for ARIMA.\n20x faster than pmdarima.\n1.5x faster than R.\n500x faster than Prophet.\n4x faster than statsmodels.\nCompiled to high performance machine code through numba.\n1,000,000 series in 30 min with ray.\nReplace FB-Prophet in two lines of code and gain speed and accuracy. Check the experiments here.\nFit 10 benchmark models on 1,000,000 series in under 5 min.\n\nMissing something? Please open an issue or write us in"
  },
  {
    "objectID": "index.html#examples-and-guides",
    "href": "index.html#examples-and-guides",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Examples and Guides",
    "text": "Examples and Guides\nüìö End to End Walkthrough: Model training, evaluation and selection for multiple time series\nüîé Anomaly Detection: detect anomalies for time series using in-sample prediction intervals.\nüë©‚Äçüî¨ Cross Validation: robust model‚Äôs performance evaluation.\n‚ùÑÔ∏è Multiple Seasonalities: how to forecast data with multiple seasonalities using an MSTL.\nüîå Predict Demand Peaks: electricity load forecasting for detecting daily peaks and reducing electric bills.\nüìà Intermittent Demand: forecast series with very few non-zero observations.\nüå°Ô∏è Exogenous Regressors: like weather or prices"
  },
  {
    "objectID": "index.html#models",
    "href": "index.html#models",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "Models",
    "text": "Models\n\nAutomatic Forecasting\nAutomatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoETS\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoCES\n‚úÖ\n\n‚úÖ\n\n\n\nAutoTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\nTheta Family\nfit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nOptimizedTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nDynamicTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nDynamicOptimizedTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\nMultiple Seasonalities\nSuited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nMSTL\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\nBaseline Models\nClassical models for establishing baseline.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nHistoricAverage\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nNaive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nRandomWalkWithDrift\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nSeasonalNaive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nWindowAverage\n‚úÖ\n\n\n\n\n\nSeasonalWindowAverage\n‚úÖ\n\n\n\n\n\n\n\n\nExponential Smoothing\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with clear trend and/or seasonality. Use the SimpleExponential family for data with no clear trend or seasonality.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nSimpleExponentialSmoothing\n‚úÖ\n\n\n\n\n\nSimpleExponentialSmoothingOptimized\n‚úÖ\n\n\n\n\n\nSeasonalExponentialSmoothing\n‚úÖ\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n‚úÖ\n\n\n\n\n\nHolt\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nHoltWinters\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n\n\nSparse of Inttermitent\nSuited for series with very few non-zero observations\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nADIDA\n‚úÖ\n\n\n\n\n\nCrostonClassic\n‚úÖ\n\n\n\n\n\nCrostonOptimized\n‚úÖ\n\n\n\n\n\nCrostonSBA\n‚úÖ\n\n\n\n\n\nIMAPA\n‚úÖ\n\n\n\n\n\nTSB\n‚úÖ"
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "StatsForecast ‚ö°Ô∏è",
    "section": "How to contribute",
    "text": "How to contribute\nSee CONTRIBUTING.md."
  },
  {
    "objectID": "theta.html",
    "href": "theta.html",
    "title": "Theta Model",
    "section": "",
    "text": "source\n\n\n\n theta_target_fn (optimal_param, init_level, init_alpha, init_theta,\n                  opt_level, opt_alpha, opt_theta, y, modeltype, nmse)\n\n\nsource\n\n\n\n\n is_constant (x)\n\n\nis_constant(ap)\n\n\nforecast_theta(res, 12, level=[90, 80])\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "StatsForecast Blog",
    "section": "",
    "text": "Scalable Time Series Modeling with open-source projects\n\n\n\n\n\nHow to Forecast 1M Time Series in 15 Minutes with Spark, Fugue and Nixtla‚Äôs Statsforecast.\n\n\n\n\n\n\nOct 5, 2022\n\n\nFugue, Nixtla\n\n\n\n\n\n\nNo matching items\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "",
    "text": "By Fugue and Nixtla. Originally posted on TDS.\nTime-series modeling, analysis, and prediction of trends and seasonalities for data collected over time is a rapidly growing category of software applications.\nBusinesses, from electricity and economics to healthcare analytics, collect time-series data daily to predict patterns and build better data-driven product experiences. For example, temperature and humidity prediction is used in manufacturing to prevent defects, streaming metrics predictions help identify music‚Äôs popular artists, and sales forecasting for thousands of SKUs across different locations in the supply chain is used to optimize inventory costs. As data generation increases, the forecasting necessities have evolved from modeling a few time series to predicting millions.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#motivation",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#motivation",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Motivation",
    "text": "Motivation\nNixtla is an open-source project focused on state-of-the-art time series forecasting. They have a couple of libraries such as StatsForecast for statistical models, NeuralForecast for deep learning, and HierarchicalForecast for forecast aggregations across different levels of hierarchies. These are production-ready time series libraries focused on different modeling techniques.\nThis article looks at StatsForecast, a lightning-fast forecasting library with statistical and econometrics models. The AutoARIMA model of Nixtla is 20x faster than pmdarima, and the ETS (error, trend, seasonal) models performed 4x faster than statsmodels and are more robust. The benchmarks and code to reproduce can be found here. A huge part of the performance increase is due to using a JIT compiler called numba to achieve high speeds.\nThe faster iteration time means that data scientists can run more experiments and converge to more accurate models faster. It also means that running benchmarks at scale becomes easier.\nIn this article, we are interested in the scalability of the StatsForecast library in fitting models over Spark or Dask using the Fugue library. This combination will allow us to train a huge number of models distributedly over a temporary cluster quickly."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#experiment-setup",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#experiment-setup",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Experiment Setup",
    "text": "Experiment Setup\nWhen dealing with large time series data, users normally have to deal with thousands of logically independent time series (think of telemetry of different users or different product sales). In this case, we can train one big model over all of the series, or we can create one model for each series. Both are valid approaches since the bigger model will pick up trends across the population, while training thousands of models may fit individual series data better.\n\n\n\n\n\n\nNote\n\n\n\nNote: to pick up both the micro and macro trends of the time series population in one model, check the Nixtla HierarchicalForecast library, but this is also more computationally expensive and trickier to scale.\n\n\nThis article will deal with the scenario where we train a couple of models (AutoARIMA or ETS) per univariate time series. For this setup, we group the full data by time series, and then train each model for each group. The image below illustrates this. The distributed DataFrame can either be a Spark or Dask DataFrame.\n\n\n\nAutoARIMA per partition\n\n\nNixtla previously released benchmarks with Anyscale on distributing this model training on Ray. The setup and results can be found in this blog. The results are also shown below. It took 2000 cpus to run one million AutoARIMA models in 35 minutes. We‚Äôll compare this against running on Spark.\n\n\n\nStatsForecast on Ray results"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#statsforecast-code",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#statsforecast-code",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "StatsForecast code",
    "text": "StatsForecast code\nFirst, we‚Äôll look at the StatsForecast code used to run the AutoARIMA distributedly on Ray. This is a simplified version to run the scenario with a one million time series. It is also updated for the recent StatsForecast v1.0.0 release, so it may look a bit different from the code in the previous benchmarks.\nfrom time import time\n\nimport pandas as pd\nfrom statsforecast.utils import generate_series\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\nseries = generate_series(n_series=1000000, seed=1)\n\nmodel = StatsForecast(df=series,\n                      models=[AutoARIMA()], \n                      freq='D', \n                      n_jobs=-1,\n              ray_address=ray_address)\n\ninit = time()\nforecasts = model.forecast(7)\nprint(f'n_series: 1000000 total time: {(time() - init) / 60}')\nThe interface of StatsForecast is very minimal. It is already designed to perform the AutoARIMA on each group of data. Just supplying the ray_address will make this code snippet run distributedly. Without it, n_jobswill indicate the number of parallel processes for forecasting. model.forecast() will do the fit and predict in one step, and the input to this method in the time horizon to forecast."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#using-fugue-to-run-on-spark-and-dask",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#using-fugue-to-run-on-spark-and-dask",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Using Fugue to run on Spark and Dask",
    "text": "Using Fugue to run on Spark and Dask\nFugue is an abstraction layer that ports Python, Pandas, and SQL code to Spark and Dask. The most minimal interface is the transform() function. This function takes in a function and DataFrame, and brings it to Spark or Dask. We can use the transform() function to bring StatsForecast execution to Spark.\nThere are two parts to the code below. First, we have the forecast logic defined in the forecast_series function. Some parameters are hardcoded for simplicity. The most important one is that n_jobs=1. This is because Spark or Dask will already serve as the parallelization layer, and having two stages of parallelism can cause resource deadlocks.\nfrom fugue import transform\n\ndef forecast_series(df: pd.DataFrame, models) -> pd.DataFrame:\n    tdf = df.set_index(\"unique_id\")\n    model = StatsForecast(df=tdf, models=models, freq='D', n_jobs=1)\n    return model.forecast(7).reset_index()\n\ntransform(series.reset_index(),\n          forecast_series,\n          params=dict(models=[AutoARIMA()]),\n          schema=\"unique_id:int, ds:date, AutoARIMA:float\",\n          partition={\"by\": \"unique_id\"},\n          engine=\"spark\"\n          ).show()\nSecond, the transform() function is used to apply the forecast_series() function on Spark. The first two arguments are the DataFrame and function to be applied. Output schema is a requirement for Spark, so we need to pass it in, and the partition argument will take care of splitting the time series modelling by unique_id.\nThis code already works and returns a Spark DataFrame output."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#nixtlas-fuguebackend",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#nixtlas-fuguebackend",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Nixtla‚Äôs FugueBackend",
    "text": "Nixtla‚Äôs FugueBackend\nThe transform() above is a general look at what Fugue can do. In practice, the Fugue and Nixtla teams collaborated to add a more native FugueBackend to the StatsForecast library. Along with it is a utility forecast() function to simplify the forecasting interface. Below is an end-to-end example of running StatsForecast on one million time series.\nfrom statsforecast.distributed.utils import forecast\nfrom statsforecast.distributed.fugue import FugueBackend\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nbackend = FugueBackend(spark, {\"fugue.spark.use_pandas_udf\":True})\n\nforecast(spark.read.parquet(\"/tmp/1m.parquet\"), \n         [AutoARIMA()], \n         freq=\"D\", \n         h=7, \n         parallel=backend).toPandas()\nWe just need to create the FugueBackend, which takes in a SparkSession and passes it to forecast(). This function can take either a DataFrame or file path to the data. If a file path is provided, it will be loaded with the parallel backend. In this example above, we replaced the file each time we ran the experiment to generate benchmarks.\n\n\n\n\n\n\nDanger\n\n\n\nIt‚Äôs also important to note that we can test locally before running the forecast() on full data. All we have to do is not supply anything for the parallel argument; everything will run on Pandas sequentially."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#benchmark-results",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#benchmark-results",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Benchmark Results",
    "text": "Benchmark Results\nThe benchmark results can be seen below. As of the time of this writing, Dask and Ray made recent releases, so only the Spark metrics are up to date. We will make a follow-up article after running these experiments with the updates.\n\n\n\nSpark and Dask benchmarks for StatsForecast at scale\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: The attempt was to use 2000 cpus but we were limited by available compute instances on AWS.\n\n\nThe important part here is that AutoARIMA trained one million time series models in less than 15 minutes. The cluster configuration is attached in the appendix. With very few lines of code, we were able to orchestrate the training of these time series models distributedly."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#conclusion",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#conclusion",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Conclusion",
    "text": "Conclusion\nTraining thousands of time series models distributedly normally takes a lot of coding with Spark and Dask, but we were able to run these experiments with very few lines of code. Nixtla‚Äôs StatsForecast offers the ability to quickly utilize all of the compute resources available to find the best model for each time series. All users need to do is supply a relevant parallel backend (Ray or Fugue) to run on a cluster.\nOn the scale of one million timeseries, our total training time took 12 minutes for AutoARIMA. This is the equivalent of close to 400 cpu-hours that we ran immediately, allowing data scientists to quickly iterate at scale without having to write the explicit code for parallelization. Because we used an ephemeral cluster, the cost is effectively the same as running this sequentially on an EC2 instance (parallelized over all cores)."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#resources",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#resources",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Resources",
    "text": "Resources\n\nNixtla StatsForecast repo\nStatsForecast docs\nFugue repo\nFugue tutorials\n\nTo chat with us:\n\nFugue Slack\nNixtla Slack"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#appendix",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#appendix",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Appendix",
    "text": "Appendix\nFor anyone. interested in the cluster configuration, it can be seen below. This will spin up a Databricks cluster. The important thing is the node_type_id that has the machines used.\n{\n    \"num_workers\": 20,\n    \"cluster_name\": \"fugue-nixtla-2\",\n    \"spark_version\": \"10.4.x-scala2.12\",\n    \"spark_conf\": {\n        \"spark.speculation\": \"true\",\n        \"spark.sql.shuffle.partitions\": \"8000\",\n        \"spark.sql.adaptive.enabled\": \"false\",\n        \"spark.task.cpus\": \"1\"\n    },\n    \"aws_attributes\": {\n        \"first_on_demand\": 1,\n        \"availability\": \"SPOT_WITH_FALLBACK\",\n        \"zone_id\": \"us-west-2c\",\n        \"spot_bid_price_percent\": 100,\n        \"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n        \"ebs_volume_count\": 1,\n        \"ebs_volume_size\": 32\n    },\n    \"node_type_id\": \"m5.24xlarge\",\n    \"driver_node_type_id\": \"m5.2xlarge\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"MKL_NUM_THREADS\": \"1\",\n        \"OPENBLAS_NUM_THREADS\": \"1\",\n        \"VECLIB_MAXIMUM_THREADS\": \"1\",\n        \"OMP_NUM_THREADS\": \"1\",\n        \"NUMEXPR_NUM_THREADS\": \"1\"\n    },\n    \"autotermination_minutes\": 20,\n    \"enable_elastic_disk\": false,\n    \"cluster_source\": \"UI\",\n    \"init_scripts\": [],\n    \"runtime_engine\": \"STANDARD\",\n    \"cluster_id\": \"0728-004950-oefym0ss\"\n}"
  },
  {
    "objectID": "examples/electricityloadforecasting.html",
    "href": "examples/electricityloadforecasting.html",
    "title": "Electricity load forecast",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/electricityloadforecasting.html#introduction",
    "href": "examples/electricityloadforecasting.html#introduction",
    "title": "Electricity load forecast",
    "section": "Introduction",
    "text": "Introduction\nSome time series are generated from very low frequency data. These data generally exhibit multiple seasonalities. For example, hourly data may exhibit repeated patterns every hour (every 24 observations) or every day (every 24 * 7, hours per day, observations). This is the case for electricity load. Electricity load may vary hourly, e.g., during the evenings electricity consumption may be expected to increase. But also, the electricity load varies by week. Perhaps on weekends there is an increase in electrical activity.\nIn this example we will show how to model the two seasonalities of the time series to generate accurate forecasts in a short time. We will use hourly PJM electricity load data. The original data can be found here."
  },
  {
    "objectID": "examples/electricityloadforecasting.html#libraries",
    "href": "examples/electricityloadforecasting.html#libraries",
    "title": "Electricity load forecast",
    "section": "Libraries",
    "text": "Libraries\nIn this example we will use the following libraries: - [StatsForecast](https://Nixtla.github.io/statsforecast/core.html#statsforecast). Lightning ‚ö°Ô∏è fast forecasting with statistical and econometric models. Includes the MSTL model for multiple seasonalities. - DatasetsForecast. Used to evaluate the performance of the forecasts. - Prophet. Benchmark model developed by Facebook. - NeuralProphet. Deep Learning version of Prophet. Used as benchark.\n\n!pip install statsforecast\n!pip install datasetsforecast\n!pip install prophet\n!pip install \"neuralprophet[live]\""
  },
  {
    "objectID": "examples/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "href": "examples/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "title": "Electricity load forecast",
    "section": "Forecast using Multiple Seasonalities",
    "text": "Forecast using Multiple Seasonalities\n\nElectricity Load Data\nAccording to the dataset‚Äôs page,\n\nPJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia. The hourly power consumption data comes from PJM‚Äôs website and are in megawatts (MW).\n\nLet‚Äôs take a look to the data.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\npd.plotting.register_matplotlib_converters()\nplt.rc(\"figure\", figsize=(10, 8))\nplt.rc(\"font\", size=10)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/jnagura/Energy-consumption-prediction-analysis/master/PJM_Load_hourly.csv')\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf['ds'] = pd.to_datetime(df['ds'])\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\ndf.tail()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      32891\n      PJM_Load_hourly\n      2001-12-31 20:00:00\n      36392.0\n    \n    \n      32892\n      PJM_Load_hourly\n      2001-12-31 21:00:00\n      35082.0\n    \n    \n      32893\n      PJM_Load_hourly\n      2001-12-31 22:00:00\n      33890.0\n    \n    \n      32894\n      PJM_Load_hourly\n      2001-12-31 23:00:00\n      32590.0\n    \n    \n      32895\n      PJM_Load_hourly\n      2002-01-01 00:00:00\n      31569.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.plot(x='ds', y='y')\n\n<matplotlib.axes._subplots.AxesSubplot>\n\n\n\n\n\nWe clearly observe that the time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods to display them in production.\n\n\nMSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\nStatsForecast contains a fast implementation of the MSTL model. Also, the decomposition of the time series can be calculated.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA, SeasonalNaive\nfrom statsforecast.utils import AirPassengers as ap\n\n/usr/local/lib/python3.7/dist-packages/statsforecast/core.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\nFirst we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities that the MSTL model receives. We must also specify the manner in which the trend will be forecasted. In this case we will use the AutoARIMA model.\n\nmstl = MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA() # model used to forecast trend\n)\n\nOnce the model is instantiated, we have to instantiate the StatsForecast class to create forecasts.\n\nsf = StatsForecast(\n    models=[mstl], # model used to fit each time series \n    freq='H', # frequency of the data\n)\n\n\nFit the model\nAfer that, we just have to use the fit method to fit each model to each time series.\n\nsf = sf.fit(df=df)\n\n\n\nDecompose the time series in multiple seasonalities\nOnce the model is fitted, we can access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      data\n      trend\n      seasonal24\n      seasonal168\n      remainder\n    \n  \n  \n    \n      0\n      22259.0\n      26183.898892\n      -5215.124554\n      609.000432\n      681.225229\n    \n    \n      1\n      21244.0\n      26181.599305\n      -6255.673234\n      603.823918\n      714.250011\n    \n    \n      2\n      20651.0\n      26179.294886\n      -6905.329895\n      636.820423\n      740.214587\n    \n    \n      3\n      20421.0\n      26176.985472\n      -7073.420118\n      615.825999\n      701.608647\n    \n    \n      4\n      20713.0\n      26174.670877\n      -7062.395760\n      991.521912\n      609.202971\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32891\n      36392.0\n      33123.552727\n      4387.149171\n      -488.177882\n      -630.524015\n    \n    \n      32892\n      35082.0\n      33148.242575\n      3479.852929\n      -682.928737\n      -863.166767\n    \n    \n      32893\n      33890.0\n      33172.926165\n      2307.808829\n      -650.566775\n      -940.168219\n    \n    \n      32894\n      32590.0\n      33197.603322\n      748.587723\n      -555.177849\n      -801.013195\n    \n    \n      32895\n      31569.0\n      33222.273902\n      -967.124123\n      -265.895357\n      -420.254422\n    \n  \n\n32896 rows √ó 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLet‚Äôs look graphically at the different components of the time series.\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe observe that there is a clear trend towards the high (orange line). This component would be predicted with the AutoARIMA model. We can also observe that every 24 hours and every 24 * 7 hours there is a very well defined pattern. These two components will be forecast separately using a SeasonalNaive model.\n\n\nProduce forecasts\nTo generate forecasts we only have to use the predict method specifying the forecast horizon (h). In addition, to calculate prediction intervals associated to the forecasts, we can include the parameter level that receives a list of levels of the prediction intervals we want to build. In this case we will only calculate the 90% forecast interval (level=[90]).\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ds\n      MSTL\n      MSTL-lo-90\n      MSTL-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      PJM_Load_hourly\n      2002-01-01 01:00:00\n      29956.744141\n      29585.187500\n      30328.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 02:00:00\n      29057.691406\n      28407.498047\n      29707.884766\n    \n    \n      PJM_Load_hourly\n      2002-01-01 03:00:00\n      28654.699219\n      27767.101562\n      29542.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 04:00:00\n      28499.009766\n      27407.640625\n      29590.378906\n    \n    \n      PJM_Load_hourly\n      2002-01-01 05:00:00\n      28821.716797\n      27552.236328\n      30091.197266\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLet‚Äôs look at our forecasts graphically.\n\n_, ax = plt.subplots(1, 1, figsize = (20, 7))\ndf_plot = pd.concat([df, forecasts]).set_index('ds').tail(24 * 7)\ndf_plot[['y', 'MSTL']].plot(ax=ax, linewidth=2)\nax.fill_between(df_plot.index, \n                df_plot['MSTL-lo-90'], \n                df_plot['MSTL-hi-90'],\n                alpha=.35,\n                color='orange',\n                label='MSTL-level-90')\nax.set_title('PJM Load Hourly', fontsize=22)\nax.set_ylabel('Electricity Load', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\nIn the next section we will plot different models so it is convenient to reuse the previous code with the following function.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(24 * 7)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2)\n    colors = ['orange', 'green', 'red']\n    for model, color in zip(models, colors):\n        ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-90'], \n                        df_plot[f'{model}-hi-90'],\n                        alpha=.35,\n                        color=color,\n                        label=f'{model}-level-90')\n    ax.set_title('PJM Load Hourly', fontsize=22)\n    ax.set_ylabel('Electricity Load', fontsize=20)\n    ax.set_xlabel('Timestamp [t]', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid()\n\n\n\n\nPerformance of the MSTL model\n\nSplit Train/Test sets\nTo validate the accuracy of the MSTL model, we will show its performance on unseen data. We will use a classical time series technique that consists of dividing the data into a training set and a test set. We will leave the last 24 observations (the last day) as the test set. So the model will train on 32,872 observations.\n\ndf_test = df.tail(24)\ndf_train = df.drop(df_test.index)\n\n\n\nMSTL model\nIn addition to the MSTL model, we will include the SeasonalNaive model as a benchmark to validate the added value of the MSTL model. Including StatsForecast models is as simple as adding them to the list of models to be fitted.\n\nsf = StatsForecast(\n    models=[mstl, SeasonalNaive(season_length=24)], # add SeasonalNaive model to the list\n    freq='H'\n)\n\nTo measure the fitting time we will use the time module.\n\nfrom time import time\n\nTo retrieve the forecasts of the test set we only have to do fit and predict as before.\n\ninit = time()\nsf = sf.fit(df=df_train)\nforecasts_test = sf.predict(h=len(df_test), level=[90])\nend = time()\nforecasts_test.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ds\n      MSTL\n      MSTL-lo-90\n      MSTL-hi-90\n      SeasonalNaive\n      SeasonalNaive-lo-90\n      SeasonalNaive-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      PJM_Load_hourly\n      2001-12-31 01:00:00\n      28345.212891\n      27973.570312\n      28716.853516\n      28326.0\n      23468.693359\n      33183.308594\n    \n    \n      PJM_Load_hourly\n      2001-12-31 02:00:00\n      27567.455078\n      26917.085938\n      28217.824219\n      27362.0\n      22504.693359\n      32219.306641\n    \n    \n      PJM_Load_hourly\n      2001-12-31 03:00:00\n      27260.001953\n      26372.138672\n      28147.865234\n      27108.0\n      22250.693359\n      31965.306641\n    \n    \n      PJM_Load_hourly\n      2001-12-31 04:00:00\n      27328.125000\n      26236.410156\n      28419.839844\n      26865.0\n      22007.693359\n      31722.306641\n    \n    \n      PJM_Load_hourly\n      2001-12-31 05:00:00\n      27640.673828\n      26370.773438\n      28910.572266\n      26808.0\n      21950.693359\n      31665.306641\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntime_mstl = (end - init) / 60\nprint(f'MSTL Time: {time_mstl:.2f} minutes')\n\nMSTL Time: 0.43 minutes\n\n\nThen we were able to generate forecasts for the next 24 hours. Now let‚Äôs look at the graphical comparison of the forecasts with the actual values.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n\nLet‚Äôs look at those produced only by MSTL.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL'])\n\n\n\n\nWe note that MSTL produces very accurate forecasts that follow the behavior of the time series. Now let us calculate numerically the accuracy of the model. We will use the following metrics: MAE, MAPE, MASE, RMSE, SMAPE.\n\nfrom datasetsforecast.losses import (\n    mae, mape, mase, rmse, smape\n)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, models):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    for model in models:\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                 y_true[model].values, \n                                                 y_hist['y'].values, seasonality=24)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mase\n      mae\n      mape\n      rmse\n      smape\n    \n  \n  \n    \n      MSTL\n      0.341926\n      709.932048\n      2.182804\n      892.888012\n      2.162832\n    \n    \n      SeasonalNaive\n      0.894653\n      1857.541667\n      5.648190\n      2201.384101\n      5.868604\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe observe that MSTL has an improvement of about 60% over the SeasonalNaive method in the test set measured in MASE.\n\n\nComparison with Prophet\nOne of the most widely used models for time series forecasting is Prophet. This model is known for its ability to model different seasonalities (weekly, daily yearly). We will use this model as a benchmark to see if the MSTL adds value for this time series.\n\nfrom prophet import Prophet\n\n# create prophet model\nprophet = Prophet(interval_width=0.9)\ninit = time()\nprophet.fit(df_train)\n# produce forecasts\nfuture = prophet.make_future_dataframe(periods=len(df_test), freq='H', include_history=False)\nforecast_prophet = prophet.predict(future)\nend = time()\n# data wrangling\nforecast_prophet = forecast_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\nforecast_prophet.columns = ['ds', 'Prophet', 'Prophet-lo-90', 'Prophet-hi-90']\nforecast_prophet.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_prophet.head()\n\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpc6f7_v9l/08tirxvt.json\nDEBUG:cmdstanpy:input tempfile: /tmp/tmpc6f7_v9l/gp60b_b1.json\nDEBUG:cmdstanpy:idx 0\nDEBUG:cmdstanpy:running CmdStan, num_threads: None\nDEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.7/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=27632', 'data', 'file=/tmp/tmpc6f7_v9l/08tirxvt.json', 'init=/tmp/tmpc6f7_v9l/gp60b_b1.json', 'output', 'file=/tmp/tmpc6f7_v9l/prophet_modelkou9navr/prophet_model-20221111002506.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n00:25:06 - cmdstanpy - INFO - Chain [1] start processing\nINFO:cmdstanpy:Chain [1] start processing\n00:25:58 - cmdstanpy - INFO - Chain [1] done processing\nINFO:cmdstanpy:Chain [1] done processing\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      unique_id\n      ds\n      Prophet\n      Prophet-lo-90\n      Prophet-hi-90\n    \n  \n  \n    \n      0\n      PJM_Load_hourly\n      2001-12-31 01:00:00\n      25255.037078\n      20589.007557\n      30472.856691\n    \n    \n      1\n      PJM_Load_hourly\n      2001-12-31 02:00:00\n      23961.427979\n      19078.763798\n      28752.007690\n    \n    \n      2\n      PJM_Load_hourly\n      2001-12-31 03:00:00\n      23285.418846\n      18491.391041\n      28213.042892\n    \n    \n      3\n      PJM_Load_hourly\n      2001-12-31 04:00:00\n      23293.143264\n      18620.414846\n      28364.115196\n    \n    \n      4\n      PJM_Load_hourly\n      2001-12-31 05:00:00\n      24067.737572\n      19062.318724\n      29162.041606\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntime_prophet = (end - init) / 60\nprint(f'Prophet Time: {time_prophet:.2f} minutes')\n\nProphet Time: 0.93 minutes\n\n\n\ntimes = pd.DataFrame({'model': ['MSTL', 'Prophet'], 'time (mins)': [time_mstl, time_prophet]})\ntimes\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      model\n      time (mins)\n    \n  \n  \n    \n      0\n      MSTL\n      0.425080\n    \n    \n      1\n      Prophet\n      0.928628\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe observe that the time required for Prophet to perform the fit and predict pipeline is greater than MSTL. Let‚Äôs look at the forecasts produced by Prophet.\n\nforecasts_test = forecasts_test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive', 'Prophet'])\n\n\n\n\nWe note that Prophet is able to capture the overall behavior of the time series. However, in some cases it produces forecasts well below the actual value. It also does not correctly adjust the valleys.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'Prophet', 'SeasonalNaive'])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mase\n      mae\n      mape\n      rmse\n      smape\n    \n  \n  \n    \n      MSTL\n      0.341926\n      709.932048\n      2.182804\n      892.888012\n      2.162832\n    \n    \n      Prophet\n      1.107472\n      2299.413375\n      7.427523\n      2742.792022\n      7.789355\n    \n    \n      SeasonalNaive\n      0.894653\n      1857.541667\n      5.648190\n      2201.384101\n      5.868604\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nIn terms of accuracy, Prophet is not able to produce better forecasts than the SeasonalNaive model, however, the MSTL model improves Prophet‚Äôs forecasts by 69% (MASE).\n\n\nComparison with NeuralProphet\nNeuralProphet is the version of Prophet using deep learning. This model is also capable of handling different seasonalities so we will also use it as a benchmark.\n\nfrom neuralprophet import NeuralProphet\n\nneuralprophet = NeuralProphet(quantiles=[0.05, 0.95])\ninit = time()\nneuralprophet.fit(df_train.drop(columns='unique_id'))\nfuture = neuralprophet.make_future_dataframe(df=df_train.drop(columns='unique_id'), periods=len(df_test))\nforecast_np = neuralprophet.predict(future)\nend = time()\nforecast_np = forecast_np[['ds', 'yhat1', 'yhat1 5.0%', 'yhat1 95.0%']]\nforecast_np.columns = ['ds', 'NeuralProphet', 'NeuralProphet-lo-90', 'NeuralProphet-hi-90']\nforecast_np.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_np.head()\n\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\nINFO:NP.df_utils:Major frequency H corresponds to 99.973% of the data.\nINFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as H\nINFO:NP.df_utils:Dataframe freq automatically defined as H\nINFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\nINFO:NP.config:Setting normalization to global as only one dataframe provided for training.\nINFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\nINFO:NP.config:Auto-set batch_size to 64\nINFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 76\nINFO:NP.config:Auto-set epochs to 76\n\n\n\n\n\nWARNING - (py.warnings._showwarnmsg) - /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:922: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n\nWARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:922: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n\nWARNING - (py.warnings._showwarnmsg) - /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:922: UserWarning: Using a target size (torch.Size([324, 1, 1])) that is different to the input size (torch.Size([324, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n\nWARNING:py.warnings:/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:922: UserWarning: Using a target size (torch.Size([324, 1, 1])) that is different to the input size (torch.Size([324, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n\nINFO - (NP.utils_torch.lr_range_test) - lr-range-test results: steep: 3.89E-02, min: 2.68E-01\nINFO:NP.utils_torch:lr-range-test results: steep: 3.89E-02, min: 2.68E-01\n\n\n\n\n\nINFO - (NP.utils_torch.lr_range_test) - lr-range-test results: steep: 5.03E-02, min: 1.82E-01\nINFO:NP.utils_torch:lr-range-test results: steep: 5.03E-02, min: 1.82E-01\nINFO - (NP.forecaster._init_train_loader) - lr-range-test selected learning rate: 5.81E-02\nINFO:NP.forecaster:lr-range-test selected learning rate: 5.81E-02\nEpoch[76/76]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76/76 [02:17<00:00,  1.81s/it, SmoothL1Loss=0.00872, MAE=2.27e+3, RMSE=3.03e+3, Loss=0.00862, RegLoss=0]\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\nINFO:NP.df_utils:Major frequency H corresponds to 99.973% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO:NP.df_utils:Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\nINFO:NP.df_utils:Returning df with no ID column\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 95.833% of the data.\nINFO:NP.df_utils:Major frequency H corresponds to 95.833% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO:NP.df_utils:Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 95.833% of the data.\nINFO:NP.df_utils:Major frequency H corresponds to 95.833% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO:NP.df_utils:Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\nINFO:NP.df_utils:Returning df with no ID column\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      unique_id\n      ds\n      NeuralProphet\n      NeuralProphet-lo-90\n      NeuralProphet-hi-90\n    \n  \n  \n    \n      0\n      PJM_Load_hourly\n      2001-12-31 01:00:00\n      25015.931641\n      22304.785156\n      27442.351562\n    \n    \n      1\n      PJM_Load_hourly\n      2001-12-31 02:00:00\n      24125.677734\n      21462.275391\n      26571.785156\n    \n    \n      2\n      PJM_Load_hourly\n      2001-12-31 03:00:00\n      23730.714844\n      20973.000000\n      26321.234375\n    \n    \n      3\n      PJM_Load_hourly\n      2001-12-31 04:00:00\n      23469.591797\n      20735.953125\n      26097.132812\n    \n    \n      4\n      PJM_Load_hourly\n      2001-12-31 05:00:00\n      23887.208984\n      21221.152344\n      26459.939453\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntime_np = (end - init) / 60\nprint(f'Prophet Time: {time_np:.2f} minutes')\n\nProphet Time: 2.39 minutes\n\n\n\ntimes = times.append({'model': 'NeuralProphet', 'time (mins)': time_np}, ignore_index=True)\ntimes\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      model\n      time (mins)\n    \n  \n  \n    \n      0\n      MSTL\n      0.425080\n    \n    \n      1\n      Prophet\n      0.928628\n    \n    \n      2\n      NeuralProphet\n      2.393841\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe observe that NeuralProphet requires a longer processing time than Prophet and MSTL.\n\nforecasts_test = forecasts_test.merge(forecast_np, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'NeuralProphet', 'Prophet'])\n\n\n\n\nThe forecasts graph shows that NeuralProphet generates very similar results to Prophet, as expected.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'NeuralProphet', 'Prophet', 'SeasonalNaive'])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mase\n      mae\n      mape\n      rmse\n      smape\n    \n  \n  \n    \n      MSTL\n      0.341926\n      709.932048\n      2.182804\n      892.888012\n      2.162832\n    \n    \n      NeuralProphet\n      1.089964\n      2263.060303\n      7.310210\n      2681.742759\n      7.651353\n    \n    \n      Prophet\n      1.107472\n      2299.413375\n      7.427523\n      2742.792022\n      7.789355\n    \n    \n      SeasonalNaive\n      0.894653\n      1857.541667\n      5.648190\n      2201.384101\n      5.868604\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWith respect to numerical evaluation, NeuralProphet improves the results of Prohet, as expected, however, MSTL improves over NeuralProphet‚Äôs foreacasts by 68% (MASE).\n\n\n\n\n\n\nImportant\n\n\n\nThe performance of NeuralProphet can be improved using hyperparameter optimization, which can increase the fitting time significantly. In this example we show its performance with the default version."
  },
  {
    "objectID": "examples/electricityloadforecasting.html#conclusion",
    "href": "examples/electricityloadforecasting.html#conclusion",
    "title": "Electricity load forecast",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we introduced MSTL, a model originally developed by Kasun Bandara, Rob Hyndman and Christoph Bergmeir capable of handling time series with multiple seasonalities. We also showed that for the PJM electricity load time series offers better performance in time and accuracy than the Prophet and NeuralProphet models."
  },
  {
    "objectID": "examples/electricityloadforecasting.html#references",
    "href": "examples/electricityloadforecasting.html#references",
    "title": "Electricity load forecast",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). ‚ÄúMSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns‚Äù."
  },
  {
    "objectID": "examples/contributing.html",
    "href": "examples/contributing.html",
    "title": "How to Contribute",
    "section": "",
    "text": "Ensure the bug was not already reported by searching on GitHub under Issues.\nIf you‚Äôre unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/contributing.html#do-you-have-a-feature-request",
    "href": "examples/contributing.html#do-you-have-a-feature-request",
    "title": "How to Contribute",
    "section": "Do you have a feature request?",
    "text": "Do you have a feature request?\n\nEnsure that it hasn‚Äôt been yet implemented in the main branch of the repository and that there‚Äôs not an Issue requesting it yet.\nOpen a new issue and make sure to describe it clearly, mention how it improves the project and why its useful."
  },
  {
    "objectID": "examples/contributing.html#do-you-want-to-fix-a-bug-or-implement-a-feature",
    "href": "examples/contributing.html#do-you-want-to-fix-a-bug-or-implement-a-feature",
    "title": "How to Contribute",
    "section": "Do you want to fix a bug or implement a feature?",
    "text": "Do you want to fix a bug or implement a feature?\nBug fixes and features are added through pull requests (PRs)."
  },
  {
    "objectID": "examples/contributing.html#pr-submission-guidelines",
    "href": "examples/contributing.html#pr-submission-guidelines",
    "title": "How to Contribute",
    "section": "PR submission guidelines",
    "text": "PR submission guidelines\n\nKeep each PR focused. While it‚Äôs more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nEnsure that your PR includes a test that fails without your patch, and passes with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\nDo not mix style changes/fixes with ‚Äúfunctional‚Äù changes. It‚Äôs very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won‚Äôt need to review the whole PR again. In the exception case where you realize it‚Äôll take many many commits to complete the requests, then it‚Äôs probably best to close the PR, do the work and then submit it again. Use common sense where you‚Äôd choose one way over another.\n\n\nLocal setup for working on a PR\n\n1. Clone the repository\n\nHTTPS: git clone https://github.com/Nixtla/statsforecast.git\nSSH: git clone git@github.com:Nixtla/statsforecast.git\nGitHub CLI: gh repo clone Nixtla/statsforecast\n\n\n\n2. Set up a conda environment\nThe repo comes with an environment.yml file which contains the libraries needed to run all the tests. In order to set up the environment you must have conda installed, we recommend miniconda.\nOnce you have conda go to the top level directory of the repository and run:\nconda env create -f environment.yml\n\n\n3. Install the library\nOnce you have your environment setup, activate it using conda activate statsforecast and then install the library in editable mode using pip install -e \".[dev]\"\n\n\n4. Install git hooks\nBefore doing any changes to the code, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts).\nnbdev_install_hooks\n\n\n\nBuilding the library\nThe library is built using the notebooks contained in the nbs folder. If you want to make any changes to the library you have to find the relevant notebook, make your changes and then call nbdev_export.\n\n\nRunning tests\nIf you‚Äôre working on the local interface you can just use nbdev_test.\n\n\nLinters\nThis project uses a couple of linters to validate different aspects of the code. Before opening a PR, please make sure that it passes all the linting tasks by following the next steps.\n\nRun the linting tasks\n\nmypy statsforecast/\nflake8 --select=F statsforecast/\n\n\n\n\nCleaning notebooks\nSince the notebooks output cells can vary from run to run (even if they produce the same outputs) the notebooks are cleaned before committing them. Please make sure to run nbdev_clean before committing your changes."
  },
  {
    "objectID": "examples/contributing.html#do-you-want-to-contribute-to-the-documentation",
    "href": "examples/contributing.html#do-you-want-to-contribute-to-the-documentation",
    "title": "How to Contribute",
    "section": "Do you want to contribute to the documentation?",
    "text": "Do you want to contribute to the documentation?\n\nDocs are automatically created from the notebooks in the nbs folder.\nIn order to modify the documentation:\n\nFind the relevant notebook.\nMake your changes.\nRun all cells.\nIf you are modifying library notebooks (not in nbs/examples), clean all outputs using Edit > Clear All Outputs.\nRun nbdev_preview."
  },
  {
    "objectID": "examples/uncertaintyintervals.html",
    "href": "examples/uncertaintyintervals.html",
    "title": "Prediction Intervals",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#introduction",
    "href": "examples/uncertaintyintervals.html#introduction",
    "title": "Prediction Intervals",
    "section": "Introduction",
    "text": "Introduction\nIf we only produce point forecasts, there is no way to tell how much uncertainty there is around our forecasts. A prediction interval gives us a measurement of this uncertainty with some prespecified probability o confidence level. In this notebook, we‚Äôll see how StatsForecast allows us to easily generate prediction intervals for multiple forecasting models.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the terms are often confused, prediction intervals are not the same as confidence intervals.\n\n\nOutline:\n\nInstall libraries\nLoad and explore the data\nPrepare dataset for StatsForecast modeling\nTrain the models\nPlot prediction intervals\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#install-libraries",
    "href": "examples/uncertaintyintervals.html#install-libraries",
    "title": "Prediction Intervals",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast\n\n\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, Naive, SeasonalNaive, RandomWalkWithDrift, HistoricAverage"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#load-and-explore-the-data",
    "href": "examples/uncertaintyintervals.html#load-and-explore-the-data",
    "title": "Prediction Intervals",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we‚Äôll use the Hourly dataset from the M4 Competition\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\n\n\ntrain = pd.read_csv('M4-Hourly.csv')\ntest = pd.read_csv('M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\nSince the goal of this notebook is to show how to generate prediction intervals, we‚Äôll use only a subset of the data to reduce the total computational time.\n\nn_series = 8\nuids = train['unique_id'].unique()[:n_series]\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\n\nStatsForecast.plot(train, test, plot_random=False)"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#prepare-dataset-for-statsforecast-modelling",
    "href": "examples/uncertaintyintervals.html#prepare-dataset-for-statsforecast-modelling",
    "title": "Prediction Intervals",
    "section": "Prepare dataset for StatsForecast modelling",
    "text": "Prepare dataset for StatsForecast modelling\nIn this particular case, the data already has the format requiered by StatsForecast\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n    \n    \n      1\n      H1\n      2\n      586.0\n    \n    \n      2\n      H1\n      3\n      586.0\n    \n    \n      3\n      H1\n      4\n      559.0\n    \n    \n      4\n      H1\n      5\n      511.0\n    \n  \n\n\n\n\nAs a reminder, the input to StatsForecast is always a data frame in long format with three columns: unique_id, ds, and y.\n\nThe unique_id (string, int, or category) column is a unique identifier for the series.\nThe ds (int or datestamp) column is either an integer indexing time or a datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS.\nThe y (numeric) column is the measurement we want to forecast, in this case, the number of trips."
  },
  {
    "objectID": "examples/uncertaintyintervals.html#train-models",
    "href": "examples/uncertaintyintervals.html#train-models",
    "title": "Prediction Intervals",
    "section": "Train models",
    "text": "Train models\nTo train multiple models with their respective prediction intervals using StatsForecast, we‚Äôll need to:\n\nCreate a list of models we want to use.\nFit the models by instantiating a new StatsForecastobject.\nSelect the confidence levels associated to the prediction intervals.\nGenerate the forecasts.\n\n\nmodels = [\n    AutoARIMA(season_length=24, approximation=True),\n    Naive(),\n    SeasonalNaive(season_length=24),\n    RandomWalkWithDrift(),\n    HistoricAverage()\n]\n\n\n\n\n\n\n\nTip\n\n\n\nFor this example, we selected AutoARIMA and several baseline models:\n\nNaive\nSeasonalNaive\nRandomWalkWithDrift\nHistoricAverage.\n\n\n\n\nfcst = StatsForecast(df=train, \n                     models=models, \n                     freq='H', \n                     n_jobs=-1)\n\n\nlevels = [80, 90, 95, 99] # for the prediction intervals\n\n\nforecasts = fcst.forecast(h=48, level=levels)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoARIMA\n      AutoARIMA-lo-99\n      AutoARIMA-lo-95\n      AutoARIMA-lo-90\n      AutoARIMA-lo-80\n      AutoARIMA-hi-80\n      AutoARIMA-hi-90\n      AutoARIMA-hi-95\n      ...\n      RWD-hi-99\n      HistoricAverage\n      HistoricAverage-lo-80\n      HistoricAverage-lo-90\n      HistoricAverage-lo-95\n      HistoricAverage-lo-99\n      HistoricAverage-hi-80\n      HistoricAverage-hi-90\n      HistoricAverage-hi-95\n      HistoricAverage-hi-99\n    \n  \n  \n    \n      0\n      H1\n      701\n      616.084167\n      585.106445\n      592.513000\n      596.302612\n      600.671814\n      631.496460\n      635.865662\n      639.655273\n      ...\n      789.416626\n      638.488586\n      436.697418\n      379.492432\n      329.875641\n      232.90242\n      840.279724\n      897.484741\n      947.101562\n      1044.074707\n    \n    \n      1\n      H1\n      702\n      544.432129\n      494.394348\n      506.358063\n      512.479370\n      519.536865\n      569.327393\n      576.384888\n      582.506165\n      ...\n      833.254150\n      638.488586\n      436.697418\n      379.492432\n      329.875641\n      232.90242\n      840.279724\n      897.484741\n      947.101562\n      1044.074707\n    \n    \n      2\n      H1\n      703\n      510.414490\n      443.625366\n      459.594238\n      467.764801\n      477.184906\n      543.644043\n      553.064148\n      561.234741\n      ...\n      866.990601\n      638.488586\n      436.697418\n      379.492432\n      329.875641\n      232.90242\n      840.279724\n      897.484741\n      947.101562\n      1044.074707\n    \n    \n      3\n      H1\n      704\n      481.046539\n      404.228729\n      422.595398\n      431.992798\n      442.827393\n      519.265686\n      530.100281\n      539.497681\n      ...\n      895.510132\n      638.488586\n      436.697418\n      379.492432\n      329.875641\n      232.90242\n      840.279724\n      897.484741\n      947.101562\n      1044.074707\n    \n    \n      4\n      H1\n      705\n      460.893066\n      378.863678\n      398.476410\n      408.511383\n      420.081024\n      501.705109\n      513.274780\n      523.309692\n      ...\n      920.702881\n      638.488586\n      436.697418\n      379.492432\n      329.875641\n      232.90242\n      840.279724\n      897.484741\n      947.101562\n      1044.074707\n    \n  \n\n5 rows √ó 47 columns\n\n\n\nOnce the forecasts are ready, we‚Äôll merge them with the actual values. In the next section, we‚Äôll use this dataframe to visualize the forecasts and their respective prediction intervals.\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "examples/uncertaintyintervals.html#plot-prediction-intervals",
    "href": "examples/uncertaintyintervals.html#plot-prediction-intervals",
    "title": "Prediction Intervals",
    "section": "Plot prediction intervals",
    "text": "Plot prediction intervals\nWe‚Äôll plot each baseline model separately to better visualize their prediction intervals.\n\nAutoARIMA\n\nStatsForecast.plot(train, test, level=levels, models=['AutoARIMA'], plot_random=False)\n\n\n                                                \n\n\n\n\nNaive\n\nStatsForecast.plot(train, test, level=levels, models=['Naive'], plot_random=False)\n\n\n                                                \n\n\n\n\nSeasonal Naive\n\nStatsForecast.plot(train, test, level=levels, models=['SeasonalNaive'], plot_random=False)\n\n\n                                                \n\n\n\n\nRandom Walk with Drift\n\nStatsForecast.plot(train, test, level=levels, models=['RWD'], plot_random=False)\n\n\n                                                \n\n\n\n\nHistoric Average\n\nStatsForecast.plot(train, test, level=levels, models=['HistoricAverage'], plot_random=False)\n\n\n                                                \n\n\nFrom these plots, we can conclude that the uncertainty around each forecast varies according to the method that is being used. For the same time series, one method can predict a wider range of possible future values than others."
  },
  {
    "objectID": "examples/uncertaintyintervals.html#references",
    "href": "examples/uncertaintyintervals.html#references",
    "title": "Prediction Intervals",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, The Statistical Forecasting Perspective‚Äù."
  },
  {
    "objectID": "examples/forecastingatscale.html",
    "href": "examples/forecastingatscale.html",
    "title": "Forecasting at Scale",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/getting_started_short.html",
    "href": "examples/getting_started_short.html",
    "title": "Quick Start",
    "section": "",
    "text": "StatsForecast follows the sklearn model API. For this minimal example, you will create an instance of the StatsForecast class and then call its fit and predict methods. We recommend this option if speed is not paramount and you want to explore the fitted values and parameters.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to forecast many series, we recommend using the forecast method. Check this Getting Started with multiple time series guide.\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\nAs an example, let‚Äôs look at the US Air Passengers dataset. This time series consists of monthly totals of a US airline passengers from 1949 to 1960. The CSV is available here.\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nFirst, we‚Äôll import the data:\n\n! pip install StatsForecast\n\nUsageError: unrecognized arguments: hide output\n\n\n\nimport pandas as pd\n\n\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/air-passengers.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      AirPassengers\n      1949-01-01\n      112\n    \n    \n      1\n      AirPassengers\n      1949-02-01\n      118\n    \n    \n      2\n      AirPassengers\n      1949-03-01\n      132\n    \n    \n      3\n      AirPassengers\n      1949-04-01\n      129\n    \n    \n      4\n      AirPassengers\n      1949-05-01\n      121\n    \n  \n\n\n\n\nWe fit the model by instantiating a new StatsForecast object with its two required parameters:\n\nmodels: a list of models. Select the models you want from models and import them. For this example, we will use a AutoARIMA model. We set seson_lenght to 12 because we expect seasonal effects every 12 months. (See: Seasonal periods)\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nsf = StatsForecast(\n    models = [AutoARIMA(season_length = 12)],\n    freq = 'M'\n)\n\nsf.fit(df)\n\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\nforecast_df = sf.predict(h=12, level=[90]) \n\nforecast_df.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      AutoARIMA-lo-90\n      AutoARIMA-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      AirPassengers\n      1961-07-31\n      633.230774\n      589.562378\n      676.899170\n    \n    \n      AirPassengers\n      1961-08-31\n      535.230774\n      489.082153\n      581.379456\n    \n    \n      AirPassengers\n      1961-09-30\n      488.230804\n      439.728699\n      536.732910\n    \n    \n      AirPassengers\n      1961-10-31\n      417.230804\n      366.484253\n      467.977356\n    \n    \n      AirPassengers\n      1961-11-30\n      459.230804\n      406.334930\n      512.126648\n    \n  \n\n\n\n\nYou can plot the forecast by calling the StatsForecast.plot method and passing in your forecast dataframe.\n\ndf[\"ds\"]=pd.to_datetime(df[\"ds\"])\nsf.plot(df, forecast_df, level=[90])\n\n\n                                                \n\n\n\n\n\n\n\n\nNext Steps\n\n\n\n\nBuild and end to end forecasting pipeline following best practices in End to End Walkthrough\nForecast millions of series in a scalable cluster in the cloud using Spark and Nixtla\nDetect anomalies in your past observations\n\n\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/autoarima_vs_prophet.html",
    "href": "examples/autoarima_vs_prophet.html",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/autoarima_vs_prophet.html#motivation",
    "href": "examples/autoarima_vs_prophet.html#motivation",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Motivation",
    "text": "Motivation\nThe AutoARIMA model is widely used to forecast time series in production and as a benchmark. However, the python implementation (pmdarima) is so slow that prevent data scientist practioners from quickly iterating and deploying AutoARIMA in production for a large number of time series. In this notebook we present Nixtla‚Äôs AutoARIMA based on the R implementation (developed by Rob Hyndman) and optimized using numba."
  },
  {
    "objectID": "examples/autoarima_vs_prophet.html#example",
    "href": "examples/autoarima_vs_prophet.html#example",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Example",
    "text": "Example\n\nLibraries\n\n!pip install statsforecast prophet statsmodels sklearn matplotlib pmdarima\n\n\nimport logging\nimport os\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\nfrom multiprocessing import cpu_count, Pool # for prophet\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima as auto_arima_p\nfrom prophet import Prophet\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, _TS\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom sklearn.model_selection import ParameterGrid\n\nImporting plotly failed. Interactive plots will not work.\n\n\n\nUseful functions\nThe plot_grid function defined below will be useful to plot different time series, and different models‚Äô forecasts.\n\ndef plot_grid(df_train, df_test=None, plot_random=True):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train')\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            for model in df_test.drop(['unique_id', 'ds'], axis=1).columns:\n                if all(np.isnan(test_uid[model])):\n                    continue\n                axes[idx, idy].plot(test_uid['ds'], test_uid[model], label=model)\n\n        axes[idx, idy].set_title(f'M4 Hourly: {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\ndef plot_autocorrelation_grid(df_train):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n\n    unique_ids = random.sample(list(unique_ids), k=8)\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        plot_acf(train_uid['y'].values, ax=axes[idx, idy], \n                 title=f'ACF M4 Hourly {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Autocorrelation')\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\n\n\nData\nFor testing purposes, we will use the Hourly dataset from the M4 competition.\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\n\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\n\n\ntrain = pd.read_csv('M4-Hourly.csv')\ntest = pd.read_csv('M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\nIn this example we will use a subset of the data to avoid waiting too long. You can modify the number of series if you want.\n\nn_series = 16\nuids = train['unique_id'].unique()[:n_series]\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\n\nplot_grid(train, test)\n\n\n\n\nWould an autorregresive model be the right choice for our data? There is no doubt that we observe seasonal periods. The autocorrelation function (acf) can help us to answer the question. Intuitively, we have to observe a decreasing correlation to opt for an AR model.\n\nplot_autocorrelation_grid(train)\n\n\n\n\nThus, we observe a high autocorrelation for previous lags and also for the seasonal lags. Therefore, we will let auto_arima to handle our data.\n\n\nTraining and forecasting\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Hourly data, it would be benefitial to use 24 as seasonality.\n\n?AutoARIMA\n\n\nInit signature:\nAutoARIMA(\n    d: Optional[int] = None,\n    D: Optional[int] = None,\n    max_p: int = 5,\n    max_q: int = 5,\n    max_P: int = 2,\n    max_Q: int = 2,\n    max_order: int = 5,\n    max_d: int = 2,\n    max_D: int = 1,\n    start_p: int = 2,\n    start_q: int = 2,\n    start_P: int = 1,\n    start_Q: int = 1,\n    stationary: bool = False,\n    seasonal: bool = True,\n    ic: str = 'aicc',\n    stepwise: bool = True,\n    nmodels: int = 94,\n    trace: bool = False,\n    approximation: Optional[bool] = False,\n    method: Optional[str] = None,\n    truncate: Optional[bool] = None,\n    test: str = 'kpss',\n    test_kwargs: Optional[str] = None,\n    seasonal_test: str = 'seas',\n    seasonal_test_kwargs: Optional[Dict] = None,\n    allowdrift: bool = False,\n    allowmean: bool = False,\n    blambda: Optional[float] = None,\n    biasadj: bool = False,\n    parallel: bool = False,\n    num_cores: int = 2,\n    season_length: int = 1,\n)\nDocstring:      <no docstring>\nFile:           ~/fede/statsforecast/statsforecast/models.py\nType:           type\nSubclasses:     \n\n\n\n\nAs we see, we can pass season_length to AutoARIMA, so the definition of our models would be,\n\nmodels = [AutoARIMA(season_length=24, approximation=True)]\n\n\nfcst = StatsForecast(df=train, \n                     models=models, \n                     freq='H', \n                     n_jobs=-1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla = end - init\ntime_nixtla\n\n20.36360502243042\n\n\n\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      H1\n      701\n      616.084167\n    \n    \n      H1\n      702\n      544.432129\n    \n    \n      H1\n      703\n      510.414490\n    \n    \n      H1\n      704\n      481.046539\n    \n    \n      H1\n      705\n      460.893066\n    \n  \n\n\n\n\n\nforecasts = forecasts.reset_index()\n\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test)"
  },
  {
    "objectID": "examples/autoarima_vs_prophet.html#alternatives",
    "href": "examples/autoarima_vs_prophet.html#alternatives",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Alternatives",
    "text": "Alternatives\n\npmdarima\nYou can use the StatsForecast class to parallelize your own models. In this section we will use it to run the auto_arima model from pmdarima.\n\nclass PMDAutoARIMA(_TS):\n    \n    def __init__(self, season_length: int):\n        self.season_length = season_length\n        \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        mod = auto_arima_p(\n            y, m=self.season_length,\n            with_intercept=False #ensure comparability with Nixtla's implementation\n        ) \n        return {'mean': mod.predict(h)}\n    \n    def __repr__(self):\n        return 'pmdarima'\n\n\nn_series_pmdarima = 2\n\n\nfcst = StatsForecast(\n    df = train.query('unique_id in [\"H1\", \"H10\"]'), \n    models=[PMDAutoARIMA(season_length=24)],\n    freq='H',\n    n_jobs=-1\n)\n\n\ninit = time.time()\nforecast_pmdarima = fcst.forecast(48)\nend = time.time()\n\ntime_pmdarima = end - init\ntime_pmdarima\n\n349.93623208999634\n\n\n\nforecast_pmdarima.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      pmdarima\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      H1\n      701\n      627.479370\n    \n    \n      H1\n      702\n      570.364380\n    \n    \n      H1\n      703\n      541.831482\n    \n    \n      H1\n      704\n      516.475647\n    \n    \n      H1\n      705\n      503.044586\n    \n  \n\n\n\n\n\nforecast_pmdarima = forecast_pmdarima.reset_index()\n\n\ntest = test.merge(forecast_pmdarima, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test, plot_random=False)\n\n\n\n\n\n\nProphet\nProphet is designed to receive a pandas dataframe, so we cannot use StatForecast. Therefore, we need to parallize from scratch.\n\nparams_grid = {'seasonality_mode': ['multiplicative','additive'],\n               'growth': ['linear', 'flat'], \n               'changepoint_prior_scale': [0.1, 0.2, 0.3, 0.4, 0.5], \n               'n_changepoints': [5, 10, 15, 20]} \ngrid = ParameterGrid(params_grid)\n\n\ndef fit_and_predict(index, ts):\n    df = ts.drop(columns='unique_id', axis=1)\n    max_ds = df['ds'].max()\n    df['ds'] = pd.date_range(start='1970-01-01', periods=df.shape[0], freq='H')\n    df_val = df.tail(48) \n    df_train = df.drop(df_val.index) \n    y_val = df_val['y'].values\n    \n    if len(df_train) >= 48:\n        val_results = {'losses': [], 'params': []}\n\n        for params in grid:\n            model = Prophet(seasonality_mode=params['seasonality_mode'],\n                            growth=params['growth'],\n                            weekly_seasonality=True,\n                            daily_seasonality=True,\n                            yearly_seasonality=True,\n                            n_changepoints=params['n_changepoints'],\n                            changepoint_prior_scale=params['changepoint_prior_scale'])\n            model = model.fit(df_train)\n            \n            forecast = model.make_future_dataframe(periods=48, \n                                                   include_history=False, \n                                                   freq='H')\n            forecast = model.predict(forecast)\n            forecast['unique_id'] = index\n            forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n            \n            loss = np.mean(abs(y_val - forecast['yhat'].values))\n            \n            val_results['losses'].append(loss)\n            val_results['params'].append(params)\n\n        idx_params = np.argmin(val_results['losses']) \n        params = val_results['params'][idx_params]\n    else:\n        params = {'seasonality_mode': 'multiplicative',\n                  'growth': 'flat',\n                  'n_changepoints': 150,\n                  'changepoint_prior_scale': 0.5}\n    model = Prophet(seasonality_mode=params['seasonality_mode'],\n                    growth=params['growth'],\n                    weekly_seasonality=True,\n                    daily_seasonality=True,\n                    yearly_seasonality=True,\n                    n_changepoints=params['n_changepoints'],\n                    changepoint_prior_scale=params['changepoint_prior_scale'])\n    model = model.fit(df)\n    \n    forecast = model.make_future_dataframe(periods=48, \n                                           include_history=False, \n                                           freq='H')\n    forecast = model.predict(forecast)\n    forecast.insert(0, 'unique_id', index)\n    forecast['ds'] = np.arange(max_ds + 1, max_ds + 48 + 1)\n    forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n    \n    return forecast\n\n\nlogging.getLogger('prophet').setLevel(logging.WARNING)\n\n\nclass suppress_stdout_stderr(object):\n    '''\n    A context manager for doing a \"deep suppression\" of stdout and stderr in\n    Python, i.e. will suppress all print, even if the print originates in a\n    compiled C/Fortran sub-function.\n       This will not suppress raised exceptions, since exceptions are printed\n    to stderr just before a script exits, and after the context manager has\n    exited (at least, I think that is why it lets exceptions through).\n\n    '''\n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = [os.dup(1), os.dup(2)]\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        for fd in self.null_fds + self.save_fds:\n            os.close(fd)\n\n\ninit = time.time()\nwith suppress_stdout_stderr():\n    with Pool(cpu_count()) as pool:\n        forecast_prophet = pool.starmap(fit_and_predict, train.groupby('unique_id'))\nend = time.time()\nforecast_prophet = pd.concat(forecast_prophet).rename(columns={'yhat': 'prophet'})\ntime_prophet = end - init\ntime_prophet\n\n2022-08-19 23:07:24 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:25 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:41 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:42 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:08:00 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n\n\n120.9244737625122\n\n\n\nforecast_prophet\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      prophet\n    \n  \n  \n    \n      0\n      H1\n      701\n      631.867439\n    \n    \n      1\n      H1\n      702\n      561.001661\n    \n    \n      2\n      H1\n      703\n      499.299334\n    \n    \n      3\n      H1\n      704\n      456.132082\n    \n    \n      4\n      H1\n      705\n      431.884528\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      H112\n      744\n      5634.503804\n    \n    \n      44\n      H112\n      745\n      5622.643542\n    \n    \n      45\n      H112\n      746\n      5546.302705\n    \n    \n      46\n      H112\n      747\n      5457.777165\n    \n    \n      47\n      H112\n      748\n      5373.944098\n    \n  \n\n768 rows √ó 3 columns\n\n\n\n\ntest = test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test)\n\n\n\n\n\n\nEvaluation\n\n\nTime\nSince AutoARIMA works with numba is useful to calculate the time for just one time series.\n\nfcst = StatsForecast(df=train.query('unique_id == \"H1\"'), \n                     models=models, freq='H', \n                     n_jobs=1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla_1 = end - init\ntime_nixtla_1\n\n11.437001705169678\n\n\n\ntimes = pd.DataFrame({'n_series': np.arange(1, 414 + 1)})\ntimes['pmdarima'] = time_pmdarima * times['n_series'] / n_series_pmdarima\ntimes['prophet'] = time_prophet * times['n_series'] / n_series\ntimes['AutoARIMA_nixtla'] = time_nixtla_1 + times['n_series'] * (time_nixtla - time_nixtla_1) / n_series\ntimes = times.set_index('n_series')\n\n\ntimes.tail(5)\n\n\n\n\n\n  \n    \n      \n      pmdarima\n      prophet\n      AutoARIMA_nixtla\n    \n    \n      n_series\n      \n      \n      \n    \n  \n  \n    \n      410\n      71736.927578\n      3098.689640\n      240.181212\n    \n    \n      411\n      71911.895694\n      3106.247420\n      240.739124\n    \n    \n      412\n      72086.863811\n      3113.805199\n      241.297037\n    \n    \n      413\n      72261.831927\n      3121.362979\n      241.854950\n    \n    \n      414\n      72436.800043\n      3128.920759\n      242.412863\n    \n  \n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize = (24, 7))\n(times/3600).plot(ax=axes[0], linewidth=4)\nnp.log10(times).plot(ax=axes[1], linewidth=4)\naxes[0].set_title('Time across models [Hours]', fontsize=22)\naxes[1].set_title('Time across models [Log10 Scale]', fontsize=22)\naxes[0].set_ylabel('Time [Hours]', fontsize=20)\naxes[1].set_ylabel('Time Seconds [Log10 Scale]', fontsize=20)\nfig.suptitle('Time comparison using M4-Hourly data', fontsize=27)\nfor ax in axes:\n    ax.set_xlabel('Number of Time Series [N]', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid()\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n        label.set_fontsize(20)\n\n\n\n\n\nfig.savefig('computational-efficiency.png', dpi=300)\n\n\n\nPerformance\n\npmdarima (only two time series)\n\nname_models = test.drop(['unique_id', 'ds', 'y_test'], 1).columns.tolist()\n\n\ntest_pmdarima = test.query('unique_id in [\"H1\", \"H10\"]')\neval_pmdarima = []\nfor model in name_models:\n    mae = np.mean(abs(test_pmdarima[model] - test_pmdarima['y_test']))\n    eval_pmdarima.append({'model': model, 'mae': mae})\npd.DataFrame(eval_pmdarima).sort_values('mae')\n\n\n\n\n\n  \n    \n      \n      model\n      mae\n    \n  \n  \n    \n      0\n      AutoARIMA\n      20.289669\n    \n    \n      1\n      pmdarima\n      26.461525\n    \n    \n      2\n      prophet\n      43.155861\n    \n  \n\n\n\n\n\n\nProphet\n\neval_prophet = []\nfor model in name_models:\n    if 'pmdarima' in model:\n        continue\n    mae = np.mean(abs(test[model] - test['y_test']))\n    eval_prophet.append({'model': model, 'mae': mae})\npd.DataFrame(eval_prophet).sort_values('mae')\n\n\n\n\n\n  \n    \n      \n      model\n      mae\n    \n  \n  \n    \n      0\n      AutoARIMA\n      680.202970\n    \n    \n      1\n      prophet\n      1066.049049\n    \n  \n\n\n\n\nFor a complete comparison check the complete experiment."
  },
  {
    "objectID": "examples/experiments.html",
    "href": "examples/experiments.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nMeanwhile, please check this set of self-contained experiments and benchmarks with other popular libraries in Python and R.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/scale/dask.html",
    "href": "examples/scale/dask.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nMeanwhile, please check this content.\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/scale/spark.html",
    "href": "examples/scale/spark.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nMeanwhile, please check this content.\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/scale/ray.html",
    "href": "examples/scale/ray.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nMeanwhile, please check this content.\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/intermittentdata.html",
    "href": "examples/intermittentdata.html",
    "title": "Intermitent/Sparse data",
    "section": "",
    "text": "Sparse or intermittent series are series with very few non-zero observations. They are notoriously hard to forecast, and so, different methods have been developed especifically for them.\nIn this notebook we‚Äôll implement and benchmark some of the most popular methods for intermitent forecast using the StatsForecast library.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/intermittentdata.html#installing-statsforecast-library",
    "href": "examples/intermittentdata.html#installing-statsforecast-library",
    "title": "Intermitent/Sparse data",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install -U numba\n!pip install -U statsmodels\n!pip install statsforecast\n!pip install neuralforecast\n\n\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom itertools import product\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom neuralforecast.data.datasets import m5\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import (\n    ADIDA, CrostonClassic, CrostonOptimized,\n    CrostonSBA, IMAPA, TSB, ETS, AutoARIMA\n)\n\nplt.rcParams[\"figure.figsize\"] = (9,6)\n\n\nPlot functions\n\ndef plot_grid(df_train, plot_titles, model_cols=[\"y_50\"], df_test=None, plot_random=True):\n    \"\"\"Plots multiple time series.\"\"\"\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train', c='black')\n        axes[idx, idy].xaxis.set_tick_params(rotation=45)\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            axes[idx, idy].plot(test_uid['ds'], test_uid['y'], c='black', label='True')\n\n            for col in model_cols:\n                axes[idx, idy].plot(test_uid['ds'], test_uid[col], label=col)\n\n        axes[idx, idy].set_title(f'State: {plot_titles[uid]}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.7)\n    plt.show()"
  },
  {
    "objectID": "examples/intermittentdata.html#loading-and-exploring-the-m5-dataset",
    "href": "examples/intermittentdata.html#loading-and-exploring-the-m5-dataset",
    "title": "Intermitent/Sparse data",
    "section": "Loading and Exploring the M5 Dataset",
    "text": "Loading and Exploring the M5 Dataset\nThe M5 dataset consists of real-life data from Walmart and was used on a competition conducted on Kaggle. The data consists of around 42,000 hierarchical daily time series, starting at the level of SKUs and ending with the total demand of some large geographical area.\n\nY_df_total, *_ = m5.M5.load('./data')\n\n\nprint(Y_df_total['unique_id'].nunique())\nY_df_total.head()\n\n30490\n\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2011-01-29\n      3.0\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2011-01-30\n      0.0\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2011-01-31\n      0.0\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2011-02-01\n      1.0\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2011-02-02\n      4.0\n    \n  \n\n\n\n\n\nPrepare dataset for StatsForecast modelling\n\nCreate a subset of the dataset\n\nids = random.sample(list(Y_df_total['unique_id'].unique()), 100)\n\nY_df = Y_df_total.query('unique_id in @ids')\n\nY_df[\"unique_id\"] = Y_df[\"unique_id\"].astype(str)\n\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      573344\n      FOODS_1_035_CA_1\n      2011-01-29\n      3.0\n    \n    \n      573345\n      FOODS_1_035_CA_1\n      2011-01-30\n      5.0\n    \n    \n      573346\n      FOODS_1_035_CA_1\n      2011-01-31\n      1.0\n    \n    \n      573347\n      FOODS_1_035_CA_1\n      2011-02-01\n      0.0\n    \n    \n      573348\n      FOODS_1_035_CA_1\n      2011-02-02\n      0.0\n    \n  \n\n\n\n\n\nplot_titles = dict(zip(ids, ids))\nplot_grid(Y_df, plot_titles=plot_titles)"
  },
  {
    "objectID": "examples/intermittentdata.html#modelling-intermitent-data",
    "href": "examples/intermittentdata.html#modelling-intermitent-data",
    "title": "Intermitent/Sparse data",
    "section": "Modelling intermitent data",
    "text": "Modelling intermitent data\n\nADIDA: Temporal aggregation is used for reducing the presence of zero observations, thus mitigating the undesirable effect of the variance observed in the intervals. ADIDA uses equally sized time buckets to perform non-overlapping temporal aggregation and predict the demand over a pre-specified lead-time.\niMAPA: iMAPA stands for Intermittent Multiple Aggregation Prediction Algorithm. Another way for implementing temporal aggregation in demand forecasting. However, in contrast to ADIDA that considers a single aggregation level, iMAPA considers multiple ones, aiming at capturing different dynamics of the data\nTSB: TSB stands for Teunter-Syntetos-Babai. A modification to Croston‚Äôs method that replaces the inter-demand intervals component with the demand probability.\n\n\n# Split train test\nY_train_df = Y_df[Y_df.ds <= '2016-06-12']\nY_test_df = Y_df[Y_df.ds > '2016-06-12']\nprint(f\"Train: {Y_train_df.ds.nunique()}\")\nprint(f\"Test: {Y_test_df.ds.nunique()}\")\n\nTrain: 1962\nTest: 7\n\n\n\nmodels = [ADIDA(), IMAPA(), TSB(alpha_d=0.2, alpha_p=0.2)]\nhorizon = 7\nfreq = \"D\"\n\n\n# Create the forecast object and forecast test set\nmodel = StatsForecast(df=Y_train_df, models=models, freq=freq, n_jobs=-1)\n\nY_hat_df = model.forecast(horizon).reset_index()\nY_hat_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      ADIDA\n      IMAPA\n      TSB\n    \n  \n  \n    \n      0\n      FOODS_1_035_CA_1\n      2016-06-13\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      1\n      FOODS_1_035_CA_1\n      2016-06-14\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      2\n      FOODS_1_035_CA_1\n      2016-06-15\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      3\n      FOODS_1_035_CA_1\n      2016-06-16\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      4\n      FOODS_1_035_CA_1\n      2016-06-17\n      2.153789\n      2.233399\n      2.424801\n    \n  \n\n\n\n\n\nY_test_df\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      575306\n      FOODS_1_035_CA_1\n      2016-06-13\n      0.0\n    \n    \n      575307\n      FOODS_1_035_CA_1\n      2016-06-14\n      1.0\n    \n    \n      575308\n      FOODS_1_035_CA_1\n      2016-06-15\n      0.0\n    \n    \n      575309\n      FOODS_1_035_CA_1\n      2016-06-16\n      4.0\n    \n    \n      575310\n      FOODS_1_035_CA_1\n      2016-06-17\n      2.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      46805207\n      HOUSEHOLD_2_460_CA_1\n      2016-06-15\n      0.0\n    \n    \n      46805208\n      HOUSEHOLD_2_460_CA_1\n      2016-06-16\n      0.0\n    \n    \n      46805209\n      HOUSEHOLD_2_460_CA_1\n      2016-06-17\n      1.0\n    \n    \n      46805210\n      HOUSEHOLD_2_460_CA_1\n      2016-06-18\n      0.0\n    \n    \n      46805211\n      HOUSEHOLD_2_460_CA_1\n      2016-06-19\n      0.0\n    \n  \n\n700 rows √ó 3 columns"
  },
  {
    "objectID": "examples/intermittentdata.html#model-evaluation",
    "href": "examples/intermittentdata.html#model-evaluation",
    "title": "Intermitent/Sparse data",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nQuantitative evaluation\nFor the evaluation we use the Mean Absolute Error\n\\[ \\mathrm{MAE}(y, \\hat{y}) = \\frac{1}{N*H} \\sum_{i,\\tau} |y_{i,\\tau}-\\hat{y}_{i,\\tau}| \\]\n\ndef mae(y_hat, y_true):\n    return np.mean(np.abs(y_hat-y_true))\n\ny_true = Y_test_df['y'].values\nadida_preds = Y_hat_df['ADIDA'].values\nimapa_preds = Y_hat_df['IMAPA'].values\ntsb_preds = Y_hat_df['TSB'].values\n\nprint('ADIDA MAE: \\t %0.3f' % mae(adida_preds, y_true))\nprint('iMAPA MAE: \\t %0.3f' % mae(imapa_preds, y_true))\nprint('TSB   MAE: \\t %0.3f' % mae(tsb_preds, y_true))\n\nADIDA MAE:   1.172\niMAPA MAE:   1.173\nTSB   MAE:   1.180\n\n\n\n\nPlotting predictions\n\nY_hat_df\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      ADIDA\n      IMAPA\n      TSB\n    \n  \n  \n    \n      0\n      FOODS_1_035_CA_1\n      2016-06-13\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      1\n      FOODS_1_035_CA_1\n      2016-06-14\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      2\n      FOODS_1_035_CA_1\n      2016-06-15\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      3\n      FOODS_1_035_CA_1\n      2016-06-16\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      4\n      FOODS_1_035_CA_1\n      2016-06-17\n      2.153789\n      2.233399\n      2.424801\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      695\n      HOUSEHOLD_2_460_CA_1\n      2016-06-15\n      0.075524\n      0.059694\n      0.083630\n    \n    \n      696\n      HOUSEHOLD_2_460_CA_1\n      2016-06-16\n      0.075524\n      0.059694\n      0.083630\n    \n    \n      697\n      HOUSEHOLD_2_460_CA_1\n      2016-06-17\n      0.075524\n      0.059694\n      0.083630\n    \n    \n      698\n      HOUSEHOLD_2_460_CA_1\n      2016-06-18\n      0.075524\n      0.059694\n      0.083630\n    \n    \n      699\n      HOUSEHOLD_2_460_CA_1\n      2016-06-19\n      0.075524\n      0.059694\n      0.083630\n    \n  \n\n700 rows √ó 5 columns\n\n\n\n\nY_test_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n\nplot_grid(Y_train_df.groupby('unique_id').tail(3*horizon), \n          plot_titles=plot_titles, \n          model_cols=['ADIDA', 'IMAPA', 'TSB'], \n          df_test=Y_test_df)"
  },
  {
    "objectID": "examples/multipleseasonalities.html",
    "href": "examples/multipleseasonalities.html",
    "title": "Multiple seasonalities",
    "section": "",
    "text": "Tip\n\n\n\nFor this task, StatsForecast‚Äôs MSTL is 68% more accurate and 600% faster than Prophet and NeuralProphet. (Reproduce experiments here)\nMultiple seasonal data refers to time series that have more than one clear seasonality. Multiple seasonality is traditionally present in data that is sampled at a low frequency. For example, hourly electricity data exhibits daily and weekly seasonality. That means that there are clear patterns of electricity consumption for specific hours of the day like 6:00pm vs 3:00am or for specific days like Sunday vs Friday.\nTraditional statistical models are not able to model more than one seasonal length. In this example, we will show how to model the two seasonalities efficiently using Multiple Seasonal-Trend decompositions with LOESS (MSTL).\nFor this example, we will use hourly electricity load data from Pennsylvania, New Jersey, and Maryland (PJM). The original data can be found here. (Click here for info on PJM)\nFirst, we will load the data, then we will use the StatsForecast.fit and StatsForecast.predict methods to predict the next 24 hours. We will then decompose the different elements of the time series into trends and its multiple seasonalities. At the end, you will use the StatsForecast.forecast for production-ready forecasting.\nOutline\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/multipleseasonalities.html#install-libraries",
    "href": "examples/multipleseasonalities.html#install-libraries",
    "title": "Multiple seasonalities",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast ``"
  },
  {
    "objectID": "examples/multipleseasonalities.html#load-data",
    "href": "examples/multipleseasonalities.html#load-data",
    "title": "Multiple seasonalities",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nYou will read the data with pandas and change the necessary names. This step should take around 2s.\n\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/jnagura/Energy-consumption-prediction-analysis/master/PJM_Load_hourly.csv')\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf.tail()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      32891\n      PJM_Load_hourly\n      2001-01-01 20:00:00\n      35209.0\n    \n    \n      32892\n      PJM_Load_hourly\n      2001-01-01 21:00:00\n      34791.0\n    \n    \n      32893\n      PJM_Load_hourly\n      2001-01-01 22:00:00\n      33669.0\n    \n    \n      32894\n      PJM_Load_hourly\n      2001-01-01 23:00:00\n      31809.0\n    \n    \n      32895\n      PJM_Load_hourly\n      2001-01-02 00:00:00\n      29506.0\n    \n  \n\n\n\n\nStatsForecast can handle unsorted data, however, for plotting purposes, it is convenient to sort the data frame.\n\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA. In this case, it will print just one series given that we have just one unique_id.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n/Users/max.mergenthaler/Nixtla/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n                                                \n\n\nThe time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods."
  },
  {
    "objectID": "examples/multipleseasonalities.html#fit-an-mstl-model",
    "href": "examples/multipleseasonalities.html#fit-an-mstl-model",
    "title": "Multiple seasonalities",
    "section": "Fit an MSTL model",
    "text": "Fit an MSTL model\nThe MSTL (Multiple Seasonal-Trend decompositions using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a non-seasonal model and each seasonality using a SeasonalNaive model. You can choose the non-seasonal model you want to use to forecast the trend component of the MSTL model. In this example, we will use an AutoARIMA.\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] for season length. The trend component will be forecasted with an AutoARIMA model. (You can also try with: AutoTheta, AutoCES, and AutoETS)\n\n# Create a list of models and instantiation parameters\n\nmodels = [MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA() # model used to forecast trend\n)]\n\nWe fit the models by instantiating a new StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(\n    models=models, # model used to fit each time series \n    freq='H', # frequency of the data\n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nUse the fit method to fit each model to each time series. In this case, we are just fitting one model to one series. Check this guide to learn how to fit many models to many series.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 10 seconds. The second time -once Numba compiled your settings- it should take less than 5s.\n\n\n\nsf = sf.fit(df=df)"
  },
  {
    "objectID": "examples/multipleseasonalities.html#decompose-the-series",
    "href": "examples/multipleseasonalities.html#decompose-the-series",
    "title": "Multiple seasonalities",
    "section": "Decompose the series",
    "text": "Decompose the series\nOnce the model is fitted, access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case, we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n\n\n  \n    \n      \n      data\n      trend\n      seasonal24\n      seasonal168\n      remainder\n    \n  \n  \n    \n      0\n      22259.0\n      26183.898892\n      -5215.124554\n      609.000432\n      681.225229\n    \n    \n      1\n      21244.0\n      26181.599305\n      -6255.673234\n      603.823918\n      714.250011\n    \n    \n      2\n      20651.0\n      26179.294886\n      -6905.329895\n      636.820423\n      740.214587\n    \n    \n      3\n      20421.0\n      26176.985472\n      -7073.420118\n      615.825999\n      701.608647\n    \n    \n      4\n      20713.0\n      26174.670877\n      -7062.395760\n      991.521912\n      609.202971\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32891\n      36392.0\n      33123.552727\n      4387.149171\n      -488.177882\n      -630.524015\n    \n    \n      32892\n      35082.0\n      33148.242575\n      3479.852929\n      -682.928737\n      -863.166767\n    \n    \n      32893\n      33890.0\n      33172.926165\n      2307.808829\n      -650.566775\n      -940.168219\n    \n    \n      32894\n      32590.0\n      33197.603322\n      748.587723\n      -555.177849\n      -801.013195\n    \n    \n      32895\n      31569.0\n      33222.273902\n      -967.124123\n      -265.895357\n      -420.254422\n    \n  \n\n32896 rows √ó 5 columns\n\n\n\nWe will use matplotlib, to visualize the different components of the series.\n\nimport matplotlib.pyplot as plt\n\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe observe a clear upward trend (orange line) and seasonality repeating every day (24H) and every week (168H)."
  },
  {
    "objectID": "examples/multipleseasonalities.html#predict-the-next-24-hours",
    "href": "examples/multipleseasonalities.html#predict-the-next-24-hours",
    "title": "Multiple seasonalities",
    "section": "Predict the next 24 hours",
    "text": "Predict the next 24 hours\n\nProbabilistic forecasting with levels\n\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      MSTL\n      MSTL-lo-90\n      MSTL-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      PJM_Load_hourly\n      2002-01-01 01:00:00\n      29956.744141\n      29585.187500\n      30328.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 02:00:00\n      29057.691406\n      28407.498047\n      29707.884766\n    \n    \n      PJM_Load_hourly\n      2002-01-01 03:00:00\n      28654.699219\n      27767.101562\n      29542.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 04:00:00\n      28499.009766\n      27407.640625\n      29590.378906\n    \n    \n      PJM_Load_hourly\n      2002-01-01 05:00:00\n      28821.716797\n      27552.236328\n      30091.197266\n    \n  \n\n\n\n\nYou can plot the forecast by calling the StatsForecast.plot method and passing in your forecast dataframe.\n\nsf.plot(df, forecasts, max_insample_length=24 * 7)"
  },
  {
    "objectID": "examples/multipleseasonalities.html#forecast-in-production",
    "href": "examples/multipleseasonalities.html#forecast-in-production",
    "title": "Multiple seasonalities",
    "section": "Forecast in production",
    "text": "Forecast in production\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 10 seconds. The second time -once Numba compiled your settings- it should take less than 5s.\n\n\n\nforecasts_df = sf.forecast(h=24, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      MSTL\n      MSTL-lo-90\n      MSTL-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      PJM_Load_hourly\n      2002-01-01 01:00:00\n      29956.744141\n      29585.187500\n      30328.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 02:00:00\n      29057.691406\n      28407.498047\n      29707.884766\n    \n    \n      PJM_Load_hourly\n      2002-01-01 03:00:00\n      28654.699219\n      27767.101562\n      29542.298828\n    \n    \n      PJM_Load_hourly\n      2002-01-01 04:00:00\n      28499.009766\n      27407.640625\n      29590.378906\n    \n    \n      PJM_Load_hourly\n      2002-01-01 05:00:00\n      28821.716797\n      27552.236328\n      30091.197266"
  },
  {
    "objectID": "examples/multipleseasonalities.html#references",
    "href": "examples/multipleseasonalities.html#references",
    "title": "Multiple seasonalities",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). ‚ÄúMSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns‚Äù."
  },
  {
    "objectID": "examples/multipleseasonalities.html#next-steps",
    "href": "examples/multipleseasonalities.html#next-steps",
    "title": "Multiple seasonalities",
    "section": "Next Steps",
    "text": "Next Steps\n\nLearn how to use cross-validation to assess the robustness of your model."
  },
  {
    "objectID": "examples/anomalydetection.html",
    "href": "examples/anomalydetection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/anomalydetection.html#installing-statsforecast-library",
    "href": "examples/anomalydetection.html#installing-statsforecast-library",
    "title": "Anomaly Detection",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install statsforecast neuralforecast\n\n\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom ipywidgets import interact\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\n\nUseful functions\nThe plot_grid function defined below will be useful to plot different time series, and different models‚Äô forecasts.\n\ndef plot_grid(df_train, df_test=None, plot_random=True, level=None, anomalies=False):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) >= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train')\n        if level is not None and anomalies: \n            axes[idx, idy].fill_between(\n                train_uid['ds'], \n                train_uid[f'AutoARIMA-lo-{level}'], \n                train_uid[f'AutoARIMA-hi-{level}'],\n                alpha=0.9,\n                color='orange',\n                label=f'AutoARIMA_level_{level}',\n            )\n            filt = (train_uid['y'] > train_uid[f'AutoARIMA-hi-{level}']) | (train_uid[f'AutoARIMA-lo-{level}'] > train_uid['y'])\n            anomalies_df = train_uid[filt][['ds', 'y']]\n            axes[idx, idy].scatter(\n                anomalies_df['ds'],\n                anomalies_df['y'],\n                color='red',\n                label='Anomalies'\n            )\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            models = df_test.drop(['unique_id', 'ds'], axis=1).columns\n            for model in models:\n                if all(np.isnan(test_uid[model])):\n                    continue\n                if 'lo' in model or 'hi' in model:\n                    continue\n                axes[idx, idy].plot(test_uid['ds'], test_uid[model], label=model)\n            if level is not None:\n                for l in level:\n                    axes[idx, idy].fill_between(\n                        test_uid['ds'], \n                        test_uid[f'AutoARIMA-lo-{l}'], \n                        test_uid[f'AutoARIMA-hi-{l}'],\n                        alpha=1 - l // 100,\n                        color='orange',\n                        label=f'AutoARIMA_level_{l}',\n                    )\n        axes[idx, idy].set_title(f'M4 Hourly: {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()"
  },
  {
    "objectID": "examples/anomalydetection.html#download-data",
    "href": "examples/anomalydetection.html#download-data",
    "title": "Anomaly Detection",
    "section": "Download data",
    "text": "Download data\nFor testing purposes, we will use the Hourly dataset from the M4 competition.\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\n\n\ntrain = pd.read_csv('M4-Hourly.csv')\n\nIn this example we will use a subset of the data to avoid waiting too long. You can modify the number of series if you want.\n\nn_series = 8\nuids = train['unique_id'].unique()[:n_series]\ntrain = train.query('unique_id in @uids')\n\n\nplot_grid(train)"
  },
  {
    "objectID": "examples/anomalydetection.html#train-the-model",
    "href": "examples/anomalydetection.html#train-the-model",
    "title": "Anomaly Detection",
    "section": "Train the model",
    "text": "Train the model\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Hourly data, it would be benefitial to use 24 as seasonality.\n\nmodels = [AutoARIMA(season_length=24, approximation=True)]\n\n\nfcst = StatsForecast(df=train, \n                     models=models, \n                     freq='H', \n                     n_jobs=-1)\n\nWe can define the level of the forecast intervals we want to produce. StatsForecast will produce these levels for both forecasts and intra-sample forecasts.\n\nlevels = [80, 90, 95, 99]\n\nObserve that we need to pass fitted=True to the forecast method to recover the insample forecasts.\n\nforecasts = fcst.forecast(h=48, fitted=True, level=levels)\n\n\nforecasts = forecasts.reset_index()\n\n\nforecasts.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoARIMA\n      AutoARIMA-lo-99\n      AutoARIMA-lo-95\n      AutoARIMA-lo-90\n      AutoARIMA-lo-80\n      AutoARIMA-hi-80\n      AutoARIMA-hi-90\n      AutoARIMA-hi-95\n      AutoARIMA-hi-99\n    \n  \n  \n    \n      0\n      H1\n      701\n      616.084167\n      585.106445\n      592.513000\n      596.302612\n      600.671814\n      631.496460\n      635.865662\n      639.655273\n      647.061890\n    \n    \n      1\n      H1\n      702\n      544.432129\n      494.394348\n      506.358063\n      512.479370\n      519.536865\n      569.327393\n      576.384888\n      582.506165\n      594.469910\n    \n    \n      2\n      H1\n      703\n      510.414490\n      443.625366\n      459.594238\n      467.764801\n      477.184906\n      543.644043\n      553.064148\n      561.234741\n      577.203613\n    \n    \n      3\n      H1\n      704\n      481.046539\n      404.228729\n      422.595398\n      431.992798\n      442.827393\n      519.265686\n      530.100281\n      539.497681\n      557.864380\n    \n    \n      4\n      H1\n      705\n      460.893066\n      378.863678\n      398.476410\n      408.511383\n      420.081024\n      501.705109\n      513.274780\n      523.309692\n      542.922424\n    \n  \n\n\n\n\n\nplot_grid(train, forecasts)"
  },
  {
    "objectID": "examples/anomalydetection.html#recover-insample-forecasts",
    "href": "examples/anomalydetection.html#recover-insample-forecasts",
    "title": "Anomaly Detection",
    "section": "Recover insample forecasts",
    "text": "Recover insample forecasts\nOnce the model is fitted, we can recover the insample forecasts and their prediction intervals using forecast_fitted_values.\n\ninsample_forecasts = fcst.forecast_fitted_values().reset_index()\n\n\ninsample_forecasts.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n      AutoARIMA\n      AutoARIMA-lo-99\n      AutoARIMA-lo-95\n      AutoARIMA-lo-90\n      AutoARIMA-lo-80\n      AutoARIMA-hi-80\n      AutoARIMA-hi-90\n      AutoARIMA-hi-95\n      AutoARIMA-hi-99\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n      604.395020\n      573.417297\n      580.823853\n      584.613464\n      588.982666\n      619.807312\n      624.176514\n      627.966125\n      635.372742\n    \n    \n      1\n      H1\n      2\n      586.0\n      585.414001\n      554.436279\n      561.842896\n      565.632507\n      570.001648\n      600.826355\n      605.195557\n      608.985168\n      616.391724\n    \n    \n      2\n      H1\n      3\n      586.0\n      585.414001\n      554.436279\n      561.842896\n      565.632507\n      570.001709\n      600.826355\n      605.195557\n      608.985168\n      616.391724\n    \n    \n      3\n      H1\n      4\n      559.0\n      558.441040\n      527.463318\n      534.869873\n      538.659485\n      543.028687\n      573.853333\n      578.222534\n      582.012146\n      589.418762\n    \n    \n      4\n      H1\n      5\n      511.0\n      510.489014\n      479.511292\n      486.917877\n      490.707520\n      495.076691\n      525.901367\n      530.270569\n      534.060181\n      541.466736"
  },
  {
    "objectID": "examples/anomalydetection.html#plot-anomalies",
    "href": "examples/anomalydetection.html#plot-anomalies",
    "title": "Anomaly Detection",
    "section": "Plot anomalies",
    "text": "Plot anomalies\nIn this example, we consider as an anomaly an observation that is above the upper prediction interval or below the lower prediction interval of a certain probability level.\n\nplot_grid(insample_forecasts, level=99, anomalies=True)\n\n\n\n\nThe following code, allows us to inspect the anomalies in detail.\n\nuid = \"H1\"\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\ndf_plot = insample_forecasts.query('unique_id == @uid')[:60]\nax.plot(df_plot['ds'], df_plot['y'], linewidth=2)\nax.fill_between(df_plot['ds'], \n                df_plot['AutoARIMA-lo-99'], \n                df_plot['AutoARIMA-hi-99'],\n                alpha=.35,\n                color='orange',\n                label='AutoARIMA_level_99')\nanomalies_df = df_plot.query('y > `AutoARIMA-hi-99` or y < `AutoARIMA-lo-99`')[['ds', 'y']]\nax.scatter(\n    anomalies_df['ds'],\n    anomalies_df['y'],\n    color='red',\n    label='Anomalies',\n    s=100\n)\nax.set_title(f'{uid} Anomalies', fontsize=22)\nax.set_ylabel('Target', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\nfor label in (ax.get_xticklabels() + ax.get_yticklabels()):\n    label.set_fontsize(20)\n\n\n\n\n\nWidget\nWe can also create a widget to change the probability level of the prediction intervals.\n\n@interact(level=reversed(levels))\ndef plot_anomalies(level):\n    plot_grid(insample_forecasts, level=level, anomalies=True)"
  },
  {
    "objectID": "examples/outliers.html",
    "href": "examples/outliers.html",
    "title": "Outliers",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/migrating_R.html",
    "href": "examples/migrating_R.html",
    "title": "statsforecast",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html",
    "href": "examples/electricitypeakforecasting.html",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers‚Äô combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an MSTL (Multiple Seasonal-Trend decomposition using LOESS) model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the StatsForecast.cross_validation method to fit the MSTL model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit MSTL model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#libraries",
    "href": "examples/electricitypeakforecasting.html#libraries",
    "title": "Detect Demand Peaks",
    "section": "Libraries",
    "text": "Libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast"
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#load-data",
    "href": "examples/electricitypeakforecasting.html#load-data",
    "title": "Detect Demand Peaks",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nFirst, download and read the 2022 historic total demand of the ERCOT market, available here. The data processing includes adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest. This step should take around 2s.\n\nimport numpy as np\nimport pandas as pd\n\n\n# Load data\nhistoric = pd.read_csv('./Native_Load_2022.csv')\n# Add missing hour due to daylight saving time\nhistoric = pd.concat([historic, pd.DataFrame({'Hour Ending':['03/13/2022 03:00'], 'ERCOT':['43980.57']})])\nhistoric = historic.sort_values('Hour Ending').reset_index(drop=True)\n# Convert to datetime\nhistoric['ERCOT'] = historic['ERCOT'].str.replace(',','').astype(float)\nhistoric = historic[~pd.isna(historic['ERCOT'])]\nhistoric['ds'] = pd.to_datetime(historic['Hour Ending'].str[:10]) + pd.to_timedelta(np.tile(range(24), len(historic)//24),'h')\nhistoric['unique_id'] = 'ERCOT'\nhistoric['y'] = historic['ERCOT']\n# Select relevant columns and dates\nY_df = historic[['unique_id', 'ds', 'y']]\nY_df = Y_df[Y_df['ds']<='2022-10-01']\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)\n\n/Users/cchallu/NIXTLA/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n                                                \n\n\nWe observe that the time series exhibits seasonal patterns. Moreover, the time series contains 6,552 observations, so it is necessary to use computationally efficient methods to deploy them in production."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "href": "examples/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "title": "Detect Demand Peaks",
    "section": "Fit and Forecast MSTL model",
    "text": "Fit and Forecast MSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\n\n\n\n\n\n\nTip\n\n\n\nCheck our detailed explanation and tutorial on MSTL here\n\n\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, instantiate the model and define the parameters. The electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities. See this link for a detailed explanation on how to set seasonal lengths. In this example we use the AutoARIMA model for the trend component, however, any StatsForecast model can be used. The complete list of models is available here.\n\nmodels = [MSTL(\n            season_length=[24, 24 * 7], # seasonalities of the time series \n            trend_forecaster=AutoARIMA(nmodels=10) # model used to forecast trend\n            )\n          ]\n\n\n\n\n\n\n\nTip\n\n\n\nThe parameter nmodels of the AutoARIMA controls the number of models considered in stepwise search. The default is 94, reduce it to decrease training times!\n\n\nWe fit the model by instantiating a StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\n\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nThe cross_validation method allows the user to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods. This method re-trains the model and forecast each window. See this tutorial for an animation of how the windows are defined.\nUse the cross_validation method to produce all the daily forecasts for September. To produce daily forecasts set the forecasting horizon h as 24. In this example we are simulating deploying the pipeline during September, so set the number of windows as 30 (one for each day). Finally, set the step size between windows as 24, to only produce one forecast per day.\n\ncrossvalidation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=30\n  )\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      MSTL\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      ERCOT\n      2022-09-01 00:00:00\n      2022-08-31 23:00:00\n      45482.468750\n      47126.179688\n    \n    \n      ERCOT\n      2022-09-01 01:00:00\n      2022-08-31 23:00:00\n      43602.660156\n      45088.542969\n    \n    \n      ERCOT\n      2022-09-01 02:00:00\n      2022-08-31 23:00:00\n      42284.820312\n      43897.175781\n    \n    \n      ERCOT\n      2022-09-01 03:00:00\n      2022-08-31 23:00:00\n      41663.160156\n      43187.812500\n    \n    \n      ERCOT\n      2022-09-01 04:00:00\n      2022-08-31 23:00:00\n      41710.621094\n      43369.859375\n    \n  \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using cross_validation make sure the forecasts are produced at the desired timestamps. Check the cutoff column which specifices the last timestamp before the forecasting window."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#peak-detection",
    "href": "examples/electricitypeakforecasting.html#peak-detection",
    "title": "Detect Demand Peaks",
    "section": "Peak Detection",
    "text": "Peak Detection\nFinally, we use the forecasts in crossvaldation_df to detect the daily hourly demand peaks. For each day, we set the detected peaks as the highest forecasts. In this case, we want to predict one peak (npeaks); depending on your setting and goals, this parameter might change. For example, the number of peaks can correspond to how many hours a battery can be discharged to reduce demand.\n\nnpeaks = 1 # Number of peaks\n\nFor the ERCOT 4CP detection task we are interested in correctly predicting the highest monthly load. Next, we filter the day in September with the highest hourly demand and predict the peak.\n\ncrossvalidation_df = crossvalidation_df.reset_index()[['ds','y','MSTL']]\nmax_day = crossvalidation_df.iloc[crossvalidation_df['y'].argmax()].ds.day # Day with maximum load\ncv_df_day = crossvalidation_df.query('ds.dt.day == @max_day')\nmax_hour = cv_df_day['y'].argmax()\npeaks = cv_df_day['MSTL'].argsort().iloc[-npeaks:].values # Predicted peaks\n\nIn the following plot we see how the MSTL model is able to correctly detect the coincident peak for September 2022.\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(10, 5))\nplt.axvline(cv_df_day.iloc[max_hour]['ds'], color='black', label='True Peak')\nplt.scatter(cv_df_day.iloc[peaks]['ds'], cv_df_day.iloc[peaks]['MSTL'], color='green', label=f'Predicted Top-{npeaks}')\nplt.plot(cv_df_day['ds'], cv_df_day['y'], label='y', color='blue')\nplt.plot(cv_df_day['ds'], cv_df_day['MSTL'], label='Forecast', color='red')\nplt.xlabel('Time')\nplt.ylabel('Load (MW)')\nplt.grid()\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this example we only include September. However, MSTL can correctly predict the peaks for the 4 months of 2022. You can try this by increasing the nwindows parameter of cross_validation or filtering the Y_df dataset. The complete run for all months take only 10 minutes."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#next-steps",
    "href": "examples/electricitypeakforecasting.html#next-steps",
    "title": "Detect Demand Peaks",
    "section": "Next steps",
    "text": "Next steps\nStatsForecast and MSTL in particular are good benchmarking models for peak detection. However, it might be useful to explore further and newer forecasting algorithms. We have seen particularly good results with the N-HiTS, a deep-learning model from Nixtla‚Äôs NeuralForecast library.\nLearn how to predict ERCOT demand peaks with our deep-learning N-HiTS model and the NeuralForecast library in this tutorial."
  },
  {
    "objectID": "examples/electricitypeakforecasting.html#references",
    "href": "examples/electricitypeakforecasting.html#references",
    "title": "Detect Demand Peaks",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). ‚ÄúMSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns‚Äù.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). ‚ÄúN-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting‚Äù. Accepted at AAAI 2023."
  },
  {
    "objectID": "examples/models_intro.html",
    "href": "examples/models_intro.html",
    "title": "StatsForecast‚Äôs Models",
    "section": "",
    "text": "Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoETS\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nAutoCES\n‚úÖ\n\n‚úÖ\n\n\n\nAutoTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/models_intro.html#theta-family",
    "href": "examples/models_intro.html#theta-family",
    "title": "StatsForecast‚Äôs Models",
    "section": "Theta Family",
    "text": "Theta Family\nfit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts.\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nOptimizedTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nDynamicTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nDynamicOptimizedTheta\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "examples/models_intro.html#multiple-seasonalities",
    "href": "examples/models_intro.html#multiple-seasonalities",
    "title": "StatsForecast‚Äôs Models",
    "section": "Multiple Seasonalities",
    "text": "Multiple Seasonalities\nSuited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs.\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nMSTL\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "examples/models_intro.html#baseline-models",
    "href": "examples/models_intro.html#baseline-models",
    "title": "StatsForecast‚Äôs Models",
    "section": "Baseline Models",
    "text": "Baseline Models\nClassical models for establishing baseline.\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nHistoricAverage\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nNaive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nRandomWalkWithDrift\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nSeasonalNaive\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nWindowAverage\n‚úÖ\n\n\n\n\n\nSeasonalWindowAverage\n‚úÖ"
  },
  {
    "objectID": "examples/models_intro.html#exponential-smoothing",
    "href": "examples/models_intro.html#exponential-smoothing",
    "title": "StatsForecast‚Äôs Models",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality.\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nSimpleExponentialSmoothing\n‚úÖ\n\n\n\n\n\nSimpleExponentialSmoothingOptimized\n‚úÖ\n\n\n\n\n\nSeasonalExponentialSmoothing\n‚úÖ\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n‚úÖ\n\n\n\n\n\nHolt\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nHoltWinters\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ"
  },
  {
    "objectID": "examples/models_intro.html#sparse-of-inttermitent",
    "href": "examples/models_intro.html#sparse-of-inttermitent",
    "title": "StatsForecast‚Äôs Models",
    "section": "Sparse of Inttermitent",
    "text": "Sparse of Inttermitent\nSuited for series with very few non-zero observations\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nADIDA\n‚úÖ\n\n\n\n\n\nCrostonClassic\n‚úÖ\n\n\n\n\n\nCrostonOptimized\n‚úÖ\n\n\n\n\n\nCrostonSBA\n‚úÖ\n\n\n\n\n\nIMAPA\n‚úÖ\n\n\n\n\n\nTSB\n‚úÖ"
  },
  {
    "objectID": "examples/nondailydata.html",
    "href": "examples/nondailydata.html",
    "title": "Non Daily Data",
    "section": "",
    "text": "Give us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/crossvalidation.html",
    "href": "examples/crossvalidation.html",
    "title": "Cross validation",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/crossvalidation.html#introduction",
    "href": "examples/crossvalidation.html#introduction",
    "title": "Cross validation",
    "section": "Introduction",
    "text": "Introduction\nTime series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it.\n\nStatsforecast has an implementation of time series cross-validation that is fast and easy to use. This implementation makes cross-validation a distributed operation, which makes it less time-consuming. In this notebook, we‚Äôll use it on a subset of the M4 Competition hourly dataset.\nOutline:\n\nInstall libraries\nLoad and explore data\nTrain model\nPerform time series cross-validation\nEvaluate results\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "examples/crossvalidation.html#install-libraries",
    "href": "examples/crossvalidation.html#install-libraries",
    "title": "Cross validation",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages with pip install statsforecast\n\npip install statsforecast\n\n\nfrom statsforecast import StatsForecast # required to instantiate StastForecast object and use cross-validation method"
  },
  {
    "objectID": "examples/crossvalidation.html#load-and-explore-the-data",
    "href": "examples/crossvalidation.html#load-and-explore-the-data",
    "title": "Cross validation",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nAs stated in the introduction, we‚Äôll use the M4 Competition hourly dataset. We‚Äôll first import the data from an URL using pandas.\n\nimport pandas as pd \n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet') # load the data \nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n    \n    \n      1\n      H1\n      2\n      586.0\n    \n    \n      2\n      H1\n      3\n      586.0\n    \n    \n      3\n      H1\n      4\n      559.0\n    \n    \n      4\n      H1\n      5\n      511.0\n    \n  \n\n\n\n\nThe input to StatsForecast is a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int, or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS.\nThe y (numeric) represents the measurement we wish to forecast.\n\nThe data in this example already has this format, so no changes are needed.\nTo keep the time required to execute this notebook to a minimum, we‚Äôll only use one time series from the data, namely the one with unique_id == 'H1'. However, you can use as many as you want, with no additional changes to the code needed.\n\ndf = Y_df[Y_df['unique_id'] == 'H1'] # select time series\n\nWe can plot the time series we‚Äôll work with using StatsForecast.plot method.\n\nStatsForecast.plot(df)"
  },
  {
    "objectID": "examples/crossvalidation.html#train-model",
    "href": "examples/crossvalidation.html#train-model",
    "title": "Cross validation",
    "section": "Train model",
    "text": "Train model\nFor this example, we‚Äôll use StastForecast AutoETS. We first need to import it from statsforecast.models and then we need to instantiate a new StatsForecast object.\nThe StatsForecast object has the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. See panda‚Äôs available frequencies.\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame df.\n\nfrom statsforecast.models import AutoETS\n\nmodels = [AutoETS(season_length = 24)]\n\nsf = StatsForecast(\n    df = df, \n    models = models, \n    freq = 'H', \n    n_jobs = -1\n)"
  },
  {
    "objectID": "examples/crossvalidation.html#perform-time-series-cross-validation",
    "href": "examples/crossvalidation.html#perform-time-series-cross-validation",
    "title": "Cross validation",
    "section": "Perform time series cross-validation",
    "text": "Perform time series cross-validation\nOnce the StatsForecastobject has been instantiated, we can use the cross_validation method, which takes the following arguments:\n\ndf: training data frame with StatsForecast format\nh (int): represents the h steps into the future that will be forecasted\nstep_size (int): step size between each window, meaning how often do you want to run the forecasting process.\nn_windows (int): number of windows used for cross-validation, meaning the number of forecasting processes in the past you want to evaluate.\n\nFor this particular example, we‚Äôll use 3 windows of 24 hours.\n\ncrossvalidation_df = sf.cross_validation(\n    df = df,\n    h = 24,\n    step_size = 24,\n    n_windows = 3\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      AutoETS\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      677\n      676\n      691.0\n      677.761047\n    \n    \n      H1\n      678\n      676\n      618.0\n      607.817871\n    \n    \n      H1\n      679\n      676\n      563.0\n      569.437744\n    \n    \n      H1\n      680\n      676\n      529.0\n      537.340027\n    \n    \n      H1\n      681\n      676\n      504.0\n      515.571106\n    \n  \n\n\n\n\nWe‚Äôll now plot the forecast for each cutoff period. To make the plots clearer, we‚Äôll rename the actual values in each period.\n\ncrossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True) # rename actual values \n\ncutoff = crossvalidation_df['cutoff'].unique()\n\nfor k in range(len(cutoff)): \n    cv = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff[k]]\n    StatsForecast.plot(df, cv.loc[:, cv.columns != 'cutoff'])\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nNotice that in each cutoff period, we generated a forecast for the next 24 hours using only the data y before said period."
  },
  {
    "objectID": "examples/crossvalidation.html#evaluate-results",
    "href": "examples/crossvalidation.html#evaluate-results",
    "title": "Cross validation",
    "section": "Evaluate results",
    "text": "Evaluate results\nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we‚Äôll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\npip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\n\nThe forecasts, in this case, AutoETS.\n\n\nrmse = rmse(crossvalidation_df['actual'], crossvalidation_df['AutoETS'])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  33.90342\n\n\nThis measure should better reflect the predictive abilities of our model, since it used different time periods to test its accuracy.\n\n\n\n\n\n\nTip\n\n\n\nCross validation is especially useful when comparing multiple models. Here‚Äôs an example with multiple models and time series."
  },
  {
    "objectID": "examples/crossvalidation.html#references",
    "href": "examples/crossvalidation.html#references",
    "title": "Cross validation",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Time series cross-validation‚Äù."
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html",
    "href": "examples/getting_started_with_auto_arima_and_ets.html",
    "title": "Forecast with ARIMA and ETS",
    "section": "",
    "text": "Tip\n\n\n\nYou can use Colab to run this Notebook interactively\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#introduction",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#introduction",
    "title": "Forecast with ARIMA and ETS",
    "section": "Introduction",
    "text": "Introduction\nAutomatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\nThe R‚Äôs programming language offers two great packages by Rob Hyndmann for Automatic forecasting:\n\nAutoARIMA and\nETS\n\nBoth models are highly accurate and reliable. Time series practitioners use them as reference or baseline models in a variety of forecasting tasks.\nBefore StatsForecast, similar alternatives -in terms of accuracy and computational efficiency- did not exist for the Python ecosystem. Seeking to bridge that Gap, we developed a new and highly efficient pure-Python implementation of these classic algorithms. In this notebook, we will show you how to use them."
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#install-the-statsforecast-library",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#install-the-statsforecast-library",
    "title": "Forecast with ARIMA and ETS",
    "section": "Install the StatsForecast Library",
    "text": "Install the StatsForecast Library\n\n!pip install statsforecast"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#load-the-data",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#load-the-data",
    "title": "Forecast with ARIMA and ETS",
    "section": "Load the Data",
    "text": "Load the Data\nFor this notebook, we will use the classical AirPassenger Data set. For simplicity‚Äôs sake, you can import it from StatsForecast.\n\nfrom statsforecast.utils import AirPassengersDF\n\nY_df = AirPassengersDF\n\nY_df.head()\n\n/Users/max.mergenthaler/Nixtla/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0\n    \n  \n\n\n\n\nSplit the dataset in train and test to evaluate your model at a later stage.\n\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 monthly observations for train\nY_test_df = Y_df[Y_df.ds>'1959-12-31'] # 12 monthly observations for test\n\n\n\n\n\n\n\nTip\n\n\n\nCross Validation or Backtesting is the recommended method to evaluate performance of time series models. Cross-validation consists in evaluating the performance of a certain model across different past windows. You can use the StatsForecast.cross_validation method from the StatsForecast class. See also: Cross Validation for Time Series.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, Markdown\n\nimport matplotlib.pyplot as plt\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, ETS\nfrom statsforecast.utils import AirPassengersDF"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#forecast-with-different-models",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#forecast-with-different-models",
    "title": "Forecast with ARIMA and ETS",
    "section": "Forecast with different models",
    "text": "Forecast with different models\nStatsForecast includes a wide range of models. For this example we will use two classical univarate models:\nETS: The exponential smoothing (ETS) algorithm is especially suited for data with seasonality and trend. ETS computes a weighted average over all observations in the input time series dataset as its prediction. In contrast to moving average methods with constant weights, ETS weights exponentially decrease over time, capturing long term dependencies while prioritizing new observations.\nAutoARIMA: The autoregressive integrated moving average (ARIMA), combines differencing steps, lag regression and moving averages into a single method capable of modeling non-stationary time series. This method complements on ETS and it is based on the description of data‚Äôs autocorrelations.\nIt is always a good idea to include benchmark models to have an estimate of how much accuracy we are gaining. For this exercise, we will use a Naive model.\n\nfrom statsforecast import StatsForecast #Imports the core StatsForecast class\nfrom statsforecast.models import AutoARIMA, ETS, Naive #Imports the models you will use\n\nDefine the parameters that you want to use in your models.\n\nseason_length = 12 # Monthly data \nhorizon = len(Y_test_df) # Predict the lenght of the test df\n\n# Include the models you imported\nmodels = [\n    AutoARIMA(season_length=season_length),\n    ETS(season_length=season_length),\n    Naive()\n]\n\n# Instansiate the StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_train_df,\n    models=models,\n    freq='M', \n    n_jobs=-1\n)\n\n# Forecast for the defined horizon\nY_hat_df = sf.forecast(horizon)\n\nY_hat_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      ETS\n      Naive\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1960-01-31\n      424.160156\n      406.651276\n      405.0\n    \n    \n      1.0\n      1960-02-29\n      407.081696\n      401.732910\n      405.0\n    \n    \n      1.0\n      1960-03-31\n      470.860535\n      456.289642\n      405.0\n    \n    \n      1.0\n      1960-04-30\n      460.913605\n      440.870514\n      405.0\n    \n    \n      1.0\n      1960-05-31\n      484.900879\n      440.333923\n      405.0\n    \n  \n\n\n\n\nFor efficiency‚Äôs sake, the forecast method converts the unique_id column to an index. You can revert to the default index of the data frame using the pd.reset_index method from Pandas.\n\nY_hat_df.reset_index()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoARIMA\n      ETS\n      Naive\n    \n  \n  \n    \n      0\n      1.0\n      1960-01-31\n      424.160156\n      406.651276\n      405.0\n    \n    \n      1\n      1.0\n      1960-02-29\n      407.081696\n      401.732910\n      405.0\n    \n    \n      2\n      1.0\n      1960-03-31\n      470.860535\n      456.289642\n      405.0\n    \n    \n      3\n      1.0\n      1960-04-30\n      460.913605\n      440.870514\n      405.0\n    \n    \n      4\n      1.0\n      1960-05-31\n      484.900879\n      440.333923\n      405.0\n    \n    \n      5\n      1.0\n      1960-06-30\n      536.903931\n      496.866058\n      405.0\n    \n    \n      6\n      1.0\n      1960-07-31\n      612.903198\n      545.839111\n      405.0\n    \n    \n      7\n      1.0\n      1960-08-31\n      623.903381\n      544.672485\n      405.0\n    \n    \n      8\n      1.0\n      1960-09-30\n      527.903320\n      477.034485\n      405.0\n    \n    \n      9\n      1.0\n      1960-10-31\n      471.903320\n      412.423096\n      405.0\n    \n    \n      10\n      1.0\n      1960-11-30\n      426.903320\n      357.949158\n      405.0\n    \n    \n      11\n      1.0\n      1960-12-31\n      469.903320\n      402.032745\n      405.0"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#plot-the-predictions",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#plot-the-predictions",
    "title": "Forecast with ARIMA and ETS",
    "section": "Plot the predictions",
    "text": "Plot the predictions\nPlot the forecasts (also known as Y hat) against the real values of test using Matplot lib.\n\nimport matplotlib.pyplot as plt\n\n\n# Merge the forecasts with the true values\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\nplot_df[['y', 'AutoARIMA', 'ETS']].plot(ax=ax, linewidth=2)\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()"
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#evaluate-the-predictions",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#evaluate-the-predictions",
    "title": "Forecast with ARIMA and ETS",
    "section": "Evaluate the predictions",
    "text": "Evaluate the predictions\nFinally, we evaluate the predictions accuracy using the Mean Absolute Error:\n\\[\n\\qquad MAE = \\frac{1}{Horizon} \\sum_{\\tau} |y_{\\tau} - \\hat{y}_{\\tau}|\\qquad\n\\]\n\ndef mae(y_hat, y_true):\n    return np.mean(np.abs(y_hat-y_true))\n\ny_true = Y_test_df['y'].values\nets_preds = Y_hat_df['ETS'].values\narima_preds = Y_hat_df['AutoARIMA'].values\nnaive_preds = Y_hat_df['Naive'].values\n\nprint('ETS   MAE: %0.3f' % mae(ets_preds, y_true))\nprint('ARIMA MAE: %0.3f' % mae(arima_preds, y_true))\nprint('Naive MAE: %0.3f' % mae(naive_preds, y_true))\n\nETS   MAE: 35.612\nARIMA MAE: 18.551\nNaive MAE: 76.000\n\n\nThe best-performing model for this dataset is the ARIMA model. Notice that in both cases, our models are fare better estimates of the future than our baseline model.\n\n\n\n\n\n\nTip\n\n\n\nFor a complete list of available automatic forecasting models -as well as benchmark models- visit the model‚Äôs section of the documentation."
  },
  {
    "objectID": "examples/getting_started_with_auto_arima_and_ets.html#references",
    "href": "examples/getting_started_with_auto_arima_and_ets.html#references",
    "title": "Forecast with ARIMA and ETS",
    "section": "References",
    "text": "References\nHyndman, RJ and Khandakar, Y (2008) ‚ÄúAutomatic time series forecasting: The forecast package for R‚Äù, Journal of Statistical Software, 26(3)."
  },
  {
    "objectID": "examples/ets_ray_m5.html",
    "href": "examples/ets_ray_m5.html",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "",
    "text": "In this notebook we show how to use StatsForecast and ray to forecast thounsands of time series in less than 6 minutes (M5 dataset). Also, we show that StatsForecast has better performance in time and accuracy compared to Prophet running on a Spark cluster using DataBricks.\nIn this example, we used a ray cluster (AWS) of 11 instances of type m5.2xlarge (8 cores, 32 GB RAM).\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/ets_ray_m5.html#installing-statsforecast-library",
    "href": "examples/ets_ray_m5.html#installing-statsforecast-library",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install \"statsforecast[ray]\" neuralforecast s3fs pyarrow\n\n\nfrom time import time\n\nimport pandas as pd\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import ETS"
  },
  {
    "objectID": "examples/ets_ray_m5.html#download-data",
    "href": "examples/ets_ray_m5.html#download-data",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Download data",
    "text": "Download data\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\n\nY_df = pd.read_parquet('s3://m5-benchmarks/data/train/target.parquet')\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      FOODS_1_001_CA_1\n      2011-01-29\n      3.0\n    \n    \n      1\n      FOODS_1_001_CA_1\n      2011-01-30\n      0.0\n    \n    \n      2\n      FOODS_1_001_CA_1\n      2011-01-31\n      0.0\n    \n    \n      3\n      FOODS_1_001_CA_1\n      2011-02-01\n      1.0\n    \n    \n      4\n      FOODS_1_001_CA_1\n      2011-02-02\n      4.0\n    \n  \n\n\n\n\nSince the M5 dataset contains intermittent time series, we add a constant to avoid problems during the training phase. Later, we will substract the constant from the forecasts.\n\nconstant = 10\nY_df['y'] += constant"
  },
  {
    "objectID": "examples/ets_ray_m5.html#train-the-model",
    "href": "examples/ets_ray_m5.html#train-the-model",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Train the model",
    "text": "Train the model\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality. Observe that we need to pass the ray address to the ray_address argument.\n\nfcst = StatsForecast(\n    df=Y_df, \n    models=[ETS(season_length=7, model='ZNA')], \n    freq='D', \n    #n_jobs=-1\n    ray_address='ray://ADDRESS:10001'\n)\n\n\ninit = time()\nY_hat = fcst.forecast(28)\nend = time()\nprint(f'Minutes taken by StatsForecast using: {(end - init) / 60}')\n\n/home/ubuntu/miniconda/envs/ray/lib/python3.7/site-packages/ray/util/client/worker.py:618: UserWarning: More than 10MB of messages have been created to schedule tasks on the server. This can be slow on Ray Client due to communication overhead over the network. If you're running many fine-grained tasks, consider running them inside a single remote function. See the section on \"Too fine-grained tasks\" in the Ray Design Patterns document for more details: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.f7ins22n6nyl. If your functions frequently use large objects, consider storing the objects remotely with ray.put. An example of this is shown in the \"Closure capture of large / unserializable object\" section of the Ray Design Patterns document, available here: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.1afmymq455wu\n  UserWarning,\n\n\nMinutes taken by StatsForecast using: 5.4817593971888225\n\n\nStatsForecast and ray took only 5.48 minutes to train 30,490 time series, compared to 18.23 minutes for Prophet and Spark.\nWe remove the constant.\n\nY_hat['ETS'] -= constant\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = Y_hat.reset_index().set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50.2M/50.2M [00:00<00:00, 77.1MiB/s]\n\n\n\nM5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\n\n\n\n  \n    \n      \n      wrmsse\n    \n  \n  \n    \n      Total\n      0.677233\n    \n    \n      Level1\n      0.435558\n    \n    \n      Level2\n      0.522863\n    \n    \n      Level3\n      0.582109\n    \n    \n      Level4\n      0.488484\n    \n    \n      Level5\n      0.567825\n    \n    \n      Level6\n      0.587605\n    \n    \n      Level7\n      0.662774\n    \n    \n      Level8\n      0.647712\n    \n    \n      Level9\n      0.732107\n    \n    \n      Level10\n      1.013124\n    \n    \n      Level11\n      0.970465\n    \n    \n      Level12\n      0.916175\n    \n  \n\n\n\n\nAlso, StatsForecast is more accurate than Prophet, since the overall WMRSSE is 0.68, against 0.77 obtained by prophet."
  },
  {
    "objectID": "examples/getting_started_complete.html",
    "href": "examples/getting_started_complete.html",
    "title": "End to End Walkthrough",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/getting_started_complete.html#install-libraries",
    "href": "examples/getting_started_complete.html#install-libraries",
    "title": "End to End Walkthrough",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS and datasetsforecast for common error metrics like MAE or MASE.\nInstall the necessary packages using pip install statsforecast s3fs datasetsforecast ``"
  },
  {
    "objectID": "examples/getting_started_complete.html#read-the-data",
    "href": "examples/getting_started_complete.html#read-the-data",
    "title": "End to End Walkthrough",
    "section": "Read the data",
    "text": "Read the data\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nThis data set already satisfies the requirement.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport pandas as pd\n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      H1\n      1\n      605.0\n    \n    \n      1\n      H1\n      2\n      586.0\n    \n    \n      2\n      H1\n      3\n      586.0\n    \n    \n      3\n      H1\n      4\n      559.0\n    \n    \n      4\n      H1\n      5\n      511.0\n    \n  \n\n\n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility‚Äôs sake, we will select only 10 unique IDs and keep only the last week. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\nuids = Y_df['unique_id'].unique()[:10] # Select 10 ids to make the example faster\n\nY_df = Y_df.query('unique_id in @uids') \n\nY_df = Y_df.groupby('unique_id').tail(7 * 24) #Select last 7 days of data to make example faster"
  },
  {
    "objectID": "examples/getting_started_complete.html#explore-data-with-the-plot-method",
    "href": "examples/getting_started_complete.html#explore-data-with-the-plot-method",
    "title": "End to End Walkthrough",
    "section": "Explore Data with the plot method",
    "text": "Explore Data with the plot method\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a defaul engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)\n\n/Users/max.mergenthaler/Nixtla/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm"
  },
  {
    "objectID": "examples/getting_started_complete.html#train-multiple-models-for-many-series",
    "href": "examples/getting_started_complete.html#train-multiple-models-for-many-series",
    "title": "End to End Walkthrough",
    "section": "Train multiple models for many series",
    "text": "Train multiple models for many series\nStatsForecast can train many models on many time series efficiently.\nStart by importing and instantiating the desired models. StatsForecast offers a wide variety of models grouped in the following categories:\n\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt‚Äôs Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nHere you can check the complete list of models.\nFor this example we will use:\n\nAutoARIMA: Automatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Ref: AutoARIMA.\nHoltWinters: triple exponential smoothing, Holt-Winters‚Äô method is an extension of exponential smoothing for series that contain both trend and seasonality. Ref: HoltWinters\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive\nHistoricAverage: arthimetic mean. Ref: HistoricAverage.\nDynamicOptimizedTheta: The theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series. Ref: DynamicOptimizedTheta.\n\nImport and instantiate the models. Setting the season_length argument is sometimes tricky. This article on Seasonal periods) by the master, Rob Hyndmann, can be useful.\n\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive\n)\n\n\n# Create a list of models and instantiation parameters\nmodels = [\n    AutoARIMA(season_length=24),\n    HoltWinters(),\n    Croston(),\n    SeasonalNaive(season_length=24),\n    HistoricAverage(),\n    DOT(season_length=24)\n]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda‚Äôs available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n    n_jobs=-1,\n    fallback_model = SeasonalNaive(season_length=7)\n)\n\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nThe forecast method is compatible with distributed clusters, so it does not store any model parameters. If you want to store parameters for every model you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nforecasts_df = sf.forecast(h=48, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      AutoARIMA-lo-90\n      AutoARIMA-hi-90\n      HoltWinters\n      CrostonClassic\n      SeasonalNaive\n      SeasonalNaive-lo-90\n      SeasonalNaive-hi-90\n      HistoricAverage\n      HistoricAverage-lo-90\n      HistoricAverage-hi-90\n      DynamicOptimizedTheta\n      DynamicOptimizedTheta-lo-90\n      DynamicOptimizedTheta-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      749\n      592.461792\n      572.325623\n      612.597961\n      829.0\n      708.21405\n      635.0\n      537.471191\n      732.528809\n      660.982117\n      398.03772\n      923.926514\n      592.701843\n      577.677307\n      611.652649\n    \n    \n      H1\n      750\n      527.174316\n      495.321777\n      559.026855\n      807.0\n      708.21405\n      572.0\n      474.471222\n      669.528809\n      660.982117\n      398.03772\n      923.926514\n      525.589111\n      505.449738\n      546.621826\n    \n    \n      H1\n      751\n      488.418549\n      445.535583\n      531.301514\n      785.0\n      708.21405\n      532.0\n      434.471222\n      629.528809\n      660.982117\n      398.03772\n      923.926514\n      489.251801\n      462.072876\n      512.424133\n    \n    \n      H1\n      752\n      452.284454\n      400.677155\n      503.891785\n      756.0\n      708.21405\n      493.0\n      395.471222\n      590.528809\n      660.982117\n      398.03772\n      923.926514\n      456.195038\n      430.554291\n      478.260956\n    \n    \n      H1\n      753\n      433.127563\n      374.070984\n      492.184143\n      719.0\n      708.21405\n      477.0\n      379.471222\n      574.528809\n      660.982117\n      398.03772\n      923.926514\n      436.290527\n      411.051239\n      461.815948\n    \n  \n\n\n\n\nPlot the results of 8 randon series using the StatsForecast.plot method.\n\nsf.plot(Y_df,forecasts_df)\n\n\n                                                \n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nsf.plot(Y_df, forecasts_df, models=[\"HoltWinters\",\"DynamicOptimizedTheta\"], unique_ids=[\"H10\", \"H105\"], level=[90])\n\n\n                                                \n\n\n\n# Explore other models \nsf.plot(Y_df, forecasts_df, models=[\"AutoARIMA\"], unique_ids=[\"H10\", \"H105\"], level=[90])"
  },
  {
    "objectID": "examples/getting_started_complete.html#evaluate-the-models-performance",
    "href": "examples/getting_started_complete.html#evaluate-the-models-performance",
    "title": "End to End Walkthrough",
    "section": "Evaluate the model‚Äôs performance",
    "text": "Evaluate the model‚Äôs performance\nIn previous steps, we‚Äôve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model‚Äôs predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 2 days (n_windows=2), forecasting every second day (step_size=48). Depending on your computer, this step should take around 1 min.\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvaldation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=2\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model‚Äôs name and fitted value.\n\n\ncrossvaldation_df.head()\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      AutoARIMA\n      HoltWinters\n      CrostonClassic\n      SeasonalNaive\n      HistoricAverage\n      DynamicOptimizedTheta\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      701\n      700\n      619.0\n      603.925415\n      847.0\n      742.668762\n      691.0\n      661.674988\n      612.767517\n    \n    \n      H1\n      702\n      700\n      565.0\n      507.591736\n      820.0\n      742.668762\n      618.0\n      661.674988\n      536.846252\n    \n    \n      H1\n      703\n      700\n      532.0\n      481.281677\n      790.0\n      742.668762\n      563.0\n      661.674988\n      497.824280\n    \n    \n      H1\n      704\n      700\n      495.0\n      444.410248\n      784.0\n      742.668762\n      529.0\n      661.674988\n      464.723236\n    \n    \n      H1\n      705\n      700\n      481.0\n      421.168762\n      752.0\n      742.668762\n      504.0\n      661.674988\n      440.972351\n    \n  \n\n\n\n\nNext, we will evaluate the performance of every model for every series using common error metrics like Mean Absolute Error (MAE) or Mean Square Error (MSE) Define a utility function to evaluate different error metrics for the cross validation data frame.\nFirst import the desired error metrics from datasetsforecast.losses. Then define a utility function that takes a cross-validation data frame as a metric and returns an evaluation data frame with the average of the error metric for every unique id and fitted model and all cutoffs.\n\nfrom datasetsforecast.losses import mse, mae, rmse\n\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['ds', 'cutoff', 'y']).columns.tolist()\n    evals = []\n    for model in models:\n        eval_ = df.groupby(['unique_id', 'cutoff']).apply(lambda x: metric(x['y'].values, x[model].values)).to_frame() # Calculate loss for every unique_id, model and cutoff.\n        eval_.columns = [model]\n        evals.append(eval_)\n    evals = pd.concat(evals, axis=1)\n    evals = evals.groupby(['unique_id']).mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n    evals['best_model'] = evals.idxmin(axis=1)\n    return evals\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = evaluate_cross_validation(crossvaldation_df, mse)\n\nevaluation_df.head()\n\n\n\n\n\n  \n    \n      \n      AutoARIMA\n      HoltWinters\n      CrostonClassic\n      SeasonalNaive\n      HistoricAverage\n      DynamicOptimizedTheta\n      best_model\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      1979.302246\n      44888.019531\n      28038.736328\n      1422.666748\n      20927.664062\n      1296.333984\n      DynamicOptimizedTheta\n    \n    \n      H10\n      458.892700\n      2812.916504\n      1483.484131\n      96.895828\n      1980.367432\n      379.621124\n      SeasonalNaive\n    \n    \n      H100\n      8629.948242\n      121625.375000\n      91945.140625\n      12019.000000\n      78491.187500\n      21699.648438\n      AutoARIMA\n    \n    \n      H101\n      6818.348633\n      28453.394531\n      16183.634766\n      10944.458008\n      18208.404297\n      63698.074219\n      AutoARIMA\n    \n    \n      H102\n      65489.968750\n      232924.843750\n      132655.296875\n      12699.896484\n      309110.468750\n      31393.521484\n      SeasonalNaive\n    \n  \n\n\n\n\nCreate a summary table with a model column and the number of series where that model performs best. In this case, the Arima and Seasonal Naive are the best models for 10 series and the Theta model should be used for two.\n\nsummary_df = evaluation_df.groupby('best_model').size().sort_values().to_frame()\n\nsummary_df.reset_index().columns = [\"Model\", \"Nr. of unique_ids\"]\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nseasonal_ids = evaluation_df.query('best_model == \"SeasonalNaive\"').index\n\nsf.plot(Y_df,forecasts_df, unique_ids=seasonal_ids, models=[\"SeasonalNaive\",\"DynamicOptimizedTheta\"])"
  },
  {
    "objectID": "examples/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "href": "examples/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "title": "End to End Walkthrough",
    "section": "Select the best model for every unique series",
    "text": "Select the best model for every unique series\nDefine a utility function that takes your forecast‚Äôs data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df):\n    df = forecasts_df.set_index('ds', append=True).stack().to_frame().reset_index(level=2) # Wide to long \n    df.columns = ['model', 'best_model_forecast'] \n    df = df.join(evaluation_df[['best_model']])\n    df = df.query('model.str.replace(\"-lo-90|-hi-90\", \"\", regex=True) == best_model').copy()\n    df.loc[:, 'model'] = [model.replace(bm, 'best_model') for model, bm in zip(df['model'], df['best_model'])]\n    df = df.drop(columns='best_model').set_index('model', append=True).unstack()\n    df.columns = df.columns.droplevel()\n    df = df.reset_index(level=1)\n    return df\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(forecasts_df, evaluation_df)\n\nprod_forecasts_df.head()\n\n\n\n\n\n  \n    \n      model\n      ds\n      best_model\n      best_model-hi-90\n      best_model-lo-90\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      H1\n      749\n      592.701843\n      611.652649\n      577.677307\n    \n    \n      H1\n      750\n      525.589111\n      546.621826\n      505.449738\n    \n    \n      H1\n      751\n      489.251801\n      512.424133\n      462.072876\n    \n    \n      H1\n      752\n      456.195038\n      478.260956\n      430.554291\n    \n    \n      H1\n      753\n      436.290527\n      461.815948\n      411.051239\n    \n  \n\n\n\n\nPlot the results.\n\nsf.plot(Y_df, prod_forecasts_df, level=[90])"
  },
  {
    "objectID": "examples/installation.html",
    "href": "examples/installation.html",
    "title": "Install",
    "section": "",
    "text": "You can install the released version of StatsForecast from the Python package index with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\n\n\n\n\n\n\nWarning\n\n\n\nWe are constantly updating StatsForecast, so we suggest fixing the version to avoid issues. pip install statsforecast==\"1.0.0\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe recommend installing your libraries inside a python virtual or conda environment.\n\n\n\nUser our env (optional)\nIf you don‚Äôt have a Conda environment and need tools like Numba, Pandas, NumPy, Jupyter, StatsModels, and Nbdev you can use ours by following these steps:\n\nClone the StatsForecast repo:\n\n$ git clone https://github.com/Nixtla/statsforecast.git && cd statsforecast\n\nCreate the environment using the environment.yml file:\n\n$ conda env create -f environment.yml\n\nActivate the environment:\n\n$ conda activate statsforecast\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/prophet_spark_m5.html",
    "href": "examples/prophet_spark_m5.html",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "",
    "text": "The purpose of this notebook is to create a scalability benchmark (time and performance). To that end, Nixtla‚Äôs StatsForecast (using the ETS model) is trained on the M5 dataset using spark to distribute the training. As a comparison, Facebook‚Äôs Prophet model is used.\nAn AWS cluster (mounted on databricks) of 11 instances of type m5.2xlarge (8 cores, 32 GB RAM) with runtime 10.4 LTS was used. This notebook was used as base case.\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/prophet_spark_m5.html#main-results",
    "href": "examples/prophet_spark_m5.html#main-results",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Main results",
    "text": "Main results\n\n\n\nMethod\nTime (mins)\nPerformance (wRMSSE)\n\n\n\n\nStatsForecast\n7.5\n0.68\n\n\nProphet\n18.23\n0.77"
  },
  {
    "objectID": "examples/prophet_spark_m5.html#installing-libraries",
    "href": "examples/prophet_spark_m5.html#installing-libraries",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Installing libraries",
    "text": "Installing libraries\n\npip install prophet \"neuralforecast<1.0.0\" \"statsforecast[fugue]\""
  },
  {
    "objectID": "examples/prophet_spark_m5.html#statsforecast-pipeline",
    "href": "examples/prophet_spark_m5.html#statsforecast-pipeline",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "StatsForecast pipeline",
    "text": "StatsForecast pipeline\n\nfrom time import time\n\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom statsforecast.distributed.utils import forecast\nfrom statsforecast.distributed.fugue import FugueBackend\nfrom statsforecast.models import ETS, SeasonalNaive\nfrom statsforecast.core import StatsForecast\n\nfrom pyspark.sql import SparkSession\n\n\n\n\n\n\nspark = SparkSession.builder.getOrCreate()\nbackend = FugueBackend(spark, {\"fugue.spark.use_pandas_udf\":True})\n\n\n\n\n\n\nForecast\nWith statsforecast you don‚Äôt have to download your data. The distributed backend can handle a file with your data.\n\ninit = time()\nets_forecasts = backend.forecast(\n    \"s3://m5-benchmarks/data/train/m5-target.parquet\", \n    [ETS(season_length=7, model='ZAA')], \n    freq=\"D\", \n    h=28, \n).toPandas()\nend = time()\nprint(f'Minutes taken by StatsForecast on a Spark cluster: {(end - init) / 60}')\n\n\nMinutes taken by StatsForecast on a Spark cluster: 7.471468730767568\n\n\n\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = ets_forecasts.set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n\n\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])#.drop(columns=['unique_id'])\n\n\nwrmsse_ets = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\nwrmsse_ets\n\n\nOut[14]: \n\n\n\n\n\n\n  \n    \n      \n      wrmsse\n    \n  \n  \n    \n      Total\n      0.682358\n    \n    \n      Level1\n      0.449115\n    \n    \n      Level2\n      0.533754\n    \n    \n      Level3\n      0.592317\n    \n    \n      Level4\n      0.497086\n    \n    \n      Level5\n      0.572189\n    \n    \n      Level6\n      0.593880\n    \n    \n      Level7\n      0.665358\n    \n    \n      Level8\n      0.652183\n    \n    \n      Level9\n      0.734492\n    \n    \n      Level10\n      1.012633\n    \n    \n      Level11\n      0.969902\n    \n    \n      Level12\n      0.915380"
  },
  {
    "objectID": "examples/prophet_spark_m5.html#prophet-pipeline",
    "href": "examples/prophet_spark_m5.html#prophet-pipeline",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Prophet pipeline",
    "text": "Prophet pipeline\n\nimport logging\nfrom time import time\n\nimport pandas as pd\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom prophet import Prophet\nfrom pyspark.sql.types import *\n\n# disable informational messages from prophet\nlogging.getLogger('py4j').setLevel(logging.ERROR)\n\n\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\n\n\n\n\nDownload data\n\n# structure of the training data set\ntrain_schema = StructType([\n  StructField('unique_id', StringType()),  \n  StructField('ds', DateType()),\n  StructField('y', DoubleType())\n  ])\n \n# read the training file into a dataframe\ntrain = spark.read.parquet(\n  's3://m5-benchmarks/data/train/m5-target.parquet', \n  header=True, \n  schema=train_schema\n )\n \n# make the dataframe queriable as a temporary view\ntrain.createOrReplaceTempView('train')\n\n\n\n\n\n\nsql_statement = '''\n  SELECT\n    unique_id AS unique_id,\n    CAST(ds as date) as ds,\n    y as y\n  FROM train\n  '''\n \nm5_history = (\n  spark\n    .sql( sql_statement )\n    .repartition(sc.defaultParallelism, ['unique_id'])\n  ).cache()\n\n\n\n\n\n\n\nForecast function using Prophet\n\ndef forecast( history_pd: pd.DataFrame ) -> pd.DataFrame:\n  \n  # TRAIN MODEL AS BEFORE\n  # --------------------------------------\n  # remove missing values (more likely at day-store-item level)\n    history_pd = history_pd.dropna()\n\n    # configure the model\n    model = Prophet(\n        growth='linear',\n        daily_seasonality=False,\n        weekly_seasonality=True,\n        yearly_seasonality=True,\n        seasonality_mode='multiplicative'\n    )\n\n    # train the model\n    model.fit( history_pd )\n    # --------------------------------------\n\n    # BUILD FORECAST AS BEFORE\n    # --------------------------------------\n    # make predictions\n    future_pd = model.make_future_dataframe(\n        periods=28, \n        freq='d', \n        include_history=False\n    )\n    forecast_pd = model.predict( future_pd )  \n    # --------------------------------------\n\n    # ASSEMBLE EXPECTED RESULT SET\n    # --------------------------------------\n    # get relevant fields from forecast\n    forecast_pd['unique_id'] = history_pd['unique_id'].unique()[0]\n    f_pd = forecast_pd[['unique_id', 'ds','yhat']]\n    # --------------------------------------\n\n    # return expected dataset\n    return f_pd\n\n\n\n\n\n\nresult_schema = StructType([\n  StructField('unique_id', StringType()), \n  StructField('ds',DateType()),\n  StructField('yhat',FloatType()),\n])\n\n\n\n\n\n\nTraining Prophet on the M5 dataset\n\ninit = time()\nresults = (\n  m5_history\n    .groupBy('unique_id')\n      .applyInPandas(forecast, schema=result_schema)\n    ).toPandas()\nend = time()\nprint(f'Minutes taken by Prophet on a Spark cluster: {(end - init) / 60}')\n\n\nMinutes taken by Prophet on a Spark cluster: 18.23116923570633\n\n\n\n\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = results.set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n\n\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])#.drop(columns=['unique_id'])\n\n\n\n\n\n\nwrmsse = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\n\n\n\n\nwrmsse\n\n\nOut[10]: \n\n\n\n\n\n\n  \n    \n      \n      wrmsse\n    \n  \n  \n    \n      Total\n      0.771800\n    \n    \n      Level1\n      0.507905\n    \n    \n      Level2\n      0.586328\n    \n    \n      Level3\n      0.666686\n    \n    \n      Level4\n      0.549358\n    \n    \n      Level5\n      0.655003\n    \n    \n      Level6\n      0.647176\n    \n    \n      Level7\n      0.747047\n    \n    \n      Level8\n      0.743422\n    \n    \n      Level9\n      0.824667\n    \n    \n      Level10\n      1.207069\n    \n    \n      Level11\n      1.108780\n    \n    \n      Level12\n      1.018163"
  },
  {
    "objectID": "examples/exogenous.html",
    "href": "examples/exogenous.html",
    "title": "Exogenous Regressors",
    "section": "",
    "text": "With StatsForecast you can easily include exogenous regressors. Some methods, such as AutoARIMA, have the ability to include exogenous regressors, while other models only use the time series information. StatsForecast takes care of passing the exogenous variables to the models that use them.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "examples/exogenous.html#install-statsforecast",
    "href": "examples/exogenous.html#install-statsforecast",
    "title": "Exogenous Regressors",
    "section": "Install statsforecast",
    "text": "Install statsforecast\n\n!pip install statsforecast\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.utils import AirPassengersDF as Y_df\n\nIn this example, we will use the AirPassengers dataset to show how to work with exogenous regressors.\n\nY_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0"
  },
  {
    "objectID": "examples/exogenous.html#split-traintest-sets",
    "href": "examples/exogenous.html#split-traintest-sets",
    "title": "Exogenous Regressors",
    "section": "Split train/test sets",
    "text": "Split train/test sets\nWe will use the last 12 observations of the dataset as the test set.\n\nhorizon = 12\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31'] # 12 test"
  },
  {
    "objectID": "examples/exogenous.html#add-exogenous-regressors",
    "href": "examples/exogenous.html#add-exogenous-regressors",
    "title": "Exogenous Regressors",
    "section": "Add exogenous regressors",
    "text": "Add exogenous regressors\nIn this example, we will include the trend as exogenous regressor. But you can include as many as you have or want.\n\nY_train_df['trend'] = np.arange(1, len(Y_train_df) + 1)\n\n\nY_train_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n      trend\n    \n  \n  \n    \n      0\n      1.0\n      1949-01-31\n      112.0\n      1\n    \n    \n      1\n      1.0\n      1949-02-28\n      118.0\n      2\n    \n    \n      2\n      1.0\n      1949-03-31\n      132.0\n      3\n    \n    \n      3\n      1.0\n      1949-04-30\n      129.0\n      4\n    \n    \n      4\n      1.0\n      1949-05-31\n      121.0\n      5\n    \n  \n\n\n\n\n\nY_train_df.tail()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      y\n      trend\n    \n  \n  \n    \n      127\n      1.0\n      1959-08-31\n      559.0\n      128\n    \n    \n      128\n      1.0\n      1959-09-30\n      463.0\n      129\n    \n    \n      129\n      1.0\n      1959-10-31\n      407.0\n      130\n    \n    \n      130\n      1.0\n      1959-11-30\n      362.0\n      131\n    \n    \n      131\n      1.0\n      1959-12-31\n      405.0\n      132\n    \n  \n\n\n\n\nObserve that the exogenous regressors have to be placed after the target variable y.\n\nCreate future exogenous regressors\nIn order for the model to produce forecasts, it needs the future exogenous regressors. In this section we will construct a dataframe that includes the future trend.\n\nX_test_df = pd.DataFrame({\n  'unique_id': 1.0,\n  'ds': pd.date_range(start='1960-01-01', periods=horizon, freq='M')\n})\n# We construct xreg for test. The train series ends at the 133th step. \nX_test_df['trend'] = np.arange(133, 133 + horizon)\nX_test_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      trend\n    \n  \n  \n    \n      0\n      1.0\n      1960-01-31\n      133\n    \n    \n      1\n      1.0\n      1960-02-29\n      134\n    \n    \n      2\n      1.0\n      1960-03-31\n      135\n    \n    \n      3\n      1.0\n      1960-04-30\n      136\n    \n    \n      4\n      1.0\n      1960-05-31\n      137"
  },
  {
    "objectID": "examples/exogenous.html#train-the-model",
    "href": "examples/exogenous.html#train-the-model",
    "title": "Exogenous Regressors",
    "section": "Train the model",
    "text": "Train the model\n\nseason_length = 12\nmodel = StatsForecast(\n    df=Y_train_df, \n    models=[AutoARIMA(season_length=12)], \n    freq='M', \n    n_jobs=-1\n)\n\n\nForecast mode\nThe StatsForecast.forecast method is more computationally efficient since it does not save objects during training and predicting. The method receives the future exogenous regressors X_test_df.\n\nY_hat_df = model.forecast(horizon, X_df=X_test_df)\nY_hat_df = Y_hat_df.reset_index()\nY_hat_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoARIMA\n    \n  \n  \n    \n      0\n      1.0\n      1960-01-31\n      414.551483\n    \n    \n      1\n      1.0\n      1960-02-29\n      387.550842\n    \n    \n      2\n      1.0\n      1960-03-31\n      445.526978\n    \n    \n      3\n      1.0\n      1960-04-30\n      431.495422\n    \n    \n      4\n      1.0\n      1960-05-31\n      452.797211\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = Y_test_df.merge(Y_hat_df, how='left', on=['unique_id', 'ds'])\ndf_plot = pd.concat([Y_train_df, Y_hat_df_xreg]).set_index('ds')\ndf_plot[['y', 'AutoARIMA']].plot(ax=ax, linewidth=2)\nax.set_title('AirPassengers Forecast (with AutoARIMA external regressors)', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\nfor label in (ax.get_xticklabels() + ax.get_yticklabels()):\n    label.set_fontsize(20)\n\n\n\n\n\n\nSKlearn syntax\nThe sklearn syntax can also be used. Fist, train the model using the StatsForecast.fit method.\n\nmodel.fit()\n\nStatsForecast(models=[AutoARIMA])\n\n\nThe use the fitted model to produce forecasts. Observe that the StatsForecast.predict method receives the future exogenous regressors.\n\nmodel.predict(horizon, X_df=X_test_df)\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      1.0\n      1960-01-31\n      414.551483\n    \n    \n      1.0\n      1960-02-29\n      387.550842\n    \n    \n      1.0\n      1960-03-31\n      445.526978\n    \n    \n      1.0\n      1960-04-30\n      431.495422\n    \n    \n      1.0\n      1960-05-31\n      452.797211\n    \n    \n      1.0\n      1960-06-30\n      502.991394\n    \n    \n      1.0\n      1960-07-31\n      577.782837\n    \n    \n      1.0\n      1960-08-31\n      587.973938\n    \n    \n      1.0\n      1960-09-30\n      491.432617\n    \n    \n      1.0\n      1960-10-31\n      435.070312\n    \n    \n      1.0\n      1960-11-30\n      389.827820\n    \n    \n      1.0\n      1960-12-31\n      432.665527"
  },
  {
    "objectID": "examples/exogenous.html#including-prediction-intervals",
    "href": "examples/exogenous.html#including-prediction-intervals",
    "title": "Exogenous Regressors",
    "section": "Including prediction intervals",
    "text": "Including prediction intervals\nYou can also compute prediction intervals using exogenous regressors. Simply add the level argument.\n\nForecast mode\n\nY_hat_df = model.forecast(horizon, X_df=X_test_df, level=(80,95))\nY_hat_df = Y_hat_df.reset_index()\nY_hat_df.head()\n\n\n\n\n\n  \n    \n      \n      unique_id\n      ds\n      AutoARIMA\n      AutoARIMA-lo-95\n      AutoARIMA-lo-80\n      AutoARIMA-hi-80\n      AutoARIMA-hi-95\n    \n  \n  \n    \n      0\n      1.0\n      1960-01-31\n      414.551483\n      393.468414\n      400.765991\n      428.337006\n      435.634583\n    \n    \n      1\n      1.0\n      1960-02-29\n      387.550842\n      362.181641\n      370.962830\n      404.138855\n      412.920044\n    \n    \n      2\n      1.0\n      1960-03-31\n      445.526978\n      418.457153\n      427.826965\n      463.226990\n      472.596832\n    \n    \n      3\n      1.0\n      1960-04-30\n      431.495422\n      403.697540\n      413.319366\n      449.671478\n      459.293304\n    \n    \n      4\n      1.0\n      1960-05-31\n      452.797211\n      424.679352\n      434.411926\n      471.182495\n      480.915070\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\ndf_plot = pd.concat([Y_train_df, Y_hat_df]).set_index('ds')\ndf_plot[['y', 'AutoARIMA']].plot(ax=ax, linewidth=2)\nax.fill_between(df_plot.index, \n                df_plot['AutoARIMA-lo-80'], \n                df_plot['AutoARIMA-hi-80'],\n                alpha=.35,\n                color='orange',\n                label='auto_arima_level_80')\nax.fill_between(df_plot.index, \n                df_plot['AutoARIMA-lo-95'], \n                df_plot['AutoARIMA-hi-95'],\n                alpha=.2,\n                color='orange',\n                label='auto_arima_level_95')\nax.set_title('AirPassengers Forecast (with AutoARIMA external regressors and intervals)', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\nfor label in (ax.get_xticklabels() + ax.get_yticklabels()):\n    label.set_fontsize(20)\n\n\n\n\n\n\nSKlearn syntax\nSince the model is already fitted, just add level=(80,90) to the StatsForecast.predict method.\n\nmodel.predict(horizon, X_df=X_test_df, level=(80, 90))\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      AutoARIMA-lo-90\n      AutoARIMA-lo-80\n      AutoARIMA-hi-80\n      AutoARIMA-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1960-01-31\n      414.551483\n      396.858002\n      400.765991\n      428.337006\n      432.244995\n    \n    \n      1.0\n      1960-02-29\n      387.550842\n      366.260345\n      370.962830\n      404.138855\n      408.841339\n    \n    \n      1.0\n      1960-03-31\n      445.526978\n      422.809265\n      427.826965\n      463.226990\n      468.244720\n    \n    \n      1.0\n      1960-04-30\n      431.495422\n      408.166718\n      413.319366\n      449.671478\n      454.824127\n    \n    \n      1.0\n      1960-05-31\n      452.797211\n      429.199951\n      434.411926\n      471.182495\n      476.394470\n    \n    \n      1.0\n      1960-06-30\n      502.991394\n      479.274841\n      484.513153\n      521.469604\n      526.707947\n    \n    \n      1.0\n      1960-07-31\n      577.782837\n      554.013000\n      559.263123\n      596.302551\n      601.552612\n    \n    \n      1.0\n      1960-08-31\n      587.973938\n      564.180298\n      569.435669\n      606.512207\n      611.767517\n    \n    \n      1.0\n      1960-09-30\n      491.432617\n      467.628357\n      472.886047\n      509.979187\n      515.236877\n    \n    \n      1.0\n      1960-10-31\n      435.070312\n      411.261261\n      416.520020\n      453.620575\n      458.879333\n    \n    \n      1.0\n      1960-11-30\n      389.827820\n      366.016632\n      371.275848\n      408.379761\n      413.638977\n    \n    \n      1.0\n      1960-12-31\n      432.665527\n      408.853394\n      414.112823\n      451.218231\n      456.477661"
  },
  {
    "objectID": "arima.html",
    "href": "arima.html",
    "title": "ARIMA",
    "section": "",
    "text": "source\n\npredict_arima\n\n predict_arima (model, n_ahead, newxreg=None, se_fit=True)\n\n\nmyarima(ap, order=(2, 1, 1), seasonal={'order': (0, 1, 0), 'period': 12}, \n        constant=False, ic='aicc', method='CSS-ML')['aic']\n\n\nsource\n\n\narima_string\n\n arima_string (model, padding=False)\n\n\nsource\n\n\nforecast_arima\n\n forecast_arima (model, h=None, level=None, fan=False, xreg=None,\n                 blambda=None, bootstrap=False, npaths=5000, biasadj=None)\n\n\nsource\n\n\nfitted_arima\n\n fitted_arima (model, h=1)\n\nReturns h-step forecasts for the data used in fitting the model.\n\nsource\n\n\nauto_arima_f\n\n auto_arima_f (x, d=None, D=None, max_p=5, max_q=5, max_P=2, max_Q=2,\n               max_order=5, max_d=2, max_D=1, start_p=2, start_q=2,\n               start_P=1, start_Q=1, stationary=False, seasonal=True,\n               ic='aicc', stepwise=True, nmodels=94, trace=False,\n               approximation=None, method=None, truncate=None, xreg=None,\n               test='kpss', test_kwargs=None, seasonal_test='seas',\n               seasonal_test_kwargs=None, allowdrift=True, allowmean=True,\n               blambda=None, biasadj=False, parallel=False, num_cores=2,\n               period=1)\n\n\nsource\n\n\nprint_statsforecast_ARIMA\n\n print_statsforecast_ARIMA (model, digits=3, se=True)\n\n\nsource\n\n\nARIMASummary\n\n ARIMASummary (model)\n\nARIMA Summary.\n\nsource\n\n\nAutoARIMA\n\n AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=None, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=True, allowmean:bool=True,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            parallel:bool=False, num_cores:int=2, period:int=1)\n\nAn AutoARIMA estimator.\nReturns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided.\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "ets.html",
    "href": "ets.html",
    "title": "ETS Model",
    "section": "",
    "text": "source\n\n\n\n ets_target_fn (par, p_y, p_nstate, p_errortype, p_trendtype,\n                p_seasontype, p_damped, p_lower, p_upper, p_opt_crit,\n                p_nmse, p_bounds, p_m, p_optAlpha, p_optBeta, p_optGamma,\n                p_optPhi, p_givenAlpha, p_givenBeta, p_givenGamma,\n                p_givenPhi, alpha, beta, gamma, phi)\n\n\nsource\n\n\n\n\n is_constant (x)\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "distributed.fugue.html",
    "href": "distributed.fugue.html",
    "title": "FugueBackend",
    "section": "",
    "text": "source\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "distributed.fugue.html#dask-distributed-predictions",
    "href": "distributed.fugue.html#dask-distributed-predictions",
    "title": "FugueBackend",
    "section": "Dask Distributed Predictions",
    "text": "Dask Distributed Predictions\nHere we provide an example for the distribution of the StatsForecast predictions using Fugue to execute the code in a Dask cluster.\nTo do it we instantiate the FugueBackend class with a DaskExecutionEngine.\n\nfrom dask.distributed import Client\nfrom fugue_dask import DaskExecutionEngine\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import generate_series\n\n# Generate Synthetic Panel Data\ndf = generate_series(10).reset_index()\ndf['unique_id'] = df['unique_id'].astype(str)\n\n# Instantiate FugueBackend with DaskExecutionEngine\ndask_client = Client()\nengine = DaskExecutionEngine(dask_client=dask_client)\nbackend = FugueBackend(engine=engine, as_local=True)\n\nWe have simply pass backend to the usual StatsForecast instantiation.\n\nfcst = StatsForecast(models=[Naive()], freq='D', backend=backend)\n\n\nDistributed Forecast\nFor extremely fast distributed predictions we use FugueBackend as backend that operates like the original StatsForecast.forecast method.\nIt receives as input a pandas.DataFrame with columns [unique_id,ds,y] and exogenous, where the ds (datestamp) column should be of a format expected by Pandas. The y column must be numeric, and represents the measurement we wish to forecast. And the unique_id uniquely identifies the series in the panel data.\n\n# Distributed predictions with FugueBackend.\nfcst.forecast(df=df, h=12)\n\n\n\nDistributed Cross-Validation\nFor extremely fast distributed temporcal cross-validation we use cross_validation method that operates like the original StatsForecast.cross_validation method.\n\n# Distributed cross-validation with FugueBackend.\nfcst.cross_validation(df=df, h=12, n_windows=2)"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Models",
    "section": "",
    "text": "StatsForecast offers a wide variety of models grouped in the following categories:\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "models.html#autoarima",
    "href": "models.html#autoarima",
    "title": "Models",
    "section": "AutoARIMA",
    "text": "AutoARIMA\n\nsource\n\nAutoARIMA\n\n AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=False, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=False, allowmean:bool=False,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            parallel:bool=False, num_cores:int=2, season_length:int=1)\n\nAutoARIMA model.\nAutomatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Default is Akaike Information Criterion (AICc).\nParameters: d: int, order of first-differencing. D: int, order of seasonal-differencing. max_p: int, max autorregresives p. max_q: int, max moving averages q. max_P: int, max seasonal autorregresives P. max_Q: int, max seasonal moving averages Q. max_order: int, max p+q+P+Q value if not stepwise selection. max_d: int, max non-seasonal differences. max_D: int, max seasonal differences. start_p: int, starting value of p in stepwise procedure. start_q: int, starting value of q in stepwise procedure. start_P: int, starting value of P in stepwise procedure. start_Q: int, starting value of Q in stepwise procedure. stationary: bool, if True, restricts search to stationary models. seasonal: bool, if False, restricts search to non-seasonal models. ic: str, information criterion to be used in model selection. stepwise: bool, if True, will do stepwise selection (faster). nmodels: int, number of models considered in stepwise search. trace: bool, if True, the searched ARIMA models is reported. approximation: bool, if True, conditional sums-of-squares estimation, final MLE. method: str, fitting method between maximum likelihood or sums-of-squares. truncate: int, observations truncated series used in model selection. test: str (default ‚Äòkpss‚Äô), unit root test to use. See ndiffs for details. test_kwargs: str optional (default None), unit root test additional arguments. seasonal_test: str (default ‚Äòseas‚Äô), selection method for seasonal differences. seasonal_test_kwargs: dict (optional), seasonal unit root test arguments. allowdrift: bool (default True), If True, drift models terms considered. allowmean: bool (default True), If True, non-zero mean models considered. blambda: float optional (default None), Box-Cox transformation parameter. biasadj: bool (default False), Use adjusted back-transformed mean Box-Cox. parallel: bool, If True and stepwise=False, then parallel search. num_cores: int, amount of parallel processes to be used if parallel=True. season_length: int, number of observations per unit of time. Ex: 24 Hourly data.\nNote: This implementation is a mirror of Hyndman‚Äôs forecast::auto.arima.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\n\nsource\n\n\nAutoARIMA.fit\n\n AutoARIMA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoARIMA model.\nFit an AutoARIMA to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: AutoARIMA fitted model.\n\nsource\n\n\nAutoARIMA.predict\n\n AutoARIMA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None)\n\nPredict with fitted AutoArima.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nAutoARIMA.predict_in_sample\n\n AutoARIMA.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoArima insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nAutoARIMA.forecast\n\n AutoARIMA.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoARIMA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# AutoARIMA's usage example\n\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.utils import AirPassengers as ap\n\n\narima = AutoARIMA(season_length=4)\narima = arima.fit(y=ap)\ny_hat_dict = arima.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([497.95290566, 486.78069066, 500.0821497 , 494.10983852]),\n 'lo-80': 0    467.167464\n 1    442.112230\n 2    450.786735\n 3    440.963293\n Name: 80%, dtype: float64,\n 'hi-80': 0    528.738348\n 1    531.449151\n 2    549.377564\n 3    547.256385\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "models.html#autoets",
    "href": "models.html#autoets",
    "title": "Models",
    "section": "AutoETS",
    "text": "AutoETS\n\nsource\n\nAutoETS\n\n AutoETS (season_length:int=1, model:str='ZZZ',\n          damped:Optional[bool]=None)\n\nAutomatic Exponential Smoothing model.\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative, \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: E in [\\(M, A, Z\\)], T in [\\(N, A, M, Z\\)], and S in [\\(N, A, M, Z\\)].\nFor example when model=‚ÄòANN‚Äô (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‚ÄòZ‚Äô, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nParameters: model: str, controlling state-space-equations. season_length: int, number of observations per unit of time. Ex: 24 Hourly data. damped: bool, a parameter that ‚Äòdampens‚Äô the trend. \nNote: This implementation is a mirror of Hyndman‚Äôs forecast::ets.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\nHyndman, Rob, et al (2008). ‚ÄúForecasting with exponential smoothing: the state space approach‚Äù.\n\nsource\n\n\nAutoETS.fit\n\n AutoETS.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: Exponential Smoothing fitted model.\n\nsource\n\n\nAutoETS.predict\n\n AutoETS.predict (h:int, X:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nAutoETS.predict_in_sample\n\n AutoETS.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nAutoETS.forecast\n\n AutoETS.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                   X_future:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# AutoETS' usage example\n\nfrom statsforecast.models import AutoETS\nfrom statsforecast.utils import AirPassengers as ap\n\n# Multiplicative trend, optimal error and seasonality\nautoets = AutoETS(model='ZMZ',  \n              season_length=4)\nautoets = autoets.fit(y=ap)\ny_hat_dict = autoets.predict(h=4)\ny_hat_dict\n\n{'mean': array([416.63294737, 419.65915384, 442.66309931, 457.33314074])}\n\n\n\nsource\n\n\nETS\n\n ETS (season_length:int=1, model:str='ZZZ', damped:Optional[bool]=None)\n\nAutomatic Exponential Smoothing model.\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative, \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: E in [\\(M, A, Z\\)], T in [\\(N, A, M, Z\\)], and S in [\\(N, A, M, Z\\)].\nFor example when model=‚ÄòANN‚Äô (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‚ÄòZ‚Äô, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nParameters: model: str, controlling state-space-equations. season_length: int, number of observations per unit of time. Ex: 24 Hourly data. damped: bool, a parameter that ‚Äòdampens‚Äô the trend. \nNote: This implementation is a mirror of Hyndman‚Äôs forecast::ets.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\nHyndman, Rob, et al (2008). ‚ÄúForecasting with exponential smoothing: the state space approach‚Äù.\n\nets = ETS(model='ZMZ', season_length=4)"
  },
  {
    "objectID": "models.html#autoces",
    "href": "models.html#autoces",
    "title": "Models",
    "section": "AutoCES",
    "text": "AutoCES\n\nsource\n\nAutoCES\n\n AutoCES (season_length:int=1, model:str='Z')\n\nComplex Exponential Smoothing model.\nAutomatically selects the best Complex Exponential Smoothing model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(S\\) simple, \\(P\\) parial, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the kind of CES model: \\(N\\) for simple CES (withous seasonality), \\(S\\) for simple seasonality (lagged CES), \\(P\\) for partial seasonality (without complex part), \\(F\\) for full seasonality (lagged CES with real and complex seasonal parts).\nIf the component is selected as ‚ÄòZ‚Äô, it operates as a placeholder to ask the AutoCES model to figure out the best parameter.\nParameters: model: str, controlling state-space-equations. season_length: int, number of observations per unit of time. Ex: 24 Hourly data.\nReferences: Svetunkov, Ivan & Kourentzes, Nikolaos. (2015). ‚ÄúComplex Exponential Smoothing‚Äù. 10.13140/RG.2.1.3757.2562..\n\nsource\n\n\nAutoCES.fit\n\n AutoCES.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Complex Exponential Smoothing model.\nFit the Comples Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: Complex Exponential Smoothing fitted model.\n\nsource\n\n\nAutoCES.predict\n\n AutoCES.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted Exponential Smoothing.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None).\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nAutoCES.predict_in_sample\n\n AutoCES.predict_in_sample ()\n\nAccess fitted Exponential Smoothing insample predictions.\nParameters: X: array-like of shape (t, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nAutoCES.forecast\n\n AutoCES.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                   X_future:Optional[numpy.ndarray]=None,\n                   fitted:bool=False)\n\nMemory Efficient Complex Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# ETS' usage example\n\nfrom statsforecast.models import AutoCES\nfrom statsforecast.utils import AirPassengers as ap\n\n# Multiplicative trend, optimal error and seasonality\nces = AutoCES(model='Z',  \n              season_length=4)\nces = ces.fit(y=ap)\ny_hat_dict = ces.predict(h=4)\ny_hat_dict\n\n{'mean': array([424.30716324, 405.69589186, 442.02640533, 443.63488996])}"
  },
  {
    "objectID": "models.html#autotheta",
    "href": "models.html#autotheta",
    "title": "Models",
    "section": "AutoTheta",
    "text": "AutoTheta\n\nsource\n\nAutoTheta\n\n AutoTheta (season_length:int=1, decomposition_type:str='multiplicative',\n            model:Optional[str]=None)\n\nAutoTheta model.\nAutomatically selects the best Theta (Standard Theta Model (‚ÄòSTM‚Äô), Optimized Theta Model (‚ÄòOTM‚Äô), Dynamic Standard Theta Model (‚ÄòDSTM‚Äô), Dynamic Optimized Theta Model (‚ÄòDOTM‚Äô)) model using mse.\nParameters: season_length: int, number of observations per unit of time. Ex: 24 Hourly data. decomposition_type: str, Sesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô. model: str, controlling Theta Model. By default searchs the best model.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\nsource\n\n\nAutoTheta.fit\n\n AutoTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: AutoTheta fitted model.\n\nsource\n\n\nAutoTheta.predict\n\n AutoTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nAutoTheta.predict_in_sample\n\n AutoTheta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nAutoTheta.forecast\n\n AutoTheta.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# AutoTheta's usage example\n\nfrom statsforecast.models import AutoTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\ntheta = AutoTheta(season_length=4)\ntheta = theta.fit(y=ap)\ny_hat_dict = theta.predict(h=4)\ny_hat_dict\n\n{'mean': array([413.86262032, 410.60532872, 429.59124482, 440.16565301])}"
  },
  {
    "objectID": "models.html#simplesmooth",
    "href": "models.html#simplesmooth",
    "title": "Models",
    "section": "SimpleSmooth",
    "text": "SimpleSmooth\n\nsource\n\nSimpleExponentialSmoothing\n\n SimpleExponentialSmoothing (alpha:float)\n\nSimpleExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\(\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}\\)\nThe rate \\(0 \\leq \\alpha \\leq 1\\) at which the weights decrease is called the smoothing parameter. When \\(\\alpha = 1\\), SES is equal to the naive method.\nParameters: alpha: float, smoothing parameter.\nReferences: Charles C Holt (1957). ‚ÄúForecasting seasonals and trends by exponentially weighted moving averages‚Äù.\n\nsource\n\n\nSimpleExponentialSmoothing.forecast\n\n SimpleExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                      X:Optional[numpy.ndarray]=None, X_fu\n                                      ture:Optional[numpy.ndarray]=None,\n                                      fitted:bool=False)\n\nMemory Efficient SimpleExponentialSmoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSimpleExponentialSmoothing.fit\n\n SimpleExponentialSmoothing.fit (y:numpy.ndarray,\n                                 X:Optional[numpy.ndarray]=None)\n\nFit the SimpleExponentialSmoothing model.\nFit an SimpleExponentialSmoothing to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: SimpleExponentialSmoothing fitted model.\n\nsource\n\n\nSimpleExponentialSmoothing.predict\n\n SimpleExponentialSmoothing.predict (h:int,\n                                     X:Optional[numpy.ndarray]=None)\n\nPredict with fitted SimpleExponentialSmoothing.\nParameters: h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None).\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSimpleExponentialSmoothing.predict_in_sample\n\n SimpleExponentialSmoothing.predict_in_sample ()\n\nAccess fitted SimpleExponentialSmoothing insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# SimpleExponentialSmoothing's usage example\n\nfrom statsforecast.models import SimpleExponentialSmoothing\nfrom statsforecast.utils import AirPassengers as ap\n\n\nses = SimpleExponentialSmoothing(alpha=0.5)\nses = ses.fit(y=ap)\ny_hat_dict = ses.predict(h=4)\ny_hat_dict\n\n{'mean': array([439.256, 439.256, 439.256, 439.256], dtype=float32)}"
  },
  {
    "objectID": "models.html#simplesmoothoptimized",
    "href": "models.html#simplesmoothoptimized",
    "title": "Models",
    "section": "SimpleSmoothOptimized",
    "text": "SimpleSmoothOptimized\n\nsource\n\nSimpleExponentialSmoothingOptimized\n\n SimpleExponentialSmoothingOptimized ()\n\nSimpleExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\(\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}\\)\nThe smoothing parameter \\(\\alpha^*\\) is optimized by square error minimization.\nParameters:\nReferences: Charles C Holt (1957). ‚ÄúForecasting seasonals and trends by exponentially weighted moving averages‚Äù.\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.fit\n\n SimpleExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                          X:Optional[numpy.ndarray]=None)\n\nFit the SimpleExponentialSmoothingOptimized model.\nFit an SimpleExponentialSmoothingOptimized to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: SimpleExponentialSmoothingOptimized fitted model.\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.predict\n\n SimpleExponentialSmoothingOptimized.predict (h:int,\n                                              X:Optional[numpy.ndarray]=No\n                                              ne)\n\nPredict with fitted SimpleExponentialSmoothingOptimized.\nParameters: h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None).\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.predict_in_sample\n\n SimpleExponentialSmoothingOptimized.predict_in_sample ()\n\nAccess fitted SimpleExponentialSmoothingOptimized insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.forecast\n\n SimpleExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                               X:Optional[numpy.ndarray]=N\n                                               one, X_future:Optional[nump\n                                               y.ndarray]=None,\n                                               fitted:bool=False)\n\nMemory Efficient SimpleExponentialSmoothingOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# SimpleExponentialSmoothingOptimized's usage example\n\nfrom statsforecast.models import SimpleExponentialSmoothingOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nseso = SimpleExponentialSmoothingOptimized()\nseso = seso.fit(y=ap)\ny_hat_dict = seso.predict(h=4)\ny_hat_dict\n\n{'mean': array([431.58716, 431.58716, 431.58716, 431.58716], dtype=float32)}"
  },
  {
    "objectID": "models.html#seasonalsmooth",
    "href": "models.html#seasonalsmooth",
    "title": "Models",
    "section": "SeasonalSmooth",
    "text": "SeasonalSmooth\n\nsource\n\nSeasonalExponentialSmoothing\n\n SeasonalExponentialSmoothing (season_length:int, alpha:float)\n\nSeasonalExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations and season \\(s\\), the one-step forecast is given by: \\(\\hat{y}_{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1,s}\\)\nNote: This method is an extremely simplified of Holt-Winter‚Äôs method where the trend and level are set to zero. And a single seasonal smoothing parameter \\(\\alpha\\) is shared across seasons.\nParameters: alpha: float, smoothing parameter. season_length: int, number of observations per unit of time. Ex: 24 Hourly data.\nReferences: Charles. C. Holt (1957). ‚ÄúForecasting seasonals and trends by exponentially weighted moving averages‚Äù, ONR Research Memorandum, Carnegie Institute of Technology 52..\nPeter R. Winters (1960). ‚ÄúForecasting sales by exponentially weighted moving averages‚Äù. Management Science.\n\nsource\n\n\nSeasonalExponentialSmoothing.fit\n\n SeasonalExponentialSmoothing.fit (y:numpy.ndarray,\n                                   X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalExponentialSmoothing model.\nFit an SeasonalExponentialSmoothing to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: SeasonalExponentialSmoothing fitted model.\n\nsource\n\n\nSeasonalExponentialSmoothing.predict\n\n SeasonalExponentialSmoothing.predict (h:int,\n                                       X:Optional[numpy.ndarray]=None)\n\nPredict with fitted SeasonalExponentialSmoothing.\nParameters: h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None).\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSeasonalExponentialSmoothing.predict_in_sample\n\n SeasonalExponentialSmoothing.predict_in_sample ()\n\nAccess fitted SeasonalExponentialSmoothing insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSeasonalExponentialSmoothing.forecast\n\n SeasonalExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                        X:Optional[numpy.ndarray]=None, X_\n                                        future:Optional[numpy.ndarray]=Non\n                                        e, fitted:bool=False)\n\nMemory Efficient SeasonalExponentialSmoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# SeasonalExponentialSmoothing's usage example\n\nfrom statsforecast.models import SeasonalExponentialSmoothing\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalExponentialSmoothing(alpha=0.5, season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([376.28955, 354.71094, 396.02002, 412.06738], dtype=float32)}"
  },
  {
    "objectID": "models.html#seasonalsmoothoptimized",
    "href": "models.html#seasonalsmoothoptimized",
    "title": "Models",
    "section": "SeasonalSmoothOptimized",
    "text": "SeasonalSmoothOptimized\n\nsource\n\nSeasonalExponentialSmoothingOptimized\n\n SeasonalExponentialSmoothingOptimized (season_length:int)\n\nSeasonalExponentialSmoothingOptimized model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations and season \\(s\\), the one-step forecast is given by: \\(\\hat{y}_{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1,s}\\)\nThe smoothing parameter \\(\\alpha^*\\) is optimized by square error minimization.\nNote: This method is an extremely simplified of Holt-Winter‚Äôs method where the trend and level are set to zero. And a single seasonal smoothing parameter \\(\\alpha\\) is shared across seasons.\nParameters: season_length: int, number of observations per unit of time. Ex: 24 Hourly data.\nReferences: Charles. C. Holt (1957). ‚ÄúForecasting seasonals and trends by exponentially weighted moving averages‚Äù, ONR Research Memorandum, Carnegie Institute of Technology 52..\nPeter R. Winters (1960). ‚ÄúForecasting sales by exponentially weighted moving averages‚Äù. Management Science.\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.forecast\n\n SeasonalExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                                 X:Optional[numpy.ndarray]\n                                                 =None, X_future:Optional[\n                                                 numpy.ndarray]=None,\n                                                 fitted:bool=False)\n\nMemory Efficient SeasonalExponentialSmoothingOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.fit\n\n SeasonalExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                            X:Optional[numpy.ndarray]=None\n                                            )\n\nFit the SeasonalExponentialSmoothingOptimized model.\nFit an SeasonalExponentialSmoothingOptimized to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: SeasonalExponentialSmoothingOptimized fitted model.\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.predict\n\n SeasonalExponentialSmoothingOptimized.predict (h:int,\n                                                X:Optional[numpy.ndarray]=\n                                                None)\n\nPredict with fitted SeasonalExponentialSmoothingOptimized.\nParameters: h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None).\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.predict_in_sample\n\n SeasonalExponentialSmoothingOptimized.predict_in_sample ()\n\nAccess fitted SeasonalExponentialSmoothingOptimized insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# SeasonalExponentialSmoothingOptimized's usage example\n\nfrom statsforecast.models import SeasonalExponentialSmoothingOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalExponentialSmoothingOptimized(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([416.42798, 390.50757, 418.8656 , 460.3452 ], dtype=float32)}"
  },
  {
    "objectID": "models.html#holts-method",
    "href": "models.html#holts-method",
    "title": "Models",
    "section": "Holt‚Äôs method",
    "text": "Holt‚Äôs method\n\nsource\n\nHolt\n\n Holt (season_length:int=1, error_type:str='A')\n\nHolt‚Äôs method.\nAlso known as double exponential smoothing, Holt‚Äôs method is an extension of exponential smoothing for series with a trend. This implementation returns the corresponding ETS model with additive (A) or multiplicative (M) errors (so either ‚ÄòAAN‚Äô or ‚ÄòMAN‚Äô).\nParameters: season_length: int, number of observations per unit of time. Ex: 12 Monthly data. \nerror_type: The type of error of the ETS model. Can be additive (A) or multiplicative (M). \nReferences: - Rob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Methods with trend‚Äù.\n\nsource\n\n\nHolt.forecast\n\n Holt.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nHolt.fit\n\n Holt.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: Exponential Smoothing fitted model.\n\nsource\n\n\nHolt.predict\n\n Holt.predict (h:int, X:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nHolt.predict_in_sample\n\n Holt.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# Holt's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Holt(season_length=12, error_type='A')\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([434.27026573, 436.5445037 , 438.81874167, 441.09297963])}"
  },
  {
    "objectID": "models.html#holt-winters-method",
    "href": "models.html#holt-winters-method",
    "title": "Models",
    "section": "Holt-Winters‚Äô method",
    "text": "Holt-Winters‚Äô method\n\nsource\n\nHoltWinters\n\n HoltWinters (season_length:int=1, error_type:str='A')\n\nHolt-Winters‚Äô method.\nAlso known as triple exponential smoothing, Holt-Winters‚Äô method is an extension of exponential smoothing for series that contain both trend and seasonality. This implementation returns the corresponding ETS model with additive (A) or multiplicative (M) errors (so either ‚ÄòAAA‚Äô or ‚ÄòMAM‚Äô).\nParameters: season_length: int, number of observations per unit of time. Ex: 12 Monthly data. \nerror_type: The type of error of the ETS model. Can be additive (A) or multiplicative (M). \nReferences: - Rob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Methods with seasonality‚Äù.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nseason length\n\n\nerror_type\nstr\nA\nerror type\n\n\n\n\nsource\n\n\nHoltWinters.forecast\n\n HoltWinters.forecast (y:numpy.ndarray, h:int,\n                       X:Optional[numpy.ndarray]=None,\n                       X_future:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nHoltWinters.fit\n\n HoltWinters.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: Exponential Smoothing fitted model.\n\nsource\n\n\nHoltWinters.predict\n\n HoltWinters.predict (h:int, X:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nHoltWinters.predict_in_sample\n\n HoltWinters.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# Holt-Winters' usage example\n\n#from statsforecast.models import HoltWinters\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = HoltWinters(season_length=12, error_type='A')\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.4192839 , 414.82970205, 449.80384898, 493.35229084])}"
  },
  {
    "objectID": "models.html#historicaverage",
    "href": "models.html#historicaverage",
    "title": "Models",
    "section": "HistoricAverage",
    "text": "HistoricAverage\n\nsource\n\nHistoricAverage\n\n HistoricAverage ()\n\nHistoricAverage model.\nAlso known as mean method. Uses a simple average of all past observations. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\[ \\hat{y}_{t+1} = \\frac{1}{t} \\sum_{j=1}^t y_j \\]\nParameters:\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). ‚ÄúForecasting principles and practice, Simple Methods‚Äù.\n\nsource\n\n\nHistoricAverage.forecast\n\n HistoricAverage.forecast (y:numpy.ndarray, h:int,\n                           X:Optional[numpy.ndarray]=None,\n                           X_future:Optional[numpy.ndarray]=None,\n                           level:Optional[Tuple[int]]=None,\n                           fitted:bool=False)\n\nMemory Efficient HistoricAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nHistoricAverage.fit\n\n HistoricAverage.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the HistoricAverage model.\nFit an HistoricAverage to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: HistoricAverage fitted model.\n\nsource\n\n\nHistoricAverage.predict\n\n HistoricAverage.predict (h:int, X:Optional[numpy.ndarray]=None,\n                          level:Optional[Tuple[int]]=None)\n\nPredict with fitted HistoricAverage.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nHistoricAverage.predict_in_sample\n\n HistoricAverage.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted HistoricAverage insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# HistoricAverage's usage example\n\nfrom statsforecast.models import HistoricAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = HistoricAverage()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([280.2986, 280.2986, 280.2986, 280.2986], dtype=float32)}"
  },
  {
    "objectID": "models.html#naive",
    "href": "models.html#naive",
    "title": "Models",
    "section": "Naive",
    "text": "Naive\n\nsource\n\nNaive\n\n Naive ()\n\nNaive model.\nA random walk model, defined as \\(\\hat{y}_{t+1} = y_t\\) $\forall t$\nParameters:\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\nsource\n\n\nNaive.forecast\n\n Naive.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[Tuple[int]]=None, fitted:bool=False)\n\nMemory Efficient Naive predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nNaive.fit\n\n Naive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Naive model.\nFit an Naive to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: Naive fitted model.\n\nsource\n\n\nNaive.predict\n\n Naive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[Tuple[int]]=None)\n\nPredict with fitted Naive.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.Tuple[int]]\nNone\nconfidence level\n\n\n\n\nsource\n\n\nNaive.predict_in_sample\n\n Naive.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted Naive insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# Naive's usage example\n\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Naive()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([432., 432., 432., 432.], dtype=float32)}"
  },
  {
    "objectID": "models.html#randomwalkwithdrift",
    "href": "models.html#randomwalkwithdrift",
    "title": "Models",
    "section": "RandomWalkWithDrift",
    "text": "RandomWalkWithDrift\n\nsource\n\nRandomWalkWithDrift\n\n RandomWalkWithDrift ()\n\nRandomWalkWithDrift model.\nA variation of the naive method allows the forecasts to change over time. The amout of change, called drift, is the average change seen in the historical data.\n\\[ \\hat{y}_{t+1} = y_t+\\frac{1}{t-1}\\sum_{j=1}^t (y_j-y_{j-1}) = y_t+ \\frac{y_t-y_1}{t-1} \\]\nFrom the previous equation, we can see that this is equivalent to extrapolating a line between the first and the last observation.\nParameters:\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\nsource\n\n\nRandomWalkWithDrift.forecast\n\n RandomWalkWithDrift.forecast (y:numpy.ndarray, h:int,\n                               X:Optional[numpy.ndarray]=None,\n                               X_future:Optional[numpy.ndarray]=None,\n                               level:Optional[Tuple[int]]=None,\n                               fitted:bool=False)\n\nMemory Efficient RandomWalkWithDrift predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nRandomWalkWithDrift.fit\n\n RandomWalkWithDrift.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the RandomWalkWithDrift model.\nFit an RandomWalkWithDrift to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series.\nReturns: self: RandomWalkWithDrift fitted model.\n\nsource\n\n\nRandomWalkWithDrift.predict\n\n RandomWalkWithDrift.predict (h:int, X:Optional[numpy.ndarray]=None,\n                              level:Optional[Tuple[int]]=None)\n\nPredict with fitted RandomWalkWithDrift.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nRandomWalkWithDrift.predict_in_sample\n\n RandomWalkWithDrift.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted RandomWalkWithDrift insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# RandomWalkWithDrift's usage example\n\nfrom statsforecast.models import RandomWalkWithDrift\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = RandomWalkWithDrift()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([434.23776, 436.47552, 438.7133 , 440.95105], dtype=float32)}"
  },
  {
    "objectID": "models.html#seasonalnaive",
    "href": "models.html#seasonalnaive",
    "title": "Models",
    "section": "SeasonalNaive",
    "text": "SeasonalNaive\n\nsource\n\nSeasonalNaive\n\n SeasonalNaive (season_length:int)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSeasonalNaive.forecast\n\n SeasonalNaive.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[Tuple[int]]=None,\n                         fitted:bool=False)\n\nMemory Efficient SeasonalNaive predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSeasonalNaive.fit\n\n SeasonalNaive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalNaive model.\nFit an SeasonalNaive to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: SeasonalNaive fitted model.\n\nsource\n\n\nSeasonalNaive.predict\n\n SeasonalNaive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                        level:Optional[Tuple[int]]=None)\n\nPredict with fitted Naive.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSeasonalNaive.predict_in_sample\n\n SeasonalNaive.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted SeasonalNaive insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# SeasonalNaive's usage example\n\nfrom statsforecast.models import SeasonalNaive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalNaive(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([417., 391., 419., 461.], dtype=float32)}"
  },
  {
    "objectID": "models.html#windowaverage",
    "href": "models.html#windowaverage",
    "title": "Models",
    "section": "WindowAverage",
    "text": "WindowAverage\n\nsource\n\nWindowAverage\n\n WindowAverage (window_size:int)\n\nWindowAverage model.\nUses the average of the last \\(k\\) observations, with \\(k\\) the length of the window. Wider windows will capture global trends, while narrow windows will reveal local trends. The length of the window selected should take into account the importance of past observations and how fast the series changes.\nParameters: window_size: int, size of truncated series on which average is estimated.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\nsource\n\n\nWindowAverage.forecast\n\n WindowAverage.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         fitted:bool=False)\n\nMemory Efficient WindowAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nWindowAverage.fit\n\n WindowAverage.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the WindowAverage model.\nFit an WindowAverage to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: WindowAverage fitted model.\n\nsource\n\n\nWindowAverage.predict\n\n WindowAverage.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted WindowAverage.\nParameters: h: int, forecast horizon.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nWindowAverage.predict_in_sample\n\n WindowAverage.predict_in_sample ()\n\nAccess fitted WindowAverage insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# WindowAverage's usage example\n\nfrom statsforecast.models import WindowAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = WindowAverage(window_size=12*4)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([413.47916, 413.47916, 413.47916, 413.47916], dtype=float32)}"
  },
  {
    "objectID": "models.html#seasonalwindowaverage",
    "href": "models.html#seasonalwindowaverage",
    "title": "Models",
    "section": "SeasonalWindowAverage",
    "text": "SeasonalWindowAverage\n\nsource\n\nSeasonalWindowAverage\n\n SeasonalWindowAverage (season_length:int, window_size:int)\n\nSeasonalWindowAverage model.\nAn average of the last \\(k\\) observations of the same period, with \\(k\\) the length of the window.\nParameters: window_size: int, size of truncated series on which average is estimated. seasonal_length: int, number of observations per cycle.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). ‚Äúforecasting principles and practice, Simple Methods‚Äù.\n\nsource\n\n\nSeasonalWindowAverage.forecast\n\n SeasonalWindowAverage.forecast (y:numpy.ndarray, h:int,\n                                 X:Optional[numpy.ndarray]=None,\n                                 X_future:Optional[numpy.ndarray]=None,\n                                 fitted:bool=False)\n\nMemory Efficient SeasonalWindowAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSeasonalWindowAverage.fit\n\n SeasonalWindowAverage.fit (y:numpy.ndarray,\n                            X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalWindowAverage model.\nFit an SeasonalWindowAverage to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: SeasonalWindowAverage fitted model.\n\nsource\n\n\nSeasonalWindowAverage.predict\n\n SeasonalWindowAverage.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted SeasonalWindowAverage.\nParameters: h: int, forecast horizon.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nSeasonalWindowAverage.predict_in_sample\n\n SeasonalWindowAverage.predict_in_sample ()\n\nAccess fitted SeasonalWindowAverage insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# SeasonalWindowAverage's usage example\n\nfrom statsforecast.models import SeasonalWindowAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalWindowAverage(season_length=12, window_size=4)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([358.  , 338.  , 385.75, 388.25], dtype=float32)}"
  },
  {
    "objectID": "models.html#adida",
    "href": "models.html#adida",
    "title": "Models",
    "section": "ADIDA",
    "text": "ADIDA\n\nsource\n\nADIDA\n\n ADIDA ()\n\nADIDA model.\nAggregate-Dissagregate Intermittent Demand Approach: Uses temporal aggregation to reduce the number of zero observations. Once the data has been agregated, it uses the optimized SES to generate the forecasts at the new level. It then breaks down the forecast to the original level using equal weights.\nADIDA specializes on sparse or intermittent series are series with very few non-zero observations. They are notoriously hard to forecast, and so, different methods have been developed especifically for them.\nParameters:\nReferences: Nikolopoulos, K., Syntetos, A. A., Boylan, J. E., Petropoulos, F., & Assimakopoulos, V. (2011). An aggregate‚Äìdisaggregate intermittent demand approach (ADIDA) to forecasting: an empirical proposition and analysis. Journal of the Operational Research Society, 62(3), 544-554..\n\nsource\n\n\nADIDA.forecast\n\n ADIDA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None, fitted:bool=False)\n\nMemory Efficient ADIDA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nADIDA.fit\n\n ADIDA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the ADIDA model.\nFit an ADIDA to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series.\nReturns: self: ADIDA fitted model.\n\nsource\n\n\nADIDA.predict\n\n ADIDA.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted ADIDA.\nParameters: h: int, forecast horizon.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nADIDA.predict_in_sample\n\n ADIDA.predict_in_sample ()\n\nAccess fitted ADIDA insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# ADIDA's usage example\n\nfrom statsforecast.models import ADIDA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = ADIDA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "models.html#crostonclassic",
    "href": "models.html#crostonclassic",
    "title": "Models",
    "section": "CrostonClassic",
    "text": "CrostonClassic\n\nsource\n\nCrostonClassic\n\n CrostonClassic ()\n\nCrostonClassic model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nwhere \\(\\hat{z}_t\\) and \\(\\hat{p}_t\\) are forecasted using SES. The smoothing parameter of both components is set equal to 0.1\nParameters:\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303.\n\nsource\n\n\nCrostonClassic.forecast\n\n CrostonClassic.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          fitted:bool=False)\n\nMemory Efficient CrostonClassic predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nCrostonClassic.fit\n\n CrostonClassic.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonClassic model.\nFit an CrostonClassic to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series.\nReturns: self: CrostonClassic fitted model.\n\nsource\n\n\nCrostonClassic.predict\n\n CrostonClassic.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted CrostonClassic.\nParameters: h: int, forecast horizon.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nCrostonClassic.predict_in_sample\n\n CrostonClassic.predict_in_sample (level)\n\nAccess fitted CrostonClassic insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# CrostonClassic's usage example\n\nfrom statsforecast.models import CrostonClassic\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonClassic()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([460.30276, 460.30276, 460.30276, 460.30276], dtype=float32)}"
  },
  {
    "objectID": "models.html#crostonoptimized",
    "href": "models.html#crostonoptimized",
    "title": "Models",
    "section": "CrostonOptimized",
    "text": "CrostonOptimized\n\nsource\n\nCrostonOptimized\n\n CrostonOptimized ()\n\nCrostonOptimized model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nA variation of the classic Croston‚Äôs method where the smooting paramater is optimally selected from the range \\([0.1,0.3]\\). Both the non-zero demand \\(z_t\\) and the inter-demand intervals \\(p_t\\) are smoothed separately, so their smoothing parameters can be different.\nParameters:\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303..\n\nsource\n\n\nCrostonOptimized.forecast\n\n CrostonOptimized.forecast (y:numpy.ndarray, h:int,\n                            X:Optional[numpy.ndarray]=None,\n                            X_future:Optional[numpy.ndarray]=None,\n                            fitted:bool=False)\n\nMemory Efficient CrostonOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nCrostonOptimized.fit\n\n CrostonOptimized.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonOptimized model.\nFit an CrostonOptimized to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series.\nReturns: self: CrostonOptimized fitted model.\n\nsource\n\n\nCrostonOptimized.predict\n\n CrostonOptimized.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted CrostonOptimized.\nParameters: h: int, forecast horizon.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nCrostonOptimized.predict_in_sample\n\n CrostonOptimized.predict_in_sample ()\n\nAccess fitted CrostonOptimized insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# CrostonOptimized's usage example\n\nfrom statsforecast.models import CrostonOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonOptimized()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "models.html#crostonsba",
    "href": "models.html#crostonsba",
    "title": "Models",
    "section": "CrostonSBA",
    "text": "CrostonSBA\n\nsource\n\nCrostonSBA\n\n CrostonSBA ()\n\nCrostonSBA model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nA variation of the classic Croston‚Äôs method that uses a debiasing factor, so that the forecast is given by: \\[ \\hat{y}_t = 0.95  \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nParameters:\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303..\n\nsource\n\n\nCrostonSBA.forecast\n\n CrostonSBA.forecast (y:numpy.ndarray, h:int,\n                      X:Optional[numpy.ndarray]=None,\n                      X_future:Optional[numpy.ndarray]=None,\n                      fitted:bool=False)\n\nMemory Efficient CrostonSBA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nCrostonSBA.fit\n\n CrostonSBA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonSBA model.\nFit an CrostonSBA to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series.\nReturns: self: CrostonSBA fitted model.\n\nsource\n\n\nCrostonSBA.predict\n\n CrostonSBA.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted CrostonSBA.\nParameters: h: int, forecast horizon.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nCrostonSBA.predict_in_sample\n\n CrostonSBA.predict_in_sample ()\n\nAccess fitted CrostonSBA insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# CrostonSBA's usage example\n\nfrom statsforecast.models import CrostonSBA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonSBA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([437.28763, 437.28763, 437.28763, 437.28763], dtype=float32)}"
  },
  {
    "objectID": "models.html#imapa",
    "href": "models.html#imapa",
    "title": "Models",
    "section": "IMAPA",
    "text": "IMAPA\n\nsource\n\nIMAPA\n\n IMAPA ()\n\nIMAPA model.\nIntermittent Multiple Aggregation Prediction Algorithm: Similar to ADIDA, but instead of using a single aggregation level, it considers multiple in order to capture different dynamics of the data. Uses the optimized SES to generate the forecasts at the new levels and then combines them using a simple average.\nParameters:\nReferences: - Syntetos, A. A., & Boylan, J. E. (2021). Intermittent demand forecasting: Context, methods and applications. John Wiley & Sons..\n\nsource\n\n\nIMAPA.forecast\n\n IMAPA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None, fitted:bool=False)\n\nMemory Efficient IMAPA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nIMAPA.fit\n\n IMAPA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the IMAPA model.\nFit an IMAPA to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series.\nReturns: self: IMAPA fitted model.\n\nsource\n\n\nIMAPA.predict\n\n IMAPA.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted IMAPA.\nParameters: h: int, forecast horizon.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nIMAPA.predict_in_sample\n\n IMAPA.predict_in_sample ()\n\nAccess fitted IMAPA insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# IMAPA's usage example\n\nfrom statsforecast.models import IMAPA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = IMAPA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "models.html#tsb",
    "href": "models.html#tsb",
    "title": "Models",
    "section": "TSB",
    "text": "TSB\n\nsource\n\nTSB\n\n TSB (alpha_d:float, alpha_p:float)\n\nTSB model.\nTeunter-Syntetos-Babai: A modification of Croston‚Äôs method that replaces the inter-demand intervals with the demand probability \\(d_t\\), which is defined as follows.\n\\[\nd_t = \\begin{cases}\n    1  & \\text{if demand occurs at time t} \\\\\n    0  & \\text{otherwise.}\n\\end{cases}\n\\]\nHence, the forecast is given by\n\\[\\hat{y}_t= \\hat{d}_t\\hat{z_t}\\]\nBoth \\(d_t\\) and \\(z_t\\) are forecasted using SES. The smooting paramaters of each may differ, like in the optimized Croston‚Äôs method.\nParameters: alpha_d: float, smoothing parameter for demand alpha_p: float, smoothing parameter for probability\nReferences: - Teunter, R. H., Syntetos, A. A., & Babai, M. Z. (2011). Intermittent demand: Linking forecasting to inventory obsolescence. European Journal of Operational Research, 214(3), 606-615.\n\nsource\n\n\nTSB.forecast\n\n TSB.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None, fitted:bool=False)\n\nMemory Efficient TSB predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nTSB.fit\n\n TSB.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the TSB model.\nFit an TSB to a time series (numpy array) y.\nParameters: y: numpy array of shape (t, ), clean time series.\nReturns: self: TSB fitted model.\n\nsource\n\n\nTSB.predict\n\n TSB.predict (h:int, X:Optional[numpy.ndarray]=None)\n\nPredict with fitted TSB.\nParameters: h: int, forecast horizon.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nTSB.predict_in_sample\n\n TSB.predict_in_sample ()\n\nAccess fitted TSB insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# TSB's usage example\n\nfrom statsforecast.models import TSB\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = TSB(alpha_d=0.5, alpha_p=0.5)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([439.256, 439.256, 439.256, 439.256], dtype=float32)}"
  },
  {
    "objectID": "models.html#standard-theta-method",
    "href": "models.html#standard-theta-method",
    "title": "Models",
    "section": "Standard Theta Method",
    "text": "Standard Theta Method\n\nsource\n\nTheta\n\n Theta (season_length:int=1, decomposition_type:str='multiplicative')\n\nStandard Theta Method.\nParameters: season_length: int, number of observations per unit of time. Ex: 24 Hourly data. decomposition_type: str, Sesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\nsource\n\n\nTheta.forecast\n\n Theta.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nTheta.fit\n\n Theta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: AutoTheta fitted model.\n\nsource\n\n\nTheta.predict\n\n Theta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nTheta.predict_in_sample\n\n Theta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# Theta's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Theta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.96918204, 429.24926382, 490.69323393, 476.65998055])}"
  },
  {
    "objectID": "models.html#optimized-theta-method",
    "href": "models.html#optimized-theta-method",
    "title": "Models",
    "section": "Optimized Theta Method",
    "text": "Optimized Theta Method\n\nsource\n\nOptimizedTheta\n\n OptimizedTheta (season_length:int=1,\n                 decomposition_type:str='multiplicative')\n\nOptimized Theta Method.\nParameters: season_length: int, number of observations per unit of time. Ex: 24 Hourly data. decomposition_type: str, Sesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\nsource\n\n\nOptimizedTheta.forecast\n\n OptimizedTheta.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nOptimizedTheta.fit\n\n OptimizedTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: AutoTheta fitted model.\n\nsource\n\n\nOptimizedTheta.predict\n\n OptimizedTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nOptimizedTheta.predict_in_sample\n\n OptimizedTheta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# OptimzedThetA's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = OptimizedTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([442.94078295, 432.22936898, 495.30609727, 482.30625563])}"
  },
  {
    "objectID": "models.html#dynamic-standard-theta-method",
    "href": "models.html#dynamic-standard-theta-method",
    "title": "Models",
    "section": "Dynamic Standard Theta Method",
    "text": "Dynamic Standard Theta Method\n\nsource\n\nDynamicTheta\n\n DynamicTheta (season_length:int=1,\n               decomposition_type:str='multiplicative')\n\nDynamic Standard Theta Method.\nParameters: season_length: int, number of observations per unit of time. Ex: 24 Hourly data. decomposition_type: str, Sesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\nsource\n\n\nDynamicTheta.forecast\n\n DynamicTheta.forecast (y:numpy.ndarray, h:int,\n                        X:Optional[numpy.ndarray]=None,\n                        X_future:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nDynamicTheta.fit\n\n DynamicTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: AutoTheta fitted model.\n\nsource\n\n\nDynamicTheta.predict\n\n DynamicTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                       level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nDynamicTheta.predict_in_sample\n\n DynamicTheta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# DynStandardThetaMethod's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = DynamicTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.77412474, 429.06190332, 490.48332496, 476.46133269])}"
  },
  {
    "objectID": "models.html#dynamic-optimized-theta-method",
    "href": "models.html#dynamic-optimized-theta-method",
    "title": "Models",
    "section": "Dynamic Optimized Theta Method",
    "text": "Dynamic Optimized Theta Method\n\nsource\n\nDynamicOptimizedTheta\n\n DynamicOptimizedTheta (season_length:int=1,\n                        decomposition_type:str='multiplicative')\n\nDynamic Optimized Theta Method.\nParameters: season_length: int, number of observations per unit of time. Ex: 24 Hourly data. decomposition_type: str, Sesonal decomposition type, ‚Äòmultiplicative‚Äô (default) or ‚Äòadditive‚Äô.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). ‚ÄúModels for optimising the theta method and their relationship to state space models‚Äù. International Journal of Forecasting\n\nsource\n\n\nDynamicOptimizedTheta.forecast\n\n DynamicOptimizedTheta.forecast (y:numpy.ndarray, h:int,\n                                 X:Optional[numpy.ndarray]=None,\n                                 X_future:Optional[numpy.ndarray]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\nParameters: y: numpy array of shape (n,), clean time series. h: int, forecast horizon. X: array-like of shape (t, n_x) optional insample exogenous (default=None). X_future: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nDynamicOptimizedTheta.fit\n\n DynamicOptimizedTheta.fit (y:numpy.ndarray,\n                            X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\nParameters: y: numpy array of shape (t, ), clean time series. X: array-like of shape (t, n_x) optional exogenous (default=None).\nReturns: self: AutoTheta fitted model.\n\nsource\n\n\nDynamicOptimizedTheta.predict\n\n DynamicOptimizedTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                                level:Optional[Tuple[int]]=None)\n\nPredict with fitted AutoTheta.\nParameters: h: int, forecast horizon. X: array-like of shape (h, n_x) optional exogenous (default=None). level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\nsource\n\n\nDynamicOptimizedTheta.predict_in_sample\n\n DynamicOptimizedTheta.predict_in_sample (level:Optional[Tuple[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\nParameters: level: float list 0-100, confidence levels for prediction intervals.\nReturns: forecasts: dictionary, with entries ‚Äòmean‚Äô for point predictions and ‚Äôlevel_*‚Äô for probabilistic predictions.\n\n# OptimzedThetaMethod's usage example\n\n#from statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = DynamicOptimizedTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([442.94256075, 432.31255941, 495.49774527, 482.58930649])}"
  },
  {
    "objectID": "distributed.multiprocess.html",
    "href": "distributed.multiprocess.html",
    "title": "MultiprocessBackend",
    "section": "",
    "text": "source\n\nMultiprocessBackend\n\n MultiprocessBackend (n_jobs:int)\n\nMultiprocessBackend Parent Class for Distributed Computation.\nParameters: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nNotes:\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n generate_series (n_series:int, freq:str='D', min_length:int=50,\n                  max_length:int=500, n_static_features:int=0,\n                  equal_ends:bool=False, seed:int=0)\n\nGenerate Synthetic Panel Series.\nGenerates n_series of frequency freq of different lengths in the interval [min_length, max_length]. If n_static_features > 0, then each serie gets static features with random values. If equal_ends == True then all series end at the same date.\nParameters: n_series: int, number of series for synthetic panel. min_length: int, minimal length of synthetic panel‚Äôs series. max_length: int, minimal length of synthetic panel‚Äôs series. n_static_features: int, default=0, number of static exogenous variables for synthetic panel‚Äôs series. equal_ends: bool, if True, series finish in the same date stamp ds. freq: str, frequency of the data, panda‚Äôs available frequencies.\nReturns: freq: pandas.DataFrame, synthetic panel with columns [unique_id, ds, y] and exogenous.\n\nfrom statsforecast.utils import generate_series\n\nsynthetic_panel = generate_series(n_series=2)\nsynthetic_panel.groupby('unique_id').head(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      y\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      0\n      2000-01-01\n      0.357595\n    \n    \n      0\n      2000-01-02\n      1.301382\n    \n    \n      0\n      2000-01-03\n      2.272442\n    \n    \n      0\n      2000-01-04\n      3.211827\n    \n    \n      1\n      2000-01-01\n      5.399023\n    \n    \n      1\n      2000-01-02\n      6.092818\n    \n    \n      1\n      2000-01-03\n      0.476396\n    \n    \n      1\n      2000-01-04\n      1.343744\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "utils.html#model-utils",
    "href": "utils.html#model-utils",
    "title": "Utils",
    "section": "Model utils",
    "text": "Model utils"
  },
  {
    "objectID": "ces.html",
    "href": "ces.html",
    "title": "CES Model",
    "section": "",
    "text": "source\n\n\n\n ces_target_fn (optimal_param, init_alpha_0, init_alpha_1, init_beta_0,\n                init_beta_1, opt_alpha_0, opt_alpha_1, opt_beta_0,\n                opt_beta_1, y, m, init_states, n_components, seasontype,\n                nmse)\n\n\nforecast_ces(res, 12)\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "distributed.core.html",
    "href": "distributed.core.html",
    "title": "Core",
    "section": "",
    "text": "source\n\nParallelBackend\n\n ParallelBackend ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "adapters.prophet.html",
    "href": "adapters.prophet.html",
    "title": "Replace FB-Prophet",
    "section": "",
    "text": "source\n\n\n\n AutoARIMAProphet (growth='linear', changepoints=None, n_changepoints=25,\n                   changepoint_range=0.8, yearly_seasonality='auto',\n                   weekly_seasonality='auto', daily_seasonality='auto',\n                   holidays=None, seasonality_mode='additive',\n                   seasonality_prior_scale=10.0,\n                   holidays_prior_scale=10.0,\n                   changepoint_prior_scale=0.05, mcmc_samples=0,\n                   interval_width=0.8, uncertainty_samples=1000,\n                   stan_backend=None, d=None, D=None, max_p=5, max_q=5,\n                   max_P=2, max_Q=2, max_order=5, max_d=2, max_D=1,\n                   start_p=2, start_q=2, start_P=1, start_Q=1,\n                   stationary=False, seasonal=True, ic='aicc',\n                   stepwise=True, nmodels=94, trace=False,\n                   approximation=False, method=None, truncate=None,\n                   test='kpss', test_kwargs=None, seasonal_test='seas',\n                   seasonal_test_kwargs=None, allowdrift=False,\n                   allowmean=False, blambda=None, biasadj=False,\n                   parallel=False, num_cores=2, period=1)\n\nAutoARIMAProphet adapter.\nReturns best ARIMA model using external variables created by the Prophet interface. This class receives as parameters the same as prophet.Prophet and uses a models.AutoARIMA backend.\nIf your forecasting pipeline uses Prophet the AutoARIMAProphet adapter helps to easily substitute Prophet with an AutoARIMA.\nParameters: growth: String ‚Äòlinear‚Äô, ‚Äòlogistic‚Äô or ‚Äòflat‚Äô to specify a linear, logistic or flat trend. changepoints: List of dates of potential changepoints. Otherwise selected automatically. n_changepoints: Number of potential changepoints to include. changepoint_range: Proportion of history in which trend changepoints will be estimated. yearly_seasonality: Fit yearly seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. weekly_seasonality: Fit weekly seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. daily_seasonality: Fit daily seasonality. Can be ‚Äòauto‚Äô, True, False, or a number of Fourier terms to generate. holidays: pandas.DataFrame with columns holiday (string) and ds (date type). interval_width: float, uncertainty forecast intervals width. StatsForecast‚Äôs level \nNotes: You can create automated exogenous variables from the Prophet data processing pipeline these exogenous will be included into AutoARIMA‚Äôs exogenous features. Parameters like seasonality_mode, seasonality_prior_scale, holidays_prior_scale, changepoint_prior_scale, mcmc_samples, uncertainty_samples, stan_backend are Prophet exclusive.\nReferences: Sean J. Taylor, Benjamin Letham (2017). ‚ÄúProphet Forecasting at Scale‚Äù\nOskar Triebe, Hansika Hewamalage, Polina Pilyugina, Nikolay Laptev, Christoph Bergmeir, Ram Rajagopal (2021). ‚ÄúNeuralProphet: Explainable Forecasting at Scale‚Äù.\nRob J. Hyndman, Yeasmin Khandakar (2008). ‚ÄúAutomatic Time Series Forecasting: The forecast package for R‚Äù.\n\nsource\n\n\n\n\n AutoARIMAProphet.fit (df, disable_seasonal_features=True, **kwargs)\n\nFit the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series. disable_seasonal_features: bool, Wheter disable Prophet‚Äôs seasonal features. kwargs: Additional arguments.\nReturns: self: AutoARIMAProphet adapter object with AutoARIMA fitted model.\n\nsource\n\n\n\n\n AutoARIMAProphet.predict (df=None)\n\nPredict using the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series.\nReturns: fcsts_df: A pandas.DataFrame with the forecast components.\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "adapters.prophet.html#univariate-prophet",
    "href": "adapters.prophet.html#univariate-prophet",
    "title": "Replace FB-Prophet",
    "section": "2.1 Univariate Prophet ",
    "text": "2.1 Univariate Prophet \nHere we forecast with Prophet without external regressors. We first instantiate a new Prophet object, and define its forecasting procedure into its constructor. After that a classic sklearn fit and predict is used to obtain the predictions.\n\nm = Prophet(daily_seasonality=False)\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nHere we forecast with AutoARIMAProphet adapter without external regressors. It inherits the Prophet constructor as well as its fit and predict methods.\nWith the class AutoARIMAProphet you can simply substitute Prophet and you‚Äôll be training an AutoARIMA model without changing anything in your forecasting pipeline.\n\nm = AutoARIMAProphet(daily_seasonality=False)\nm.fit(df)\n# m.fit(df, disable_seasonal_features=False) # Uncomment for better AutoARIMA predictions\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  },
  {
    "objectID": "adapters.prophet.html#holiday-prophet",
    "href": "adapters.prophet.html#holiday-prophet",
    "title": "Replace FB-Prophet",
    "section": "2.2 Holiday Prophet ",
    "text": "2.2 Holiday Prophet \nUsually Prophet pipelines include the usage of external regressors such as holidays.\nSuppose you want to include holidays or other recurring calendar events, you can create a pandas.DataFrame for them. The DataFrame needs two columns [holiday, ds] and a row for each holiday. It requires all the occurrences of the holiday (as far as the historical data allows) and the future events of the holiday. If the future does not have the holidays registered, they will be modeled but not included in the forecast.\nYou can also include into the events DataFrame, lower_window and upper_window that extends the effect of the holidays through dates to [lower_window, upper_window] days around the date. For example if you wanted to account for Christmas Eve in addition to Christmas you‚Äôd include lower_window=-1,upper_window=0, or Black Friday in addition to Thanksgiving, you‚Äôd include lower_window=0,upper_window=1.\nHere we Peyton Manning‚Äôs playoff appearances dates:\n\nplayoffs = pd.DataFrame({\n  'holiday': 'playoff',\n  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n                        '2010-01-24', '2010-02-07', '2011-01-08',\n                        '2013-01-12', '2014-01-12', '2014-01-19',\n                        '2014-02-02', '2015-01-11', '2016-01-17',\n                        '2016-01-24', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nsuperbowls = pd.DataFrame({\n  'holiday': 'superbowl',\n  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nholidays = pd.concat((playoffs, superbowls))\n\n\nm = Prophet(daily_seasonality=False, holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nThe class AutoARIMAProphet adapter allows to handle these scenarios to fit an AutoARIMA model with exogenous variables.\nYou can enjoy your Prophet pipelines with the improved performance of a classic ARIMA.\n\nm = AutoARIMAProphet(daily_seasonality=False,\n                     holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\n# m.fit(df, disable_seasonal_features=False) # Uncomment for better AutoARIMA predictions\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core Methods",
    "section": "",
    "text": "The core methods of StatsForecast are:\nsource\nGive us a ‚≠ê¬†on Github"
  },
  {
    "objectID": "core.html#statsforecast",
    "href": "core.html#statsforecast",
    "title": "Core Methods",
    "section": "StatsForecast",
    "text": "StatsForecast\n\n StatsForecast (models:List[Any], freq:str, n_jobs:int=1,\n                ray_address:Optional[str]=None,\n                df:Optional[pandas.core.frame.DataFrame]=None,\n                sort_df:bool=True, fallback_model:Any=None,\n                verbose:bool=False, backend:Any=None)\n\ncore.StatsForecast.\nThe core.StatsForecast class allows you to efficiently fit multiple StatsForecast models for large sets of time series. It operates with pandas DataFrame df that identifies series and datestamps with the unique_id and ds columns. The y column denotes the target time series variable.\nThe class has memory-efficient StatsForecast.forecast method that avoids storing partial model outputs. While the StatsForecast.fit and StatsForecast.predict methods with Scikit-learn interface store the fitted models.\nParameters: df: pandas.DataFrame, with columns [unique_id, ds, y] and exogenous. models: List[typing.Any], list of instantiated objects models.StatsForecast. freq: str, frequency of the data, panda‚Äôs available frequencies. n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. sort_df: bool, if True, sort df by [unique_id,ds]. fallback_model: Any, Model to be used if a model fails. Only works with the forecast and cross_validation methods. verbose: bool, Prints TQDM progress bar when n_jobs=1. backend: Any, backend used to distributed processing. Only methods forecast add cross_validation are currently supported.\nNotes: The core.StatsForecast class offers parallelization utilities with Dask, Spark and Ray back-ends. See distributed computing example here.\n\n# StatsForecast's class usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import generate_series\nfrom statsforecast.models import ( \n    ADIDA,\n    AutoARIMA,\n    CrostonClassic,\n    CrostonOptimized,\n    CrostonSBA,\n    ETS,\n    HistoricAverage,\n    IMAPA,\n    Naive,\n    RandomWalkWithDrift,\n    SeasonalExponentialSmoothing,\n    SeasonalNaive,\n    SeasonalWindowAverage,\n    SimpleExponentialSmoothing,\n    TSB,\n    WindowAverage,\n)\n\n# Generate synthetic panel DataFrame for example\npanel_df = generate_series(n_series=9, equal_ends=False)\npanel_df.groupby('unique_id').tail(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      y\n    \n    \n      unique_id\n      \n      \n    \n  \n  \n    \n      0\n      2000-08-06\n      1.212726\n    \n    \n      0\n      2000-08-07\n      2.442669\n    \n    \n      0\n      2000-08-08\n      3.339940\n    \n    \n      0\n      2000-08-09\n      4.228065\n    \n    \n      1\n      2000-04-03\n      0.048275\n    \n    \n      1\n      2000-04-04\n      1.128070\n    \n    \n      1\n      2000-04-05\n      2.295968\n    \n    \n      1\n      2000-04-06\n      3.238239\n    \n    \n      2\n      2000-06-12\n      6.480128\n    \n    \n      2\n      2000-06-13\n      0.036217\n    \n    \n      2\n      2000-06-14\n      1.009650\n    \n    \n      2\n      2000-06-15\n      2.489787\n    \n    \n      3\n      2000-08-26\n      3.289840\n    \n    \n      3\n      2000-08-27\n      4.227949\n    \n    \n      3\n      2000-08-28\n      5.321176\n    \n    \n      3\n      2000-08-29\n      6.127013\n    \n    \n      4\n      2001-01-04\n      5.403709\n    \n    \n      4\n      2001-01-05\n      6.081779\n    \n    \n      4\n      2001-01-06\n      0.438420\n    \n    \n      4\n      2001-01-07\n      1.386855\n    \n    \n      5\n      2000-10-24\n      5.011166\n    \n    \n      5\n      2000-10-25\n      6.397153\n    \n    \n      5\n      2000-10-26\n      0.462146\n    \n    \n      5\n      2000-10-27\n      1.253125\n    \n    \n      6\n      2000-08-29\n      5.407805\n    \n    \n      6\n      2000-08-30\n      6.340789\n    \n    \n      6\n      2000-08-31\n      0.202894\n    \n    \n      6\n      2000-09-01\n      1.491204\n    \n    \n      7\n      2001-02-09\n      1.068102\n    \n    \n      7\n      2001-02-10\n      2.233974\n    \n    \n      7\n      2001-02-11\n      3.484143\n    \n    \n      7\n      2001-02-12\n      4.176505\n    \n    \n      8\n      2000-02-25\n      4.110373\n    \n    \n      8\n      2000-02-26\n      5.483879\n    \n    \n      8\n      2000-02-27\n      6.068916\n    \n    \n      8\n      2000-02-28\n      0.040499\n    \n  \n\n\n\n\n\n# Declare list of instantiated StatsForecast estimators to be fitted\n# You can try other estimator's hyperparameters\n# You can try other methods from the `models.StatsForecast` collection\n# Check them here: https://nixtla.github.io/statsforecast/models.html\nmodels=[AutoARIMA(), Naive(), ETS()] \n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=models,\n                     freq='D', \n                     n_jobs=1, \n                     verbose=True)\n\n# Efficiently predict\nfcsts_df = fcst.forecast(h=4, fitted=True)\nfcsts_df.groupby('unique_id').tail(4)\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.fit",
    "href": "core.html#statsforecast.fit",
    "title": "Core Methods",
    "section": "StatsForecast.fit",
    "text": "StatsForecast.fit\n\n StatsForecast.fit (df:Optional[pandas.core.frame.DataFrame]=None,\n                    sort_df:bool=True)\n\nFit the core.StatsForecast.\nFit models to a large set of time series from DataFrame df. and store fitted models for later inspection.\nParameters: df: pandas.DataFrame, with columns [unique_id, ds, y] and exogenous. sort_df: bool, if True, sort df by [unique_id,ds].\nReturns: self: Returns with stored StatsForecast fitted models.\n\nsource"
  },
  {
    "objectID": "core.html#satstforecast.predict",
    "href": "core.html#satstforecast.predict",
    "title": "Core Methods",
    "section": "SatstForecast.predict",
    "text": "SatstForecast.predict\n\n SatstForecast.predict (h:int,\n                        X_df:Optional[pandas.core.frame.DataFrame]=None,\n                        level:Optional[List[int]]=None)\n\nPredict with core.StatsForecast.\nUse stored fitted models to predict large set of time series from DataFrame df.\nParameters: h: int, forecast horizon. X_df: pandas.DataFrame, with [unique_id, ds] columns and df‚Äôs future exogenous. level: float list 0-100, confidence levels for prediction intervals.\nReturns: fcsts_df: pandas.DataFrame, with models columns for point predictions and probabilistic predictions for all fitted models.\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.fit_predict",
    "href": "core.html#statsforecast.fit_predict",
    "title": "Core Methods",
    "section": "StatsForecast.fit_predict",
    "text": "StatsForecast.fit_predict\n\n StatsForecast.fit_predict (h:int,\n                            df:Optional[pandas.core.frame.DataFrame]=None,\n                            X_df:Optional[pandas.core.frame.DataFrame]=Non\n                            e, level:Optional[List[int]]=None,\n                            sort_df:bool=True)\n\nFit and Predict with core.StatsForecast.\nThis method avoids memory burden due from object storage. It is analogous to Scikit-Learn fit_predict without storing information. It requires the forecast horizon h in advance.\nIn contrast to StatsForecast.forecast this method stores partial models outputs.\nParameters: h: int, forecast horizon. df: pandas.DataFrame, with columns [unique_id, ds, y] and exogenous. X_df: pandas.DataFrame, with [unique_id, ds] columns and df‚Äôs future exogenous. level: float list 0-100, confidence levels for prediction intervals. sort_df: bool, if True, sort df by [unique_id,ds].\nReturns: fcsts_df: pandas.DataFrame, with models columns for point predictions and probabilistic predictions for all fitted models.\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.forecast",
    "href": "core.html#statsforecast.forecast",
    "title": "Core Methods",
    "section": "StatsForecast.forecast",
    "text": "StatsForecast.forecast\n\n StatsForecast.forecast (h:int,\n                         df:Optional[pandas.core.frame.DataFrame]=None,\n                         X_df:Optional[pandas.core.frame.DataFrame]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False, sort_df:bool=True)\n\nMemory Efficient core.StatsForecast predictions.\nThis method avoids memory burden due from object storage. It is analogous to Scikit-Learn fit_predict without storing information. It requires the forecast horizon h in advance.\nParameters: h: int, forecast horizon. df: pandas.DataFrame, with columns [unique_id, ds, y] and exogenous. X_df: pandas.DataFrame, with [unique_id, ds] columns and df‚Äôs future exogenous. level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions. sort_df: bool, if True, sort df by [unique_id,ds].\nReturns: fcsts_df: pandas.DataFrame, with models columns for point predictions and probabilistic predictions for all fitted models.\n\n# StatsForecast.forecast method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import AutoARIMA, Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[AutoARIMA(), Naive()],\n                     freq='D', n_jobs=1)\n\n# Efficiently predict without storing memory\nfcsts_df = fcst.forecast(h=4, fitted=True)\nfcsts_df.groupby('unique_id').tail(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      AutoARIMA\n      Naive\n    \n    \n      unique_id\n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1961-01-01\n      476.006500\n      432.0\n    \n    \n      1.0\n      1961-01-02\n      482.846222\n      432.0\n    \n    \n      1.0\n      1961-01-03\n      512.423523\n      432.0\n    \n    \n      1.0\n      1961-01-04\n      502.038269\n      432.0\n    \n  \n\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.forecast_fitted_values",
    "href": "core.html#statsforecast.forecast_fitted_values",
    "title": "Core Methods",
    "section": "StatsForecast.forecast_fitted_values",
    "text": "StatsForecast.forecast_fitted_values\n\n StatsForecast.forecast_fitted_values ()\n\nAccess core.StatsForecast insample predictions.\nAfter executing StatsForecast.forecast, you can access the insample prediction values for each model. To get them, you need to pass fitted=True to the StatsForecast.forecast method and then use the StatsForecast.forecast_fitted_values method.\nParameters: Check StatsForecast.forecast parameters, use fitted=True.\nReturns: fcsts_df: pandas.DataFrame, with insample models columns for point predictions and probabilistic predictions for all fitted models.\n\n# StatsForecast.forecast_fitted_values method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[AutoARIMA()],\n                     freq='D', n_jobs=1)\n\n# Access insample predictions\nfcsts_df = fcst.forecast(h=12, fitted=True, level=(90, 10))\ninsample_fcsts_df = fcst.forecast_fitted_values()\ninsample_fcsts_df.tail(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      y\n      AutoARIMA\n      AutoARIMA-lo-90\n      AutoARIMA-lo-10\n      AutoARIMA-hi-10\n      AutoARIMA-hi-90\n    \n    \n      unique_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1960-09-30\n      508.0\n      572.654175\n      525.092163\n      569.020630\n      576.287781\n      620.216187\n    \n    \n      1.0\n      1960-10-31\n      461.0\n      451.528259\n      403.966248\n      447.894684\n      455.161835\n      499.090271\n    \n    \n      1.0\n      1960-11-30\n      390.0\n      437.915375\n      390.353394\n      434.281799\n      441.548981\n      485.477386\n    \n    \n      1.0\n      1960-12-31\n      432.0\n      369.718781\n      322.156769\n      366.085205\n      373.352356\n      417.280792\n    \n  \n\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.cross_validation",
    "href": "core.html#statsforecast.cross_validation",
    "title": "Core Methods",
    "section": "StatsForecast.cross_validation",
    "text": "StatsForecast.cross_validation\n\n StatsForecast.cross_validation (h:int,\n                                 df:Optional[pandas.core.frame.DataFrame]=\n                                 None, n_windows:int=1, step_size:int=1,\n                                 test_size:Optional[int]=None,\n                                 input_size:Optional[int]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False, sort_df:bool=True)\n\nTemporal Cross-Validation with core.StatsForecast.\ncore.StatsForecast‚Äôs cross-validation efficiently fits a list of StatsForecast models through multiple training windows, in either chained or rolled manner.\nStatsForecast.models‚Äô speed allows to overcome this evaluation technique high computational costs. Temporal cross-validation provides better model‚Äôs generalization measurements by increasing the test‚Äôs length and diversity.\nParameters: h: int, forecast horizon. df: pandas.DataFrame, with columns [unique_id, ds, y] and exogenous. n_windows: int, number of windows used for cross validation. step_size: int = 1, step size between each window. test_size: Optional[int] = None, length of test size. If passed, set n_windows=None. input_size: Optional[int] = None, input size for each window, if not none rolled windows. level: float list 0-100, confidence levels for prediction intervals. fitted: bool, wether or not returns insample predictions. sort_df: bool, if True, sort df by unique_id and ds.\nReturns: fcsts_df: pandas.DataFrame, with insample models columns for point predictions and probabilistic predictions for all fitted models.\n\n# StatsForecast.crossvalidation method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[Naive()],\n                     freq='D', n_jobs=1, verbose=True)\n\n# Access insample predictions\nrolled_fcsts_df = fcst.cross_validation(14, n_windows=2)\nrolled_fcsts_df.head(4)\n\nCross Validation Time Series 1:   0%|          | 0/2 [00:00<?, ?it/s]Cross Validation Time Series 1: 100%|##########| 2/2 [00:00<00:00, 10010.27it/s]\n\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      Naive\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1960-12-17\n      1960-12-16\n      407.0\n      463.0\n    \n    \n      1.0\n      1960-12-18\n      1960-12-16\n      362.0\n      463.0\n    \n    \n      1.0\n      1960-12-19\n      1960-12-16\n      405.0\n      463.0\n    \n    \n      1.0\n      1960-12-20\n      1960-12-16\n      417.0\n      463.0\n    \n  \n\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.cross_validation_fitted_values",
    "href": "core.html#statsforecast.cross_validation_fitted_values",
    "title": "Core Methods",
    "section": "StatsForecast.cross_validation_fitted_values",
    "text": "StatsForecast.cross_validation_fitted_values\n\n StatsForecast.cross_validation_fitted_values ()\n\nAccess core.StatsForecast insample cross validated predictions.\nAfter executing StatsForecast.cross_validation, you can access the insample prediction values for each model and window. To get them, you need to pass fitted=True to the StatsForecast.cross_validation method and then use the StatsForecast.cross_validation_fitted_values method.\nParameters: Check StatsForecast.cross_validation parameters, use fitted=True.\nReturns: fcsts_df: pandas.DataFrame, with insample models columns for point predictions and probabilistic predictions for all fitted models.\n\n# StatsForecast.cross_validation_fitted_values method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[Naive()],\n                     freq='D', n_jobs=1)\n\n# Access insample predictions\nrolled_fcsts_df = fcst.cross_validation(h=12, n_windows=2, fitted=True)\ninsample_rolled_fcsts_df = fcst.cross_validation_fitted_values()\ninsample_rolled_fcsts_df.tail(4)\n\n\n\n\n\n  \n    \n      \n      ds\n      cutoff\n      y\n      Naive\n    \n    \n      unique_id\n      \n      \n      \n      \n    \n  \n  \n    \n      1.0\n      1959-09-30\n      1959-12-31\n      463.0\n      559.0\n    \n    \n      1.0\n      1959-10-31\n      1959-12-31\n      407.0\n      463.0\n    \n    \n      1.0\n      1959-11-30\n      1959-12-31\n      362.0\n      407.0\n    \n    \n      1.0\n      1959-12-31\n      1959-12-31\n      405.0\n      362.0\n    \n  \n\n\n\n\n\nsource"
  },
  {
    "objectID": "core.html#statsforecast.plot",
    "href": "core.html#statsforecast.plot",
    "title": "Core Methods",
    "section": "StatsForecast.plot",
    "text": "StatsForecast.plot\n\n StatsForecast.plot (df:pandas.core.frame.DataFrame,\n                     forecasts_df:Optional[pandas.core.frame.DataFrame]=No\n                     ne, unique_ids:Union[List[str],NoneType,numpy.ndarray\n                     ]=None, plot_random:bool=True,\n                     models:Optional[List[str]]=None,\n                     level:Optional[List[float]]=None,\n                     max_insample_length:Optional[int]=None,\n                     plot_anomalies:Optional[bool]=False,\n                     engine:str='plotly')\n\nPlot forecasts and insample values.\nParameters: df: pandas.DataFrame, with columns [unique_id, ds, y]. forecasts_df: pandas.DataFrame, with columns [unique_id, ds] and models. unique_ids: List[str], Time Series to plot. plot_random: bool, Select time series to plot randomly. models: List[str], List of models to plot. level: List[float], List of prediction intervals to plot if paseed. max_insample_length: int, max number of train/insample observations to be plotted. plot_anomalies: bool, Plot anomalies for each prediction interval. engine: str, library used to plot. ‚Äòplotly‚Äô or ‚Äòmatplotlib‚Äô."
  },
  {
    "objectID": "core.html#integer-datestamp",
    "href": "core.html#integer-datestamp",
    "title": "Core Methods",
    "section": "Integer datestamp",
    "text": "Integer datestamp\nThe StatsForecast class can also receive integers as datestamp, the following example shows how to do it.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengers as ap\nfrom statsforecast.models import HistoricAverage\n\n\nint_ds_df = pd.DataFrame({'ds': np.arange(1, len(ap) + 1), 'y': ap})\nint_ds_df.insert(0, 'unique_id', 'AirPassengers')\nint_ds_df.set_index('unique_id', inplace=True)\nint_ds_df.head()\n\n\nint_ds_df.tail()\n\n\nfcst = StatsForecast(df=int_ds_df, models=[HistoricAverage()], freq='D')\nhorizon = 7\nforecast = fcst.forecast(horizon)\nforecast.head()\n\n\nlast_date = int_ds_df['ds'].max()\ntest_eq(forecast['ds'].values, np.arange(last_date + 1, last_date + 1 + horizon))\n\n\nint_ds_cv = fcst.cross_validation(h=7, test_size=8, n_windows=None)\nint_ds_cv"
  },
  {
    "objectID": "core.html#external-regressors",
    "href": "core.html#external-regressors",
    "title": "Core Methods",
    "section": "External regressors",
    "text": "External regressors\nEvery column after y is considered an external regressor and will be passed to the models that allow them. If you use them you must supply the future values to the StatsForecast.forecast method.\n\nclass LinearRegression:\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, y, X):\n        self.coefs_, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return self\n    \n    def predict(self, h, X):\n        mean = X @ coefs\n        return mean\n    \n    def __repr__(self):\n        return 'LinearRegression()'\n    \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        coefs, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return {'mean': X_future @ coefs}\n    \n    def new(self):\n        b = type(self).__new__(type(self))\n        b.__dict__.update(self.__dict__)\n        return b\n\n\nseries_xreg = series = generate_series(10_000, equal_ends=True)\nseries_xreg['intercept'] = 1\nseries_xreg['dayofweek'] = series_xreg['ds'].dt.dayofweek\nseries_xreg = pd.get_dummies(series_xreg, columns=['dayofweek'], drop_first=True)\nseries_xreg\n\n\ndates = sorted(series_xreg['ds'].unique())\nvalid_start = dates[-14]\ntrain_mask = series_xreg['ds'] < valid_start\nseries_train = series_xreg[train_mask]\nseries_valid = series_xreg[~train_mask]\nX_valid = series_valid.drop(columns=['y'])\nfcst = StatsForecast(\n    df=series_train,\n    models=[LinearRegression()],\n    freq='D',\n)\nxreg_res = fcst.forecast(14, X_df=X_valid)\nxreg_res['y'] = series_valid['y'].values\n\n\nxreg_res.groupby('ds').mean().plot()\n\n\nxreg_res_cv = fcst.cross_validation(h=3, test_size=5, n_windows=None)"
  },
  {
    "objectID": "core.html#confidence-intervals",
    "href": "core.html#confidence-intervals",
    "title": "Core Methods",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nYou can pass the argument level to the StatsForecast.forecast method to calculate confidence intervals. Not all models can calculate them at the moment, so we will only obtain the intervals of those models that have it implemented.\n\nap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap}, index=pd.Index([0] * ap.size, name='unique_id'))\nfcst = StatsForecast(\n    df=ap_df,\n    models=[\n        SeasonalNaive(season_length=12), \n        AutoARIMA(season_length=12)\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = fcst.forecast(12, level=(80, 95))\nap_ci.set_index('ds').plot(marker='.', figsize=(10, 6))\n\n\n#hide\ndef test_conf_intervals(n_jobs=1):\n    ap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap}, index=pd.Index([0] * ap.size, name='unique_id'))\n    fcst = StatsForecast(\n        df=ap_df,\n        models=[\n            SeasonalNaive(season_length=12), \n            AutoARIMA(season_length=12)\n        ],\n        freq='M',\n        n_jobs=n_jobs\n    )\n    ap_ci = fcst.forecast(12, level=(80, 95))\n    ap_ci.set_index('ds').plot(marker='.', figsize=(10, 6))\ntest_conf_intervals(n_jobs=1)"
  },
  {
    "objectID": "mstl.html",
    "href": "mstl.html",
    "title": "MSTL model",
    "section": "",
    "text": "source\n\nmstl\n\n mstl (x:numpy.ndarray, period:Union[int,List[int]],\n       blambda:Optional[float]=None, iterate:int=1,\n       s_window:Optional[numpy.ndarray]=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\ntime series\n\n\nperiod\ntyping.Union[int, typing.List[int]]\n\nseasom length\n\n\nblambda\ntyping.Optional[float]\nNone\nbox-cox transform\n\n\niterate\nint\n1\nnumber of iterations\n\n\ns_window\ntyping.Optional[numpy.ndarray]\nNone\nseasonal window\n\n\n\n\n\n\n\nGive us a ‚≠ê¬†on Github"
  }
]